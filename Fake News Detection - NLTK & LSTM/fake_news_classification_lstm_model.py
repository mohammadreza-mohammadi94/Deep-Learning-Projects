# -*- coding: utf-8 -*-
"""Fake News Classification - LSTM Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R1mwGDqdPYnaCaXZPPVyJ4ECmjnC7evJ

# Setup Enviorment
"""

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c fake-news
!unzip fake-news.zip

"""# Import Libraries"""

import os
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

import warnings
warnings.filterwarnings('ignore')

import tensorflow as tf
from tensorflow.keras.layers import (Dense,
                                    LSTM,
                                    Embedding,
                                    Dropout,
                                    Bidirectional)
from tensorflow.keras.regularizers  import l2
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.preprocessing.text import Tokenizer

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc

import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# ======================== #
# Setup Preprocessing
# ======================== #

nltk.download("stopwords")
nltk.download("punkt_tab")
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()

"""# Import & Analyze the Dataset"""

train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")

# ===================== #
# Analyze DataFrame
# ===================== #

def analyze_df(df, dataset_name, display_head=True, summary_stats=True):
    """
    Analyzes a given DataFrame and prints useful information about it.

    Parameters:
        df (pd.DataFrame): The DataFrame to analyze.
        dataset_name (str): Name of the dataset for display purposes.
        display_head (bool): If True, display the first few rows of the DataFrame.
        summary_stats (bool): If True, include summary statistics for numeric columns.

    Returns:
        None
    """
    # DataFrame Analysis
    print(f"{'-'*40}\nAnalyzing '{dataset_name}' DataFrame\n{'-'*40}\n")
    print(f"Number of Rows: {df.shape[0]} | Number of Columns: {df.shape[1]}")
    print("\nColumn Overview:")
    print(df.info())

    # Check Missing Values
    print(f"\n{'-'*40}\n Missing Values \n{'-'*40}\n")
    print("Missing Values: ")
    nans = df.isna().sum()
    if nans.sum() > 0:
        print(nans[nans > 0])
    else:
        print("No Missing Values...")

    # Check Dupes
    print(f"\n{'-'*40}\n Duplicated Rows \n{'-'*40}\n")
    dupes = df.duplicated().sum()
    print(f"Number of Duplicates: {dupes}")

    # Statistical Analysis
    print(f"\n{'-'*40}\n {dataset_name} Statistical Summary \n{'-'*40}\n")
    if summary_stats:
        print("Summary Statistics:")
        print(df.describe())

    if display_head:
        print(f"\n{'-'*40}\n DataFrame Sample \n{'-'*40}\n")
        print("\nFirst Few Rows:")
        return df.head()

    print(f"\n{'-'*40}\nAnalysis Complete for '{dataset_name}'\n{'-'*40}")

# ======================== #
# Analyze Train Dataframe
# ======================== #
analyze_df(train_df, "Train")

# ======================== #
# Analyze Test Dataframe
# ======================== #

analyze_df(test_df, "Test")

# ======================== #
# WordCloud Train Dataset
# ======================== #

# FAKE NEWS
fake_news = " ".join(train_df[train_df['label'] == 1]['text'].astype(str).tolist())
real_news = " ".join(train_df[train_df['label'] == 0]['text'].astype(str).tolist())

# Create a wordcloud
wordcloud_fake = WordCloud(width = 800, height = 400,
                           background_color='black', stopwords=set(stopwords.words('english')),
                           min_font_size=10).generate(fake_news)
# Plot
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud_fake)
plt.axis("off")
plt.tight_layout(pad = 0)
plt.title("Word Cloud for Fake News", fontsize=20)
plt.show()

# ================================================ #
# ================================================ #

# REAL NEWS
fake_news = " ".join(train_df[train_df['label'] == 1]['text'].astype(str).tolist())
real_news = " ".join(train_df[train_df['label'] == 0]['text'].astype(str).tolist())

# Create a wordcloud
wordcloud_fake = WordCloud(width = 800, height = 400,
                           background_color='black', stopwords=set(stopwords.words('english')),
                           min_font_size=10).generate(real_news)
# Plot
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud_fake)
plt.axis("off")
plt.tight_layout(pad = 0)
plt.title("Word Cloud for Real News", fontsize=20)
plt.show()

# ======================== #
# Countplot Of Label
# ======================== #

plt.figure(figsize=(6, 4))
sns.set_style("darkgrid")
sns.countplot(x='label', data=train_df, palette='viridis')
plt.title('Distribution of Labels')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

"""# Data Preparation

### Text-Preprocessing
"""

def text_preprocessing(text):
    """
    Preprocesses the given text:
    - Converts to lowercase
    - Removes non-alphabetic characters
    - Tokenizes the text
    - Removes stopwords
    - Lemmatizes the words

    Parameters:
        text (str): Input text to preprocess.

    Returns:
        str: Preprocessed text.
    """
    # Handle non-string inputs
    if not isinstance(text, str):
        return ""

    text = text.lower() # Convert text to lower
    text = re.sub(r'[^a-zA-Z]', ' ', text) # Remove non-alphabetic characters
    words = word_tokenize(text) # Tokenize the text
    words = [word for word in words if word not in stopwords.words('english')] # Remove stopwords
    words = [lemmatizer.lemmatize(word) for word in words] # Lemmatization
    return " ".join(words) # Joing words

# ======================== #
# Apply Preprocssing Func
# ======================== #

# Train Dataframe
train_df['preprocessed_text'] = train_df['text'].apply(text_preprocessing)
# Test Dataframe
test_df['preprocessed_text'] = test_df['text'].apply(text_preprocessing)

"""### Tokenization"""

# ======================== #
# Tokenization & Padding
# ======================== #
tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(train_df['preprocessed_text'])
train_sequences = tokenizer.texts_to_sequences(train_df['preprocessed_text'])
test_sequences = tokenizer.texts_to_sequences(test_df['preprocessed_text'])

max_length=100
train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')
test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')

# Seperate Labels
train_labels = train_df['label'].values

"""# Define Model"""

# Split train/Test sets
X_train, X_test, y_train, y_test = train_test_split(train_padded, train_labels,
                                                    test_size=0.2, random_state=42)

print(f"X Train Shape: ", X_train.shape)
print(f"X Test Shape: ", X_test.shape)
print(f"Y Train Shape: ", y_train.shape)
print(f"Y Test Shape: ", y_test.shape)

# ================= #
# Build The Model
# ================= #

model = Sequential([
    # Embedding Layer
    Embedding(input_dim=10000, output_dim=128,
              input_length=max_length),
    # LSTM
    Bidirectional(LSTM(256,
         return_sequences=False,
         recurrent_regularizer=l2(0.01),
         bias_regularizer=l2(0.01))),

    Dropout(0.2),

    # FC layer
    Dense(64, activation='relu',
          kernel_regularizer=l2(0.01)),
    Dropout(0.2),

    # Output layer
    Dense(1,
          activation='sigmoid')

])

# Explicitly build the model
model.build(input_shape=(None, max_length))

# Compile Model
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# Summary
model.summary()

# ================= #
# Callbacks
# ================= #

# EarlyStopping
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True,
    verbose=1
)

# Model Checkpoint
check_point = ModelCheckpoint(
    filepath='/content/best_model.h5.keras',
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

# ================= #
# Train The Model
# ================= #

history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stop, check_point]
)

# =================== #
# Evaluate The Model
# =================== #

test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

# =================== #
# Evaluate The Model
# =================== #

y_pred = (model.predict(X_test) > 0.5).astype("int32")
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

# =================== #
# ROC AUC Curve
# =================== #

# Get predicted probabilities
y_probs = model.predict(X_test)

# Calculate ROC
fpr, tpr, thresholds = roc_curve(y_test, y_probs)

# Calculate AUC
roc_auc = auc(fpr, tpr)
print(f"AUC: {roc_auc}")

# Plot the ROC curve as well to visualize the trade-off
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""# Test Model on `test_df`"""

# =================== #
# Load Saved Model
# =================== #

model = load_model('/content/best_model.h5.keras')

# ====================== #
# Preprocessing test_df
# ====================== #

test_df['preprocessed_text'] = test_df['text'].apply(text_preprocessing)

# Tokenize the text and pad sequences
X_test_df = tokenizer.texts_to_sequences(test_df['preprocessed_text'])
X_test_df = pad_sequences(X_test_df, maxlen=max_length)

# ========================== #
# Make Prediction On Test_df
# ========================== #

predictions = model.predict(X_test_df)

# Convert probabilities to class labels (0 or 1)
predictions = (predictions > 0.5).astype(int)

# Add predictions to the test_df
test_df['predictions'] = predictions

test_df['predictions']

"""# Save Pickle File"""

import pickle

# Save model architecture and weights into a pickle file
with open('fake_news_lstm_model.pkl', 'wb') as f:
    pickle.dump(model, f)