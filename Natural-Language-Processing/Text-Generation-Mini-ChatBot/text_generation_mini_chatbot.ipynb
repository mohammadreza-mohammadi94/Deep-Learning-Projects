{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMYuI5hZKFeJevZtpy2MzkH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/Deep-Learning-Projects/blob/main/Natural-Language-Processing/Text-Generation-Mini-ChatBot/text_generation_mini_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC9kKcla5_Tg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "130d46ec-1ed8-4a17-99d2-e5f8eb63a534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets==3.6.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "U2sUKJJp62cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    dataset = load_dataset(\"empathetic_dialogues\", trust_remote_code=True)\n",
        "    return dataset\n",
        "\n",
        "def format_chat_data(dataset):\n",
        "    df_train = pd.DataFrame(dataset['train'])\n",
        "    df_test = pd.DataFrame(dataset[\"test\"])\n",
        "    df_val = pd.DataFrame(dataset[\"validation\"])\n",
        "\n",
        "    df = pd.concat([df_train, df_test, df_val], ignore_index=True)\n",
        "    df = df.sort_values(by=['conv_id', 'utterance_idx'])\n",
        "    dialogs = df.groupby(\"conv_id\")[\"utterance\"].apply(list).tolist()\n",
        "\n",
        "    formatted_corpus = []\n",
        "\n",
        "    for dialog in dialogs:\n",
        "        if len(dialog) < 2:\n",
        "            continue\n",
        "        for i in range(0, len(dialog) - 1, 2):\n",
        "            user_text = dialog[i].replace(\"_comma_\", \",\")\n",
        "            bot_text = dialog[i+1].replace(\"_comma_\", \",\")\n",
        "\n",
        "            chat_turn = f\"<USER> {user_text} <BOT> {bot_text} <END>\"\n",
        "            formatted_corpus.append(chat_turn)\n",
        "\n",
        "    return formatted_corpus\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def chat_standardization(input_string):\n",
        "    text = tf.strings.lower(input_string)\n",
        "    # Keep special tokens\n",
        "    text = tf.strings.regex_replace(text, r\"(<user>|<bot>|<end>)\", r\" \\1 \")\n",
        "    # Split special characters\n",
        "    text = tf.strings.regex_replace(text, r\"([.,!?])\", r\" \\1 \")\n",
        "    text = tf.strings.regex_replace(text, r\"[^a-zA-Z0-9.,!?<> ]\", \"\")\n",
        "    text = tf.strings.regex_replace(text, r\"\\s{2,}\", \" \")\n",
        "    return text\n",
        "\n",
        "def get_vectorizer(text, max_tokens):\n",
        "    vectorizer = tf.keras.layers.TextVectorization(\n",
        "        standardize=chat_standardization,\n",
        "        max_tokens=max_tokens,\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=None,\n",
        "    )\n",
        "    vectorizer.adapt(text)\n",
        "    vocab = vectorizer.get_vocabulary()\n",
        "    vocab_size = len(vocab)\n",
        "    return vectorizer, vocab, vocab_size\n",
        "\n",
        "def prepare_dataset(vectorizer, text_corpus, batch_size, seq_len):\n",
        "    full_text = \" \".join(text_corpus)\n",
        "    full_text_ids = vectorizer([full_text])[0]\n",
        "    word_dataset = tf.data.Dataset.from_tensor_slices(full_text_ids)\n",
        "    sequences = word_dataset.batch(seq_len + 1, drop_remainder=True)\n",
        "\n",
        "    def split_input_target(seq):\n",
        "        return seq[:-1], seq[1:]\n",
        "\n",
        "    dataset = sequences.map(split_input_target)\n",
        "    dataset = dataset.shuffle(10_000).batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "def build_model(vocab_size, embedding_dim=256, rnn_units=512, batch_size=64, stateful=False):\n",
        "    if stateful:\n",
        "        input_layer = tf.keras.Input(batch_shape=(batch_size, None))\n",
        "    else:\n",
        "        input_layer = tf.keras.Input(shape=(None,))\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        input_layer,\n",
        "\n",
        "        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
        "\n",
        "        tf.keras.layers.GRU(units=rnn_units, return_sequences=True, stateful=stateful),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "        tf.keras.layers.GRU(units=rnn_units, return_sequences=True, stateful=stateful),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "        tf.keras.layers.Dense(units=vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def generate_reply(model, vectorizer, user_input, max_generate=50, temp=0.6):\n",
        "    prompt = f\"<user> {user_input} <bot>\"\n",
        "\n",
        "    input_eval = vectorizer([prompt])\n",
        "\n",
        "    vocab = vectorizer.get_vocabulary()\n",
        "    generated_tokens = []\n",
        "\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'reset_states'):\n",
        "            layer.reset_states()\n",
        "\n",
        "    for i in range(max_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = predictions[0, -1, :]\n",
        "\n",
        "        predictions = predictions / temp\n",
        "        predictions = tf.expand_dims(predictions, 0)\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[0, 0].numpy()\n",
        "\n",
        "        predicted_word = vocab[predicted_id]\n",
        "\n",
        "        if predicted_word == \"<end>\":\n",
        "            break\n",
        "\n",
        "        if predicted_id > 1:\n",
        "            generated_tokens.append(predicted_word)\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    bot_reply = \" \".join(generated_tokens)\n",
        "    bot_reply = bot_reply.replace(\" ,\", \",\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\n",
        "\n",
        "    return bot_reply\n",
        "\n",
        "def start_chat(model, vectorizer):\n",
        "    print(\"=\"*50)\n",
        "    print(\"ü§ñ AI Chatbot is ONLINE! (Type 'quit' or 'exit' to stop)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    while True:\n",
        "        user_text = input(\"\\nYou: \")\n",
        "\n",
        "        if user_text.lower() in ['quit', 'exit']:\n",
        "            print(\"ü§ñ AI: Goodbye! Have a great day.\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            reply = generate_reply(\n",
        "                model=model,\n",
        "                vectorizer=vectorizer,\n",
        "                user_input=user_text,\n",
        "                max_generate=60,\n",
        "                temp=0.6\n",
        "            )\n",
        "            print(f\"ü§ñ AI: {reply}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Something went wrong: {e}\")\n",
        "\n",
        "def main():\n",
        "    MAX_TOKENS = 12_000\n",
        "    SEQ_LENGTH = 40\n",
        "    BATCH_SIZE = 64\n",
        "    # Load Dataset\n",
        "    print(\"[INFO] - Load the dataset...\")\n",
        "    dataset = load_data()\n",
        "    print(\"[INFO] - Dataset loaded...\")\n",
        "\n",
        "    # Format dataset\n",
        "    print(\"[INFO] - Grouping conversations...\")\n",
        "    formatted_corpus = format_chat_data(dataset)\n",
        "    print(f\"[INFO] - Successfully created {len(formatted_corpus)} chat turns from all splits.\")\n",
        "\n",
        "    # Vectorization\n",
        "    print(\"[INFO] - Apply vectorization...\")\n",
        "    vectorizer, vocab, vocab_size = get_vectorizer(formatted_corpus, max_tokens=MAX_TOKENS)\n",
        "    print(\"[INFO] - Vectorization applied...\")\n",
        "    print(f\"[INFO] - Vocabulary size: {vocab_size}\")\n",
        "\n",
        "    # Creating dataset\n",
        "    print(\"[INFO] - Creating tf.data.Dataset..\")\n",
        "    dataset = prepare_dataset(vectorizer, formatted_corpus, BATCH_SIZE, SEQ_LENGTH)\n",
        "    print(\"[INFO] - Dataset created...\")\n",
        "    # Check shapes\n",
        "    for inputs, targets in dataset.take(1):\n",
        "        print(\"\\nInput shape:\", inputs.shape)\n",
        "        print(\"Target shape:\", targets.shape)\n",
        "\n",
        "    # Build model\n",
        "    print(\"\\n[INFO] - Building model...\")\n",
        "    model = build_model(\n",
        "        vocab_size=vocab_size,\n",
        "    )\n",
        "    print(\"[INFO] - Model Created Successfuly...\")\n",
        "    print(\"\\n\\nCheck Summary:\")\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    )\n",
        "    print(\"\\n[INFO] - Model Compiled...\")\n",
        "\n",
        "    checkpoint_path = \"chatbot_model_checkpoint.weights.h5\"\n",
        "    callbacks = [\n",
        "        # Stop training if the loss doesn't improve for 7 epochs, and restore the best weights.\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor=\"loss\", patience=7, restore_best_weights=True),\n",
        "        # Save model weights at each epoch, but only keep the best performing one based on loss.\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=checkpoint_path, save_weights_only=True, monitor=\"loss\", save_best_only=True),\n",
        "        # Reduce the learning rate if the loss plateaus for 3 epochs.\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor=\"loss\", factor=0.5, patience=3)\n",
        "    ]\n",
        "    print(\"[INFO] - Starting training..\")\n",
        "    history = model.fit(\n",
        "        dataset,\n",
        "        epochs=100,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    print(\"[INFO] - Training Finished\")\n",
        "    model.save_weights(\"final_weights.weights.h5\")\n",
        "\n",
        "    # Build inference model\n",
        "    print(\"\\n[INFO] - Building Inference Model...\")\n",
        "    inference_model = build_model(\n",
        "        vocab_size=vocab_size,\n",
        "        batch_size=1,\n",
        "        stateful=True\n",
        "    )\n",
        "\n",
        "    inference_model.load_weights(\"final_weights.weights.h5\")\n",
        "    inference_model.build(tf.TensorShape([1, None]))\n",
        "    print(\"[INFO] - Inference Model Ready.\")\n",
        "\n",
        "    # Start Chat\n",
        "    start_chat(inference_model, vectorizer)\n",
        "\n",
        "# Execution\n",
        "main()"
      ],
      "metadata": {
        "id": "AtXJbIiC7ARM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mkiCOadt_rHl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}