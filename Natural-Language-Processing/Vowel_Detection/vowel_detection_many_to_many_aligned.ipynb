{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF3TB/iDECfy2N/z7ogMAJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/Deep-Learning-Projects/blob/main/Vowel_Detection/vowel_detection_many_to_many_aligned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "EBpY-tM5_qid"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "S8YcBnQn_gGa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "Kr0EC3AzBK0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SEQ_LENGTH = 15      # Sequence length for the model (Must match output length for this task)\n",
        "MAX_WORD_LENGTH = 20       # Maximum length for synthetic word generation\n",
        "NUM_SAMPLES = 5000\n",
        "VOCAB_SIZE = 30\n",
        "EMBEDDING_DIM = 32\n",
        "LSTM_UNITS = 64\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "PATIENCE = 5\n",
        "\n",
        "# Special Token IDs\n",
        "PAD_ID = 0\n",
        "SOS_ID = 1 # Start of Sequence Token (NOT used in this specific Many-to-Many Aligned task, but kept for concept)\n",
        "EOS_ID = 2 # End of Sequence Token (Used to signify end of the actual word)\n",
        "# Actual letters will map from ID 3 onwards"
      ],
      "metadata": {
        "id": "2pagwcTGatIC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NYIdv--z_r3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Generation"
      ],
      "metadata": {
        "id": "Xw9EEMH8bLtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Alphabet and Mappings\n",
        "all_letters = list(string.ascii_lowercase)\n",
        "char_to_int = {char: i + 3 for i, char in enumerate(all_letters)} # Letters start from ID 3\n",
        "char_to_int['<UNK>'] = 1\n",
        "char_to_int['<PAD>'] = 0\n",
        "\n",
        "def get_char_id(char):\n",
        "    return char_to_int.get(char, char_to_int[\"<UNK>\"])\n",
        "\n",
        "def generate_synthetic_word_data(num_samples, max_len):\n",
        "    \"\"\"Generates words and their corresponding Vowel/Consonant labels.\"\"\"\n",
        "    X_sequences, Y_sequences = [], []\n",
        "    VOWELS = set('aeiou')\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        word_len = random.randint(5, max_len)\n",
        "        word = ''.join(random.choices(all_letters, k=word_len))\n",
        "\n",
        "        # Target Y: The label sequence (0 or 1) for EACH character\n",
        "        labels = [1 if char in VOWELS else 0 for char in word]\n",
        "\n",
        "        # Input X: The sequence of character IDs\n",
        "        word_ids = [get_char_id(char) for char in word]\n",
        "\n",
        "        X_sequences.append(word_ids)\n",
        "        Y_sequences.append(labels)\n",
        "\n",
        "    # Pad sequences to a fixed length (MAX_WORD_LENGTH)\n",
        "    X_padded = pad_sequences(X_sequences, maxlen=max_len, padding='post', dtype='int32', value=PAD_ID)\n",
        "    # Y must also be padded, but with PAD_ID (0), as it corresponds to an input character\n",
        "    Y_padded = pad_sequences(Y_sequences, maxlen=max_len, padding='post', value=PAD_ID)\n",
        "\n",
        "    return X_padded, Y_padded\n",
        "\n",
        "# Generate Data\n",
        "X, Y = generate_synthetic_word_data(NUM_SAMPLES, MAX_WORD_LENGTH)\n",
        "Y = np.expand_dims(Y, axis=-1)  # ✅ تبدیل (N, 20) → (N, 20, 1)\n",
        "print(f\"Generated data shape: X={X.shape}, Y={Y.shape}\")\n",
        "\n",
        "# Split into Train/Test sets (STRATIFY is difficult here, so we skip it for simplicity/speed)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, Y, test_size=0.2, random_state=42\n",
        ")\n",
        "print(f\"Train set size: {len(X_train)}, Test set size: {len(X_test)}\")\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_train dtype:\", y_train.dtype)\n",
        "print(\"Model output shape:\", model.output_shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-OqGO6BKjO",
        "outputId": "e09e873a-49a7-4b6c-86eb-ee7927ac84fd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated data shape: X=(5000, 20), Y=(5000, 20, 1)\n",
            "Train set size: 4000, Test set size: 1000\n",
            "X_train shape: (4000, 20)\n",
            "y_train shape: (4000, 20, 1)\n",
            "y_train dtype: int32\n",
            "Model output shape: (None, 20, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition"
      ],
      "metadata": {
        "id": "tjMUiQd2dzly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# تعریف DROPOUT_RATE در Config (اگر ندارید)\n",
        "DROPOUT_RATE = 0.5\n",
        "\n",
        "input_layer = Input(shape=(MAX_WORD_LENGTH,), name='input_layer')\n",
        "embedding_layer = Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True)(input_layer)\n",
        "\n",
        "bilstm_layer = Bidirectional(\n",
        "    GRU(LSTM_UNITS, recurrent_dropout=0.2, return_sequences=True),\n",
        "    name='bilstm_layer'\n",
        ")(embedding_layer)\n",
        "\n",
        "dropout_layer = Dropout(DROPOUT_RATE)(bilstm_layer)\n",
        "output_layer = Dense(1, activation='sigmoid', name='output_layer')(dropout_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "MvrHwGX5BKgJ",
        "outputId": "0b9454cb-6c9c-4d08-c1e5-bd066d27aef3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_8         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │        \u001b[38;5;34m960\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_12        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m37,632\u001b[0m │ embedding_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ bilstm_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │        \u001b[38;5;34m129\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_8         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_12        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">37,632</span> │ embedding_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bilstm_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m38,721\u001b[0m (151.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,721</span> (151.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m38,721\u001b[0m (151.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,721</span> (151.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ePZjVHTaBKdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "NYQCT7U1eCKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        'best_vowel_model.keras', save_best_only=True, monitor='val_accuracy', mode='max'),\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "# Note: X_train and y_train are both (N, MAX_WORD_LENGTH)\n",
        "history = model.fit(\n",
        "    X_train, y_train, # Input and Target have the same sequence length\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.15,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vHW9jQHBKap",
        "outputId": "71045876-921e-4ea4-ebcd-b78855d2c6e8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 62ms/step - accuracy: 0.9090 - loss: 0.3811 - val_accuracy: 1.0000 - val_loss: 0.0031\n",
            "Epoch 2/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 4.6174e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 5.5309e-04 - val_accuracy: 1.0000 - val_loss: 2.0161e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 2.7603e-04 - val_accuracy: 1.0000 - val_loss: 1.1692e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 1.7198e-04 - val_accuracy: 1.0000 - val_loss: 7.6478e-05\n",
            "Epoch 6/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.1555e-04 - val_accuracy: 1.0000 - val_loss: 5.4365e-05\n",
            "Epoch 7/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 9.0071e-05 - val_accuracy: 1.0000 - val_loss: 4.0574e-05\n",
            "Epoch 8/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 6.9774e-05 - val_accuracy: 1.0000 - val_loss: 3.1288e-05\n",
            "Epoch 9/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 5.2904e-05 - val_accuracy: 1.0000 - val_loss: 2.5001e-05\n",
            "Epoch 10/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 4.4398e-05 - val_accuracy: 1.0000 - val_loss: 2.0306e-05\n",
            "Epoch 11/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 3.5720e-05 - val_accuracy: 1.0000 - val_loss: 1.6879e-05\n",
            "Epoch 12/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 2.9527e-05 - val_accuracy: 1.0000 - val_loss: 1.4248e-05\n",
            "Epoch 13/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 2.7531e-05 - val_accuracy: 1.0000 - val_loss: 1.2068e-05\n",
            "Epoch 14/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 2.2595e-05 - val_accuracy: 1.0000 - val_loss: 1.0361e-05\n",
            "Epoch 15/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 2.1558e-05 - val_accuracy: 1.0000 - val_loss: 8.9378e-06\n",
            "Epoch 16/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.8525e-05 - val_accuracy: 1.0000 - val_loss: 7.7667e-06\n",
            "Epoch 17/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 1.5706e-05 - val_accuracy: 1.0000 - val_loss: 6.8096e-06\n",
            "Epoch 18/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 1.4889e-05 - val_accuracy: 1.0000 - val_loss: 5.9910e-06\n",
            "Epoch 19/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 1.3867e-05 - val_accuracy: 1.0000 - val_loss: 5.3008e-06\n",
            "Epoch 20/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.1954e-05 - val_accuracy: 1.0000 - val_loss: 4.7195e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A8PNOUZGeDaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "V0coN2UdeG7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "best_model = tf.keras.models.load_model('best_vowel_model.keras')\n",
        "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Generate detailed classification report\n",
        "y_pred_probs = best_model.predict(X_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Flatten y_test and y_pred before passing to classification_report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test.flatten(), y_pred.flatten(), target_names=['Consonant (0)', 'Vowel (1)']))\n",
        "\n",
        "# Example Inference\n",
        "test_word = \"programming\" # A word NOT seen during training\n",
        "# Must manually tokenize/pad the new word\n",
        "test_ids = [get_char_id(c) for c in test_word]\n",
        "test_padded = pad_sequences([test_ids], maxlen=MAX_WORD_LENGTH, padding='post')\n",
        "\n",
        "prediction_probs = best_model.predict(test_padded)\n",
        "# For sequence output, we check the probability for each position\n",
        "predicted_labels = (prediction_probs[0] > 0.5).astype(int)\n",
        "\n",
        "print(\"\\n--- Inference Example ---\")\n",
        "print(f\"Word: {test_word}\")\n",
        "print(f\"Prediction Shape (One per letter): {prediction_probs.shape}\")\n",
        "print(f\"Predicted Tags (0=C, 1=V): {predicted_labels.flatten()[:len(test_word)]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpWB9BEmeDXx",
        "outputId": "2c66ee5e-9c0d-4dac-a98d-38b37943b5f4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 100.00%\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Consonant (0)       1.00      1.00      1.00     17588\n",
            "    Vowel (1)       1.00      1.00      1.00      2412\n",
            "\n",
            "     accuracy                           1.00     20000\n",
            "    macro avg       1.00      1.00      1.00     20000\n",
            " weighted avg       1.00      1.00      1.00     20000\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\n",
            "--- Inference Example ---\n",
            "Word: programming\n",
            "Prediction Shape (One per letter): (1, 20, 1)\n",
            "Predicted Tags (0=C, 1=V): [0 0 1 0 0 1 0 0 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "at13Hs5LeDSf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}