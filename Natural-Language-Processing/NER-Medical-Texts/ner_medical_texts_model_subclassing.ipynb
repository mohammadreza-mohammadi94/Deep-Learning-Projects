{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhIN1+yP+AYMzPb/hRyLSo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/Deep-Learning-Projects/blob/main/NER-Medical-Texts/ner_medical_texts_model_subclassing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "bjx3dCUJFfa-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ooimdsMD3DD",
        "outputId": "aab89091-b7a3-4876-f6fd-0ca37e8df56a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets==3.6.0\n",
        "!pip install -q seqeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, losses, callbacks, regularizers\n",
        "from datasets import load_dataset\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from seqeval.metrics import f1_score, classification_report"
      ],
      "metadata": {
        "id": "MnSRdlY0Fkc7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "UoP-it2iGLAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 64\n",
        "EMBEDDING_DIM = 128\n",
        "RNN_UNITS = 128\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 15"
      ],
      "metadata": {
        "id": "AEGqOIjyFyJz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading & Preprocessing"
      ],
      "metadata": {
        "id": "CFMNnPEvGRvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data():\n",
        "    \"\"\"\n",
        "    Load dataset and convert tokens to int IDS with padding\n",
        "    \"\"\"\n",
        "    print(\">> Loading BC5CDR Dataset...\")\n",
        "    dataset = load_dataset(\"tner/bc5cdr\")\n",
        "\n",
        "    # Extract tokens & tags\n",
        "    train_sent, train_tags = dataset['train']['tokens'], dataset['train']['tags']\n",
        "    test_sent, test_tags = dataset['test']['tokens'], dataset['test']['tags']\n",
        "    val_sent, val_tags = dataset['validation']['tokens'], dataset['validation']['tags']\n",
        "\n",
        "    # Build Vocab\n",
        "    vocab = set(w for s in train_sent for w in s)\n",
        "    word2idx = {w: i for i, w in enumerate(sorted(list(vocab)))}\n",
        "    word2idx[\"[PAD]\"] = 0\n",
        "    word2idx[\"[UNK]\"] = 1\n",
        "\n",
        "    # Encoder and pad\n",
        "    def encode(sentences, tags_list):\n",
        "        X = [[word2idx.get(w, 1) for w in s] for s in sentences]\n",
        "        X_p = pad_sequences(\n",
        "            X, maxlen=MAX_LEN, padding='post'\n",
        "        )\n",
        "        y_p = pad_sequences(\n",
        "            tags_list, maxlen=MAX_LEN, padding='post', value=0\n",
        "        )\n",
        "        return X_p, np.array(y_p)\n",
        "\n",
        "    X_train, y_train = encode(train_sent, train_tags)\n",
        "    X_test, y_test = encode(test_sent, test_tags)\n",
        "    X_val, y_val = encode(val_sent, val_tags)\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), word2idx, test_sent\n",
        ""
      ],
      "metadata": {
        "id": "PdXOsaujGQsj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition"
      ],
      "metadata": {
        "id": "X3rJWnqCHfEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BioNERModel(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Bi-directional LSTM for medical entity recognition.\n",
        "    Inherits from tf.keras.Model for maximum flexibility.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, num_tags):\n",
        "        super(BioNERModel, self).__init__()\n",
        "        self.embedding = layers.Embedding(vocab_size, EMBEDDING_DIM, mask_zero=True)\n",
        "        self.spatial_dropout = layers.SpatialDropout1D(0.3)\n",
        "        self.bi_lstm = layers.Bidirectional(\n",
        "            layers.LSTM(\n",
        "                        RNN_UNITS,\n",
        "                        return_sequences=True,\n",
        "                        kernel_regularizer=regularizers.l2(1e-5), # Prevent weight explosion\n",
        "                        recurrent_regularizer=regularizers.l2(1e-5)\n",
        "                        )\n",
        "            )\n",
        "        self.dropout = layers.Dropout(0.3)\n",
        "        self.classifier = layers.Dense(num_tags)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.spatial_dropout(x, training=training)\n",
        "        x = self.bi_lstm(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "# Define Weighted Loss\n",
        "def get_weighted_loss(class_weights):\n",
        "    \"\"\"\n",
        "    Computes weighted cross-entropy while normalizing by the sum of weights.\n",
        "    Higher weights on index 1-4 increase the Recall for medical entities.\n",
        "    \"\"\"\n",
        "    def loss_fn(y_true, y_pred):\n",
        "        cce = losses.SparseCategoricalCrossentropy(from_logits=True, reduction=None)\n",
        "        loss_val = cce(y_true, y_pred)\n",
        "\n",
        "        weights = tf.gather(class_weights, tf.cast(y_true, tf.int32))\n",
        "        weighted_loss = loss_val * weights\n",
        "\n",
        "        # Safe normalization to prevent division by zero\n",
        "        return tf.reduce_sum(weighted_loss) / (tf.reduce_sum(weights) + 1e-8)\n",
        "\n",
        "    return loss_fn"
      ],
      "metadata": {
        "id": "cAaMnBvuGvxH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Pipeline"
      ],
      "metadata": {
        "id": "VTO5PFkXIMdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load data\n",
        "    (X_train, y_train), (X_val, y_val), (X_test, y_test), word2idx, original_test_tokens = prepare_data()\n",
        "\n",
        "    # Calculate Class Weights\n",
        "    # Standard 'O' gets 1.0, Chemicals get 6.0, Diseases get 8.0\n",
        "    # Higher weights for rarer entities to force the model to prioritize them\n",
        "    class_weights = tf.constant([1.0, 5.0, 5.0, 7.0, 7.0], dtype=tf.float32)\n",
        "\n",
        "    # Instantiate Model\n",
        "    num_tags = 5 # O, B-Chem, I-Chem, B-Dis, I-Dis\n",
        "    model = BioNERModel(len(word2idx), 5)\n",
        "\n",
        "    # Compile with SparseCategoricalAccuracy to avoid InvalidArgumentError\n",
        "    # Using explicit metric class is safer for Many-to-Many tasks\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "        loss=get_weighted_loss(class_weights),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')]\n",
        "    )\n",
        "\n",
        "    # Callbacks for training efficiency\n",
        "    cb_list = [\n",
        "        callbacks.EarlyStopping(\n",
        "            monitor='val_loss', patience=4, restore_best_weights=True),\n",
        "        callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss', factor=0.5, patience=1)\n",
        "    ]\n",
        "\n",
        "    # Model Training\n",
        "    print(\"\\n>> Training Model...\")\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=cb_list\n",
        "    )\n",
        "\n",
        "    # EVALUATION (SEQEVAL)\n",
        "    print(\"\\n>> Evaluating on Test Set...\")\n",
        "    logits = model.predict(X_test)\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Mapping for conversion\n",
        "    tag_idx_to_name = {0: \"O\", 1: \"B-CHM\", 2: \"I-CHM\", 3: \"B-DIS\", 4: \"I-DIS\"}\n",
        "\n",
        "    def get_real_tags(y_indices, original_sentences):\n",
        "        all_tags = []\n",
        "        for i, sentence in enumerate(original_sentences):\n",
        "            # Convert only valid tokens (ignore padding at the end)\n",
        "            length = len(sentence)\n",
        "            tags = [tag_idx_to_name[idx] for idx in y_indices[i][:length]]\n",
        "            all_tags.append(tags)\n",
        "        return all_tags\n",
        "\n",
        "    true_tags = get_real_tags(y_test, original_test_tokens)\n",
        "    pred_tags = get_real_tags(preds, original_test_tokens)\n",
        "\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(true_tags, pred_tags))\n",
        "\n",
        "\n",
        "# Execution\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByHsf3qMIGFK",
        "outputId": "3a0c2b5a-dcab-49f1-8b27-bd7423bc7de5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Loading BC5CDR Dataset...\n",
            "\n",
            ">> Training Model...\n",
            "Epoch 1/15\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 202ms/step - accuracy: 0.9544 - loss: 0.4959 - val_accuracy: 0.9758 - val_loss: 0.2557 - learning_rate: 0.0010\n",
            "Epoch 2/15\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 225ms/step - accuracy: 0.9840 - loss: 0.1429 - val_accuracy: 0.9820 - val_loss: 0.3139 - learning_rate: 0.0010\n",
            "Epoch 3/15\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 199ms/step - accuracy: 0.9451 - loss: 0.0704 - val_accuracy: 0.2967 - val_loss: 0.3625 - learning_rate: 5.0000e-04\n",
            "Epoch 4/15\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 218ms/step - accuracy: 0.3122 - loss: 0.0485 - val_accuracy: 0.2969 - val_loss: 0.3712 - learning_rate: 2.5000e-04\n",
            "Epoch 5/15\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 220ms/step - accuracy: 0.3159 - loss: 0.0451 - val_accuracy: 0.2976 - val_loss: 0.4010 - learning_rate: 1.2500e-04\n",
            "\n",
            ">> Evaluating on Test Set...\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         CHM       0.66      0.59      0.62      9602\n",
            "         DIS       0.64      0.46      0.54      2746\n",
            "\n",
            "   micro avg       0.65      0.56      0.60     12348\n",
            "   macro avg       0.65      0.53      0.58     12348\n",
            "weighted avg       0.65      0.56      0.60     12348\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mMMluAR3TFfJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}