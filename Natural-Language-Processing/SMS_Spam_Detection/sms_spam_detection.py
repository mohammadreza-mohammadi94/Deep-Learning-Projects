# -*- coding: utf-8 -*-
"""sms_spam_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uf-XRYUakZwj-BzOXg34OVCxyRTiDItI

# Download Dataset
"""

import os
from google.colab import userdata
os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')
os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')

!mkdir ~/kaggle
!cp kaggle.json ~/kaggle
!chmod 600 ~/kaggle/kaggle.json
!kaggle datasets download uciml/sms-spam-collection-dataset
!unzip sms-spam-collection-dataset.zip
!rm -rf sms-spam-collection-dataset.zip

"""# Libraries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re

from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import TextVectorization
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping



"""# Load & Process Data"""

df = pd.read_csv("spam.csv", encoding="latin-1")
df.info()

"""> Process Columns Name"""

# Remove cols and rename
df = df.drop(["Unnamed: 2", "Unnamed: 3", "Unnamed: 4"], axis=1)
df.columns = ["label", "text"]
df.head()

"""> Check Labels Distribution"""

plt.figure(figsize=(7,4))
sns.countplot(df["label"], width=0.3);
plt.title("Label Distribution");

"""> Convert Labels"""

df["label"] = df["label"].map({"ham": 0, "spam": 1})
df.head(2)



"""> Text Cleaning"""

def clean_text(text):
    text = text.lower()
    # Keep letters, numbers, and common symbols that might be relevant for spam
    text = re.sub(r'[^a-z0-9\s\$\!\?\%]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df['cleaned_text'] = df['text'].apply(clean_text)
df.head(2)



"""# Train/Test Split"""

X = df['cleaned_text']
y = df['label']

X_train_text, X_test_text, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print(f"X Train Shape: {X_train_text.shape}")
print(f"X Test Shape: {X_test_text.shape}")

"""# Tokenization"""

VOCAB_SIZE = 7000
sequence_lengths = X_train_text.apply(lambda x: len(x.split()))
MAX_SEQ_LEN = int(np.percentile(sequence_lengths, 95))
print(f"Max Sequence Length: {MAX_SEQ_LEN}")

vectorize_layer = TextVectorization(
    max_tokens=VOCAB_SIZE,
    output_mode='int',
    output_sequence_length=MAX_SEQ_LEN
)

vectorize_layer.adapt(X_train_text)



"""# Model Definition"""

EMBEDDING_DIM = 64
LSTM_UNITS = 32
DROPOUT_RATE = 0.5
L2_REG = 0.002

input_layer = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int64, name="input_layer")
embedding_layer = Embedding(
    input_dim=VOCAB_SIZE,
    output_dim=EMBEDDING_DIM,
    mask_zero=True,
    name="embedding_layer"
)(input_layer)
bilstm_layer = Bidirectional(LSTM(
    LSTM_UNITS,
    return_sequences=False,
    kernel_regularizer=tf.keras.regularizers.l2(L2_REG),
    recurrent_regularizer=tf.keras.regularizers.l2(L2_REG)
))(embedding_layer)
dropout_layer = Dropout(DROPOUT_RATE)(bilstm_layer)
output_layer = Dense(1, activation="sigmoid", name="output_layer")(dropout_layer)

model = Model(inputs=input_layer, outputs=output_layer)

# Compiling the model
model.compile(
    optimizer=Adam(learning_rate=0.0005),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()



"""# Training"""

y_train_array = y_train.to_numpy()
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train_array),
    y=y_train_array
)
class_weight_dict = dict(enumerate(class_weights))
print("\nComputed Class Weights to handle imbalance:")
print(f"Weight for class 0 (ham): {class_weight_dict[0]:.2f}")
print(f"Weight for class 1 (spam): {class_weight_dict[1]:.2f}")

EPOCHS = 10
BATCH_SIZE = 16
PATIENCE = 2

callbacks = [
    ModelCheckpoint('best_spam_model.keras', save_best_only=True, monitor='val_accuracy', mode='max'),
    EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)
]

# Split the training data into training and validation sets
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
    X_train_text, y_train,
    test_size=0.15, # Use the same split percentage as validation_split
    random_state=42,
    stratify=y_train
)

# Apply TextVectorization outside the model
X_train_vectorized = vectorize_layer(X_train_split)
X_val_vectorized = vectorize_layer(X_val_split)


# Create TensorFlow Datasets for training and validation from vectorized data
train_dataset = tf.data.Dataset.from_tensor_slices((X_train_vectorized, y_train_split)).batch(BATCH_SIZE)
val_dataset = tf.data.Dataset.from_tensor_slices((X_val_vectorized, y_val_split)).batch(BATCH_SIZE)


print("\n--- Starting Model Training ---")
history = model.fit(
    train_dataset,
    epochs=EPOCHS,
    validation_data=val_dataset, # Use validation_data instead of validation_split
    class_weight=class_weight_dict,
    callbacks=callbacks
)

"""# Evaluation"""

# Evaluate the model on the test set
X_test_vectorized = vectorize_layer(X_test_text)
test_dataset = tf.data.Dataset.from_tensor_slices((X_test_vectorized, y_test)).batch(BATCH_SIZE) # Create a dataset and batch it
test_loss, test_accuracy = model.evaluate(test_dataset) # Use the dataset for evaluation

print(f"\nTest Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Make predictions on the test set
# Create a dataset for predictions as well
predict_dataset = tf.data.Dataset.from_tensor_slices(X_test_vectorized).batch(BATCH_SIZE)
y_pred = model.predict(predict_dataset)
y_pred_classes = (y_pred > 0.5).astype(int)

# Display classification report and confusion matrix
print("\nClassification Report:")
print(classification_report(y_test, y_pred_classes))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_classes))

"""# Training History Visualization"""

# Plot training and validation loss
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Plot training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

