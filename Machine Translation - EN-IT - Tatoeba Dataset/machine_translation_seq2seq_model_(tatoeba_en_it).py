# -*- coding: utf-8 -*-
"""Machine Translation - Seq2Seq Model (Tatoeba EN-IT).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oEHIuK_EKemZqsk2tDrkSJv_5H8LUBWc

# Import Libraries & Setup Enviorment
"""

# Install libs
!pip install -q datasets

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding
from datasets import load_dataset

# Setup Warning
import warnings
warnings.filterwarnings("ignore")

"""# Functions"""

# Loads tatoba from hugging face
def load_tatoeba_dataset():
    dataset = load_dataset("tatoeba", lang1="en", lang2="it", trust_remote_code=True)
    data = dataset['train']
    # استخراج زوج‌های جمله (انگلیسی و ایتالیایی)
    input_texts = [item['translation']['en'] for item in data]
    target_texts = [item['translation']['it'] for item in data]
    return input_texts, target_texts

# Prepares data
def prepare_data(input_texts, target_texts):
    # Adds <start> & <end> token
    target_texts = ["<start> " + text + " <end>" for text in target_texts]
    print("Input Samples: ", input_texts[:5])
    print("Output Samples: ", target_texts[:5])

    # Tokenization for inputs (EN)
    input_tokenizer = Tokenizer()
    input_tokenizer.fit_on_texts(input_texts)
    input_sequences = input_tokenizer.texts_to_sequences(input_texts)
    input_vocab_size = len(input_tokenizer.word_index) + 1
    max_input_len = max(len(seq) for seq in input_sequences)

    # Tokenization for outputs (FA)
    target_tokenizer = Tokenizer()
    target_tokenizer.fit_on_texts(target_texts)
    target_sequences = target_tokenizer.texts_to_sequences(target_texts)
    target_vocab_size = (len(target_tokenizer.word_index) + 1)
    max_target_len = max(len(seq) for seq in target_texts)

    # Padding
    encoder_input_data = pad_sequences(input_sequences, maxlen=max_input_len, padding='post')
    decoder_input_data = pad_sequences([seq[:-1] for seq in target_sequences], maxlen=max_target_len-1, padding='post')
    decoder_target_data = pad_sequences([seq[1:] for seq in target_sequences], maxlen=max_target_len-1, padding='post')

    decoder_target_data = np.expand_dims(decoder_target_data, -1)
    print("Shape encoder_input_data:", encoder_input_data.shape)
    print("Shape decoder_input_data:", decoder_input_data.shape)
    print("Shape decoder_target_data:", decoder_target_data.shape)

    return (encoder_input_data, decoder_input_data, decoder_target_data,
            input_vocab_size, target_vocab_size, max_input_len, max_target_len,
            input_tokenizer, target_tokenizer)

# Encoder
def build_encoder(input_vocab_size, max_input_len, embedding_dim, units):
    encoder_inputs = Input(shape=(max_input_len, ))
    encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)
    encoder_lstm = LSTM(units, return_state=True)
    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)
    encoder_states = [state_h, state_c]
    return encoder_inputs, encoder_states

# Decoder
def build_decoder(target_vocab_size, max_target_len, embedding_dim , units, encoder_states):
    decoder_inputs = Input(shape=(max_target_len - 1, ))
    decoder_embedding = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)
    decoder_lstm = LSTM(units, return_sequences=True, return_state=True)
    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
    decoder_dense = Dense(target_vocab_size, activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)
    return decoder_inputs, decoder_outputs

def build_model(encoder_inputs, decoder_inputs, decoder_outputs):
    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

"""# Run Functions"""

# Loading Dataset
input_texts, target_texts = load_tatoeba_dataset()

# Sampling
input_texts = input_texts[:1000]
target_texts = target_texts[:1000]

# Preparing data
(encoder_input_data, decoder_input_data, decoder_target_data,
    input_vocab_size, target_vocab_size, max_input_len, max_target_len,
    input_tokenizer, target_tokenizer) = prepare_data(input_texts, target_texts)

# Params
EMBEDDING_DIM = 128
UNITS = 256

# Encoder
encoder_inputs, encoder_states = build_encoder(input_vocab_size, max_input_len, EMBEDDING_DIM, UNITS)
# Decoder
decoder_inputs, decoder_outputs = build_decoder(target_vocab_size, max_target_len, EMBEDDING_DIM, UNITS, encoder_states)

# Build model
model = build_model(encoder_inputs, decoder_inputs, decoder_outputs)
model.summary()

# Train the model
model.fit(
    [encoder_input_data, decoder_input_data],
    decoder_target_data,
    batch_size=32,
    epochs=50,
    validation_split=0.2,
    verbose=1
)

# Save the model
model.save('seq2seq_model.h5')





