{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/Deep-Learning-Projects/blob/main/Text-Generation-Edgar-Allan-Poems/text_generation_edgar_allan_word_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ygTAxT4GypvM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "miemgJ5bx5_Y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "gc2WURGIyrNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = tf.keras.utils.get_file(\n",
        "    \"allan.txt\",\n",
        "    origin=\"https://www.gutenberg.org/cache/epub/10031/pg10031.txt\"\n",
        ")\n",
        "\n",
        "text = open(path, \"rb\").read().decode(encoding=\"utf-8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuKQlBA-yquW",
        "outputId": "0c245090-603c-40b5-d976-ded46ebf589e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.gutenberg.org/cache/epub/10031/pg10031.txt\n",
            "\u001b[1m408498/408498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "dPM_d7epzMTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the text\n",
        "def clean_text(raw_text):\n",
        "    \"\"\"\n",
        "    Cleans raw text data by removing Project Gutenberg headers/footers,\n",
        "    license lines, special characters, and normalizing whitespace.\n",
        "\n",
        "    Args:\n",
        "        raw_text (str): The raw text content read from a file.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned and normalized text.\n",
        "    \"\"\"\n",
        "    # Initial cleaning to remove Project Gutenberg headers and footers\n",
        "    start_marker = \"EDGAR ALLAN POE\"\n",
        "    end_marker = \"End of the Project Gutenberg\"\n",
        "    start_idx = raw_text.find(start_marker)\n",
        "    end_idx = raw_text.find(end_marker)\n",
        "\n",
        "    if start_idx != -1 and end_idx != -1:\n",
        "        text = raw_text[start_idx:end_idx]\n",
        "    else:\n",
        "        text = raw_text\n",
        "\n",
        "    # Remove lines containing 'GUTENBERG' (likely license/project information)\n",
        "    lines = [line for line in text.split('\\n') if \"GUTENBERG\" not in line.upper()]\n",
        "    text = \"\\n\".join(lines)\n",
        "\n",
        "    # Normalize the text\n",
        "    text = text.replace(\"\\r\", \"\") # Remove carriage returns\n",
        "    text = re.sub(r'\\[.*?\\]', '', text) # Remove text in square brackets (e.g., [illustration])\n",
        "    text = re.sub(r'[0-9]+\\.[A-Z]\\.[0-9]+', '', text) # Remove specific numbering patterns\n",
        "    text = text.lower() # Convert all text to lowercase\n",
        "\n",
        "    # Separate punctuation marks with spaces (important for tokenization and learning sentence structure)\n",
        "    text = re.sub(r'([.,!?();:])', r' \\1 ', text)\n",
        "    text = re.sub(r'\\s{2,}', ' ', text) # Replace multiple spaces with a single space\n",
        "\n",
        "    # Return the cleaned text, stripping leading/trailing whitespace\n",
        "    return text.strip()\n",
        "\n",
        "def create_vectorizer(text, max_tokens=10000):\n",
        "    \"\"\"\n",
        "    Creates a TextVectorization layer and adapts it to the provided text\n",
        "    to build a vocabulary.\n",
        "\n",
        "    Args:\n",
        "        text (str): The cleaned text corpus.\n",
        "        max_tokens (int, optional): The maximum number of tokens to include in the vocabulary.\n",
        "                                    Defaults to 10000.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - vectorizer (tf.keras.layers.TextVectorization): The adapted vectorization layer.\n",
        "            - vocab (list): The list of words in the vocabulary.\n",
        "    \"\"\"\n",
        "    # Note: output_sequence_length should be None to return the full sequence, not a fixed length\n",
        "    # We set standardize=None because we already cleaned and standardized the text manually.\n",
        "    vectorizer = tf.keras.layers.TextVectorization(\n",
        "        standardize=None,\n",
        "        max_tokens=max_tokens,\n",
        "        output_mode=\"int\", # Output integer indices for words\n",
        "        output_sequence_length=None\n",
        "    )\n",
        "    # Adapt the vectorizer to the entire text (treated as a single sample in a list)\n",
        "    vectorizer.adapt([text])\n",
        "\n",
        "    # Get the generated vocabulary\n",
        "    vocab = vectorizer.get_vocabulary()\n",
        "    return vectorizer, vocab\n",
        "\n",
        "def prepare_dataset(vectorizer, text, batch_size, seq_length):\n",
        "    \"\"\"\n",
        "    Prepares a TensorFlow dataset for training the language model.\n",
        "\n",
        "    Args:\n",
        "        vectorizer (tf.keras.layers.TextVectorization): The adapted text vectorizer.\n",
        "        text (str): The cleaned text corpus.\n",
        "        batch_size (int): The number of sequences per batch.\n",
        "        seq_length (int): The length of each input sequence.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: A TensorFlow dataset of (input_sequence, target_sequence) pairs.\n",
        "    \"\"\"\n",
        "    # Convert the entire text into numerical IDs using the vectorizer\n",
        "    # The output shape is (1, total_words), we need (total_words,)\n",
        "    full_text_ids = vectorizer([text])[0]\n",
        "\n",
        "    # Create a dataset from the numerical IDs\n",
        "    word_dataset = tf.data.Dataset.from_tensor_slices(full_text_ids)\n",
        "\n",
        "    # Window the dataset to create sequences of `seq_length + 1` words.\n",
        "    # `drop_remainder=True` ensures all batches have the same size.\n",
        "    sequences = word_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "    # Function to split each sequence into input (first `seq_length` words)\n",
        "    # and target (last `seq_length` words, shifted by one)\n",
        "    def split_input_target(seq):\n",
        "        input_text = seq[:-1] # All but the last word\n",
        "        target_text = seq[1:] # All but the first word\n",
        "        return input_text, target_text\n",
        "\n",
        "    # Apply the splitting function to each sequence in the dataset\n",
        "    dataset = sequences.map(split_input_target)\n",
        "\n",
        "    # Shuffle, batch, and prefetch the dataset for efficient training.\n",
        "    # Shuffling is crucial for stateless RNN training.\n",
        "    dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size, stateful=False):\n",
        "    \"\"\"\n",
        "    Builds a Sequential Keras model for text generation using LSTM layers.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): The size of the vocabulary (number of unique words).\n",
        "        embedding_dim (int): The dimension of the word embeddings.\n",
        "        rnn_units (int): The number of units in the LSTM layers.\n",
        "        batch_size (int): The batch size for training or inference.\n",
        "        stateful (bool, optional): If True, the LSTM layers will maintain their internal states\n",
        "                                   across batch iterations. Defaults to False (stateless).\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: The compiled Keras sequential model.\n",
        "    \"\"\"\n",
        "    # If stateful, the input layer needs a defined batch_shape.\n",
        "    # Otherwise, it can infer the batch size (shape=(None,)).\n",
        "    if stateful:\n",
        "        input_layer = tf.keras.Input(batch_shape=(batch_size, None))\n",
        "    else:\n",
        "        input_layer = tf.keras.Input(shape=(None,))\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        input_layer,\n",
        "        # Embedding layer converts word indices into dense vectors\n",
        "        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
        "\n",
        "        # First LSTM layer with return_sequences=True to pass output to next LSTM\n",
        "        tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=stateful),\n",
        "        tf.keras.layers.Dropout(0.3), # Dropout for regularization\n",
        "\n",
        "        # Second LSTM layer\n",
        "        tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=stateful),\n",
        "        tf.keras.layers.Dropout(0.3), # Dropout for regularization\n",
        "\n",
        "        # Third LSTM layer (can be the last with return_sequences=True or False depending on next layer)\n",
        "        tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=stateful),\n",
        "\n",
        "        # Dense output layer to predict the probability distribution over the vocabulary.\n",
        "        # No softmax needed here if using SparseCategoricalCrossentropy(from_logits=True).\n",
        "        tf.keras.layers.Dense(units=vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def generate_text(model, vectorizer, start_string, num_generate=200, temp=0.5):\n",
        "    \"\"\"\n",
        "    Generates text using the trained language model.\n",
        "\n",
        "    Args:\n",
        "        model (tf.keras.Model): The trained text generation model (usually stateful).\n",
        "        vectorizer (tf.keras.layers.TextVectorization): The text vectorization layer.\n",
        "        start_string (str): The initial string to start text generation from.\n",
        "        num_generate (int, optional): The number of words to generate. Defaults to 200.\n",
        "        temp (float, optional): The sampling temperature. Higher values make predictions more random.\n",
        "                               Defaults to 0.7.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text combined with the start_string.\n",
        "    \"\"\"\n",
        "    # Convert the start_string into numerical IDs\n",
        "    input_ids = vectorizer([start_string])\n",
        "    input_eval = input_ids # This will be updated with each generated word\n",
        "\n",
        "    vocab = vectorizer.get_vocabulary()\n",
        "    text_generated = []\n",
        "\n",
        "    # Reset the states of the LSTM layers before starting generation\n",
        "    # This is crucial for stateful models to start fresh for a new sequence.\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'reset_states'):\n",
        "            layer.reset_states()\n",
        "\n",
        "    # Loop to generate `num_generate` words\n",
        "    for i in range(num_generate):\n",
        "        # Get model predictions for the current input_eval\n",
        "        predictions = model(input_eval)\n",
        "\n",
        "        # We are interested in the prediction for the last word in the sequence.\n",
        "        # tf.squeeze removes the batch dimension (1,) leaving (seq_len, vocab_size).\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # Apply temperature to the logits to control randomness.\n",
        "        # Higher temperature leads to more diverse (and sometimes less coherent) text.\n",
        "        predictions = predictions / temp\n",
        "\n",
        "        # Sample the next word ID from the probability distribution (categorical).\n",
        "        # [-1, 0] selects the last predicted word's ID.\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Convert the predicted ID back to a word, if it's a valid vocabulary token.\n",
        "        # Skip unknown tokens (ID 0) and padding tokens (ID 1).\n",
        "        if predicted_id < len(vocab) and predicted_id > 1:\n",
        "            predicted_word = vocab[predicted_id]\n",
        "            text_generated.append(predicted_word)\n",
        "\n",
        "        # The predicted word becomes the new input for the next step.\n",
        "        # We expand its dimension to match the expected input shape of the model.\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    # Combine the starting string with the generated words and return.\n",
        "    return start_string + \" \" + \" \".join(text_generated)\n"
      ],
      "metadata": {
        "id": "aq0AQjuhzKTK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Oi7sW392Sla"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution"
      ],
      "metadata": {
        "id": "rxzQURgmZZ_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load Data: Download the text corpus for Edgar Allan Poe's works.\n",
        "    # This ensures the code is self-contained and runnable.\n",
        "    path = tf.keras.utils.get_file(\n",
        "        \"allan.txt\",\n",
        "        origin=\"https://www.gutenberg.org/cache/epub/10031/pg10031.txt\"\n",
        "    )\n",
        "    # Read the downloaded text file and decode it using UTF-8 encoding.\n",
        "    raw_text = open(path, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "\n",
        "    # Cleaning: Process the raw text to remove noise and standardize it.\n",
        "    # This step involves removing headers, footers, special characters, and normalizing whitespace.\n",
        "    cleaned_text = clean_text(raw_text)\n",
        "    print(f\"[INFO] Text cleaned. Length: {len(cleaned_text)}\")\n",
        "\n",
        "    # Vectorization: Convert the cleaned text into numerical representations.\n",
        "    # A TextVectorization layer is used to create a vocabulary and map words to integer IDs.\n",
        "    # Limit the vocabulary size for faster training and better focus on common words.\n",
        "    MAX_TOKENS = 5000\n",
        "    vectorizer, vocab_list = create_vectorizer(cleaned_text, MAX_TOKENS)\n",
        "    print(f\"[INFO] Vocab size: {len(vocab_list)}\")\n",
        "\n",
        "    # Dataset Preparation: Create a TensorFlow dataset suitable for training.\n",
        "    # This involves splitting the text into sequences of a defined length and batching them.\n",
        "    BATCH_SIZE = 64 # Number of sequences processed in each training step.\n",
        "    SEQ_LENGTH = 30 # Length of input sequences for the model.\n",
        "    dataset = prepare_dataset(vectorizer, cleaned_text, BATCH_SIZE, SEQ_LENGTH)\n",
        "    print(\"[INFO] Dataset prepared.\")\n",
        "\n",
        "    # Train Model (Stateless): Build and train the LSTM-based language model.\n",
        "    # The model learns to predict the next word in a sequence.\n",
        "    EMBEDDING_DIM = 256 # Dimension of word embeddings.\n",
        "    RNN_UNITS = 512 # Number of units in the LSTM layers.\n",
        "    vocab_size = len(vocab_list) # Total number of unique words in the vocabulary.\n",
        "\n",
        "    # Build the model in a stateless manner for initial training.\n",
        "    model = build_model(vocab_size, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE, stateful=False)\n",
        "\n",
        "    # Compile the model with an optimizer and a loss function.\n",
        "    # SparseCategoricalCrossentropy is suitable for integer-encoded labels.\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    )\n",
        "    model.summary()\n",
        "\n",
        "    # Callbacks: Define actions to be performed during training.\n",
        "    # These include saving model checkpoints, early stopping, and learning rate reduction.\n",
        "    checkpoint_path = \"training_checkpoints/ckpt_{epoch}.weights.h5\"\n",
        "    # Create a directory for checkpoints if it doesn't exist.\n",
        "    if not os.path.exists(\"training_checkpoints\"):\n",
        "        os.makedirs(\"training_checkpoints\")\n",
        "\n",
        "    callbacks = [\n",
        "        # Stop training if the loss doesn't improve for 7 epochs, and restore the best weights.\n",
        "        tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=7, restore_best_weights=True),\n",
        "        # Save model weights at each epoch, but only keep the best performing one based on loss.\n",
        "        tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, monitor=\"loss\", save_best_only=True),\n",
        "        # Reduce the learning rate if the loss plateaus for 3 epochs.\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=3)\n",
        "    ]\n",
        "\n",
        "    print(\"[INFO] Starting training...\")\n",
        "    # Train the model for a specified number of epochs using the prepared dataset and callbacks.\n",
        "    model.fit(dataset, epochs=300, callbacks=callbacks)\n",
        "\n",
        "    # Save the final trained weights of the model.\n",
        "    model.save_weights(\"final_weights.weights.h5\")\n",
        "    print(\"[INFO] Training finished.\")\n",
        "\n",
        "    # Inference Model (Stateful): Rebuild the model for text generation.\n",
        "    # For generation, the model needs to be stateful and process one word at a time (batch_size=1).\n",
        "    inference_model = build_model(vocab_size, EMBEDDING_DIM, RNN_UNITS, batch_size=1, stateful=True)\n",
        "    # Load the weights from the trained stateless model into the new stateful model.\n",
        "    inference_model.load_weights(\"final_weights.weights.h5\")\n",
        "    # Build the model with the correct input shape for inference (batch_size=1, any sequence length).\n",
        "    inference_model.build(tf.TensorShape([1, None]))\n",
        "    print(\"[INFO] Inference model ready.\")\n",
        "\n",
        "    # Generate Text: Use the trained stateful model to generate new text.\n",
        "    print(\"\\n\\n--- GENERATED TEXT ---\")\n",
        "    # Start generation with a seed string and generate a specified number of words.\n",
        "    generated = generate_text(inference_model, vectorizer, start_string=\"the raven sat\", num_generate=100)\n",
        "    print(generated)\n",
        "\n",
        "# Execute the main function when the script is run.\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N3KsAdh79t5t",
        "outputId": "6cd706b1-4bea-40f2-c048-f50e1df7b3ec"
      },
      "execution_count": 25,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Text cleaned. Length: 371446\n",
            "[INFO] Vocab size: 5000\n",
            "[INFO] Dataset prepared.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,565,000</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_10 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_30 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_31 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m2,099,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_32 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m2,099,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m)     │     \u001b[38;5;34m2,565,000\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,618,312</span> (36.69 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,618,312\u001b[0m (36.69 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,618,312</span> (36.69 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,618,312\u001b[0m (36.69 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Starting training...\n",
            "Epoch 1/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 7.3247 - learning_rate: 0.0010\n",
            "Epoch 2/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 168ms/step - loss: 6.0957 - learning_rate: 0.0010\n",
            "Epoch 3/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 6.0623 - learning_rate: 0.0010\n",
            "Epoch 4/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 6.0401 - learning_rate: 0.0010\n",
            "Epoch 5/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - loss: 6.0440 - learning_rate: 0.0010\n",
            "Epoch 6/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 6.0556 - learning_rate: 0.0010\n",
            "Epoch 7/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 6.0384 - learning_rate: 0.0010\n",
            "Epoch 8/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 6.0102 - learning_rate: 0.0010\n",
            "Epoch 9/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - loss: 6.0044 - learning_rate: 0.0010\n",
            "Epoch 10/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 6.0161 - learning_rate: 0.0010\n",
            "Epoch 11/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 5.9959 - learning_rate: 0.0010\n",
            "Epoch 12/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 73ms/step - loss: 6.0065 - learning_rate: 0.0010\n",
            "Epoch 13/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 6.0067 - learning_rate: 0.0010\n",
            "Epoch 14/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 6.0123 - learning_rate: 0.0010\n",
            "Epoch 15/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 6.0191 - learning_rate: 0.0010\n",
            "Epoch 16/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 6.0028 - learning_rate: 0.0010\n",
            "Epoch 17/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 5.9812 - learning_rate: 0.0010\n",
            "Epoch 18/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - loss: 5.9935 - learning_rate: 0.0010\n",
            "Epoch 19/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - loss: 5.9880 - learning_rate: 0.0010\n",
            "Epoch 20/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - loss: 5.9589 - learning_rate: 0.0010\n",
            "Epoch 21/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - loss: 5.9564 - learning_rate: 0.0010\n",
            "Epoch 22/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - loss: 5.9475 - learning_rate: 0.0010\n",
            "Epoch 23/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - loss: 5.9009 - learning_rate: 0.0010\n",
            "Epoch 24/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 5.8144 - learning_rate: 0.0010\n",
            "Epoch 25/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 5.7585 - learning_rate: 0.0010\n",
            "Epoch 26/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 5.6992 - learning_rate: 0.0010\n",
            "Epoch 27/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 5.6372 - learning_rate: 0.0010\n",
            "Epoch 28/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 5.5799 - learning_rate: 0.0010\n",
            "Epoch 29/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 5.5285 - learning_rate: 0.0010\n",
            "Epoch 30/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 5.4826 - learning_rate: 0.0010\n",
            "Epoch 31/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 5.4372 - learning_rate: 0.0010\n",
            "Epoch 32/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 5.4046 - learning_rate: 0.0010\n",
            "Epoch 33/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 5.3488 - learning_rate: 0.0010\n",
            "Epoch 34/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 5.3353 - learning_rate: 0.0010\n",
            "Epoch 35/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 5.2825 - learning_rate: 0.0010\n",
            "Epoch 36/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 5.2492 - learning_rate: 0.0010\n",
            "Epoch 37/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 5.2216 - learning_rate: 0.0010\n",
            "Epoch 38/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 5.1729 - learning_rate: 0.0010\n",
            "Epoch 39/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 5.1365 - learning_rate: 0.0010\n",
            "Epoch 40/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 5.1137 - learning_rate: 0.0010\n",
            "Epoch 41/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 5.0826 - learning_rate: 0.0010\n",
            "Epoch 42/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 5.0568 - learning_rate: 0.0010\n",
            "Epoch 43/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - loss: 5.0203 - learning_rate: 0.0010\n",
            "Epoch 44/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 4.9971 - learning_rate: 0.0010\n",
            "Epoch 45/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 4.9786 - learning_rate: 0.0010\n",
            "Epoch 46/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 4.9353 - learning_rate: 0.0010\n",
            "Epoch 47/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 4.9077 - learning_rate: 0.0010\n",
            "Epoch 48/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 4.8783 - learning_rate: 0.0010\n",
            "Epoch 49/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 4.8588 - learning_rate: 0.0010\n",
            "Epoch 50/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 4.8518 - learning_rate: 0.0010\n",
            "Epoch 51/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 4.8591 - learning_rate: 0.0010\n",
            "Epoch 52/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 4.8160 - learning_rate: 0.0010\n",
            "Epoch 53/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 4.7894 - learning_rate: 0.0010\n",
            "Epoch 54/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 4.7748 - learning_rate: 0.0010\n",
            "Epoch 55/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 4.7066 - learning_rate: 0.0010\n",
            "Epoch 56/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 4.6957 - learning_rate: 0.0010\n",
            "Epoch 57/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 4.6448 - learning_rate: 0.0010\n",
            "Epoch 58/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 4.6363 - learning_rate: 0.0010\n",
            "Epoch 59/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 4.6152 - learning_rate: 0.0010\n",
            "Epoch 60/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 4.5624 - learning_rate: 0.0010\n",
            "Epoch 61/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 4.5460 - learning_rate: 0.0010\n",
            "Epoch 62/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 4.5130 - learning_rate: 0.0010\n",
            "Epoch 63/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 4.5026 - learning_rate: 0.0010\n",
            "Epoch 64/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 4.4821 - learning_rate: 0.0010\n",
            "Epoch 65/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 4.4795 - learning_rate: 0.0010\n",
            "Epoch 66/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 4.4092 - learning_rate: 0.0010\n",
            "Epoch 67/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 4.4088 - learning_rate: 0.0010\n",
            "Epoch 68/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 4.3635 - learning_rate: 0.0010\n",
            "Epoch 69/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - loss: 4.3035 - learning_rate: 0.0010\n",
            "Epoch 70/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 4.2755 - learning_rate: 0.0010\n",
            "Epoch 71/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 4.2493 - learning_rate: 0.0010\n",
            "Epoch 72/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 4.2213 - learning_rate: 0.0010\n",
            "Epoch 73/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 4.1886 - learning_rate: 0.0010\n",
            "Epoch 74/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 4.1520 - learning_rate: 0.0010\n",
            "Epoch 75/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 73ms/step - loss: 4.1209 - learning_rate: 0.0010\n",
            "Epoch 76/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - loss: 4.0731 - learning_rate: 0.0010\n",
            "Epoch 77/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 4.0359 - learning_rate: 0.0010\n",
            "Epoch 78/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 4.0142 - learning_rate: 0.0010\n",
            "Epoch 79/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 3.9774 - learning_rate: 0.0010\n",
            "Epoch 80/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - loss: 3.9690 - learning_rate: 0.0010\n",
            "Epoch 81/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 3.9238 - learning_rate: 0.0010\n",
            "Epoch 82/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 3.8866 - learning_rate: 0.0010\n",
            "Epoch 83/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 72ms/step - loss: 3.8407 - learning_rate: 0.0010\n",
            "Epoch 84/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 3.7998 - learning_rate: 0.0010\n",
            "Epoch 85/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 3.7811 - learning_rate: 0.0010\n",
            "Epoch 86/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 3.7460 - learning_rate: 0.0010\n",
            "Epoch 87/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 3.7200 - learning_rate: 0.0010\n",
            "Epoch 88/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 3.6776 - learning_rate: 0.0010\n",
            "Epoch 89/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 3.6551 - learning_rate: 0.0010\n",
            "Epoch 90/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 3.6160 - learning_rate: 0.0010\n",
            "Epoch 91/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 3.5805 - learning_rate: 0.0010\n",
            "Epoch 92/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 3.6018 - learning_rate: 0.0010\n",
            "Epoch 93/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 3.5285 - learning_rate: 0.0010\n",
            "Epoch 94/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 3.5050 - learning_rate: 0.0010\n",
            "Epoch 95/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 3.4634 - learning_rate: 0.0010\n",
            "Epoch 96/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - loss: 3.4664 - learning_rate: 0.0010\n",
            "Epoch 97/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 3.4203 - learning_rate: 0.0010\n",
            "Epoch 98/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - loss: 3.3793 - learning_rate: 0.0010\n",
            "Epoch 99/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 3.3648 - learning_rate: 0.0010\n",
            "Epoch 100/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 3.3264 - learning_rate: 0.0010\n",
            "Epoch 101/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 3.2730 - learning_rate: 0.0010\n",
            "Epoch 102/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 3.2550 - learning_rate: 0.0010\n",
            "Epoch 103/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - loss: 3.2143 - learning_rate: 0.0010\n",
            "Epoch 104/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 3.1657 - learning_rate: 0.0010\n",
            "Epoch 105/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 3.1540 - learning_rate: 0.0010\n",
            "Epoch 106/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 3.1292 - learning_rate: 0.0010\n",
            "Epoch 107/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 3.0854 - learning_rate: 0.0010\n",
            "Epoch 108/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 3.0473 - learning_rate: 0.0010\n",
            "Epoch 109/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 3.0233 - learning_rate: 0.0010\n",
            "Epoch 110/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - loss: 2.9991 - learning_rate: 0.0010\n",
            "Epoch 111/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 2.9682 - learning_rate: 0.0010\n",
            "Epoch 112/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 2.9114 - learning_rate: 0.0010\n",
            "Epoch 113/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 2.9166 - learning_rate: 0.0010\n",
            "Epoch 114/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 2.9063 - learning_rate: 0.0010\n",
            "Epoch 115/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 2.8867 - learning_rate: 0.0010\n",
            "Epoch 116/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 2.9501 - learning_rate: 0.0010\n",
            "Epoch 117/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 2.8850 - learning_rate: 0.0010\n",
            "Epoch 118/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 2.8084 - learning_rate: 5.0000e-04\n",
            "Epoch 119/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 2.7145 - learning_rate: 5.0000e-04\n",
            "Epoch 120/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - loss: 2.7010 - learning_rate: 5.0000e-04\n",
            "Epoch 121/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 2.6877 - learning_rate: 5.0000e-04\n",
            "Epoch 122/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 2.6615 - learning_rate: 5.0000e-04\n",
            "Epoch 123/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 2.6471 - learning_rate: 5.0000e-04\n",
            "Epoch 124/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 2.6383 - learning_rate: 5.0000e-04\n",
            "Epoch 125/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 2.6021 - learning_rate: 5.0000e-04\n",
            "Epoch 126/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 2.5780 - learning_rate: 5.0000e-04\n",
            "Epoch 127/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 2.5868 - learning_rate: 5.0000e-04\n",
            "Epoch 128/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 2.5820 - learning_rate: 5.0000e-04\n",
            "Epoch 129/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 2.5733 - learning_rate: 5.0000e-04\n",
            "Epoch 130/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 2.5229 - learning_rate: 5.0000e-04\n",
            "Epoch 131/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 2.5203 - learning_rate: 5.0000e-04\n",
            "Epoch 132/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 2.4859 - learning_rate: 5.0000e-04\n",
            "Epoch 133/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 2.5013 - learning_rate: 5.0000e-04\n",
            "Epoch 134/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 2.4721 - learning_rate: 5.0000e-04\n",
            "Epoch 135/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 2.4380 - learning_rate: 5.0000e-04\n",
            "Epoch 136/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 2.4606 - learning_rate: 5.0000e-04\n",
            "Epoch 137/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 2.4427 - learning_rate: 5.0000e-04\n",
            "Epoch 138/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 2.3854 - learning_rate: 5.0000e-04\n",
            "Epoch 139/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 2.3977 - learning_rate: 5.0000e-04\n",
            "Epoch 140/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - loss: 2.3681 - learning_rate: 5.0000e-04\n",
            "Epoch 141/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 2.3419 - learning_rate: 5.0000e-04\n",
            "Epoch 142/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 2.3397 - learning_rate: 5.0000e-04\n",
            "Epoch 143/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 2.3284 - learning_rate: 5.0000e-04\n",
            "Epoch 144/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 2.2783 - learning_rate: 5.0000e-04\n",
            "Epoch 145/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 2.2732 - learning_rate: 5.0000e-04\n",
            "Epoch 146/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 2.2726 - learning_rate: 5.0000e-04\n",
            "Epoch 147/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 2.2482 - learning_rate: 5.0000e-04\n",
            "Epoch 148/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 2.2187 - learning_rate: 5.0000e-04\n",
            "Epoch 149/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 2.2609 - learning_rate: 5.0000e-04\n",
            "Epoch 150/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 2.2108 - learning_rate: 5.0000e-04\n",
            "Epoch 151/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 2.1972 - learning_rate: 5.0000e-04\n",
            "Epoch 152/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 2.1883 - learning_rate: 5.0000e-04\n",
            "Epoch 153/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 76ms/step - loss: 2.1636 - learning_rate: 5.0000e-04\n",
            "Epoch 154/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 2.1424 - learning_rate: 5.0000e-04\n",
            "Epoch 155/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 2.1806 - learning_rate: 5.0000e-04\n",
            "Epoch 156/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 2.1252 - learning_rate: 5.0000e-04\n",
            "Epoch 157/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 2.1392 - learning_rate: 5.0000e-04\n",
            "Epoch 158/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 2.0873 - learning_rate: 5.0000e-04\n",
            "Epoch 159/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 2.0944 - learning_rate: 5.0000e-04\n",
            "Epoch 160/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 2.0671 - learning_rate: 5.0000e-04\n",
            "Epoch 161/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 2.0786 - learning_rate: 5.0000e-04\n",
            "Epoch 162/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 2.0453 - learning_rate: 5.0000e-04\n",
            "Epoch 163/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 2.0138 - learning_rate: 5.0000e-04\n",
            "Epoch 164/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 2.0134 - learning_rate: 5.0000e-04\n",
            "Epoch 165/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 2.0089 - learning_rate: 5.0000e-04\n",
            "Epoch 166/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 2.0172 - learning_rate: 5.0000e-04\n",
            "Epoch 167/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.9619 - learning_rate: 5.0000e-04\n",
            "Epoch 168/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 1.9765 - learning_rate: 5.0000e-04\n",
            "Epoch 169/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 1.9616 - learning_rate: 5.0000e-04\n",
            "Epoch 170/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - loss: 1.9370 - learning_rate: 5.0000e-04\n",
            "Epoch 171/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 1.9356 - learning_rate: 5.0000e-04\n",
            "Epoch 172/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.9030 - learning_rate: 5.0000e-04\n",
            "Epoch 173/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 1.8751 - learning_rate: 5.0000e-04\n",
            "Epoch 174/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.8938 - learning_rate: 5.0000e-04\n",
            "Epoch 175/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 1.8764 - learning_rate: 5.0000e-04\n",
            "Epoch 176/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.8648 - learning_rate: 5.0000e-04\n",
            "Epoch 177/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 1.8400 - learning_rate: 5.0000e-04\n",
            "Epoch 178/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 1.8505 - learning_rate: 5.0000e-04\n",
            "Epoch 179/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 1.8396 - learning_rate: 5.0000e-04\n",
            "Epoch 180/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.8236 - learning_rate: 5.0000e-04\n",
            "Epoch 181/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - loss: 1.8210 - learning_rate: 5.0000e-04\n",
            "Epoch 182/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 1.7959 - learning_rate: 5.0000e-04\n",
            "Epoch 183/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 1.7843 - learning_rate: 5.0000e-04\n",
            "Epoch 184/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - loss: 1.7841 - learning_rate: 5.0000e-04\n",
            "Epoch 185/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 1.7660 - learning_rate: 5.0000e-04\n",
            "Epoch 186/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.7455 - learning_rate: 5.0000e-04\n",
            "Epoch 187/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.7635 - learning_rate: 5.0000e-04\n",
            "Epoch 188/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 72ms/step - loss: 1.7363 - learning_rate: 5.0000e-04\n",
            "Epoch 189/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 1.7206 - learning_rate: 5.0000e-04\n",
            "Epoch 190/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.6803 - learning_rate: 5.0000e-04\n",
            "Epoch 191/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 1.6726 - learning_rate: 5.0000e-04\n",
            "Epoch 192/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 1.6689 - learning_rate: 5.0000e-04\n",
            "Epoch 193/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 1.6816 - learning_rate: 5.0000e-04\n",
            "Epoch 194/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.6407 - learning_rate: 5.0000e-04\n",
            "Epoch 195/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 1.6553 - learning_rate: 5.0000e-04\n",
            "Epoch 196/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 74ms/step - loss: 1.6231 - learning_rate: 5.0000e-04\n",
            "Epoch 197/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.6270 - learning_rate: 5.0000e-04\n",
            "Epoch 198/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.6109 - learning_rate: 5.0000e-04\n",
            "Epoch 199/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 1.6000 - learning_rate: 5.0000e-04\n",
            "Epoch 200/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.5844 - learning_rate: 5.0000e-04\n",
            "Epoch 201/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 1.5573 - learning_rate: 5.0000e-04\n",
            "Epoch 202/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 1.5762 - learning_rate: 5.0000e-04\n",
            "Epoch 203/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.5540 - learning_rate: 5.0000e-04\n",
            "Epoch 204/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 1.5388 - learning_rate: 5.0000e-04\n",
            "Epoch 205/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 1.5339 - learning_rate: 5.0000e-04\n",
            "Epoch 206/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 1.5283 - learning_rate: 5.0000e-04\n",
            "Epoch 207/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.5206 - learning_rate: 5.0000e-04\n",
            "Epoch 208/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - loss: 1.4918 - learning_rate: 5.0000e-04\n",
            "Epoch 209/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 1.5047 - learning_rate: 5.0000e-04\n",
            "Epoch 210/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.4891 - learning_rate: 5.0000e-04\n",
            "Epoch 211/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.4997 - learning_rate: 5.0000e-04\n",
            "Epoch 212/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - loss: 1.4577 - learning_rate: 5.0000e-04\n",
            "Epoch 213/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.4553 - learning_rate: 5.0000e-04\n",
            "Epoch 214/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - loss: 1.4415 - learning_rate: 5.0000e-04\n",
            "Epoch 215/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.4242 - learning_rate: 5.0000e-04\n",
            "Epoch 216/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 1.4294 - learning_rate: 5.0000e-04\n",
            "Epoch 217/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - loss: 1.4051 - learning_rate: 5.0000e-04\n",
            "Epoch 218/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.4049 - learning_rate: 5.0000e-04\n",
            "Epoch 219/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.4020 - learning_rate: 5.0000e-04\n",
            "Epoch 220/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.4022 - learning_rate: 5.0000e-04\n",
            "Epoch 221/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 1.3937 - learning_rate: 5.0000e-04\n",
            "Epoch 222/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.3831 - learning_rate: 5.0000e-04\n",
            "Epoch 223/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.3538 - learning_rate: 5.0000e-04\n",
            "Epoch 224/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - loss: 1.3639 - learning_rate: 5.0000e-04\n",
            "Epoch 225/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 1.3470 - learning_rate: 5.0000e-04\n",
            "Epoch 226/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.3482 - learning_rate: 5.0000e-04\n",
            "Epoch 227/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.3213 - learning_rate: 5.0000e-04\n",
            "Epoch 228/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 1.3413 - learning_rate: 5.0000e-04\n",
            "Epoch 229/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.3219 - learning_rate: 5.0000e-04\n",
            "Epoch 230/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 1.3166 - learning_rate: 5.0000e-04\n",
            "Epoch 231/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.2910 - learning_rate: 5.0000e-04\n",
            "Epoch 232/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 1.2698 - learning_rate: 5.0000e-04\n",
            "Epoch 233/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.2687 - learning_rate: 5.0000e-04\n",
            "Epoch 234/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 1.2721 - learning_rate: 5.0000e-04\n",
            "Epoch 235/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.2625 - learning_rate: 5.0000e-04\n",
            "Epoch 236/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 1.2452 - learning_rate: 5.0000e-04\n",
            "Epoch 237/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.2444 - learning_rate: 5.0000e-04\n",
            "Epoch 238/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 1.2384 - learning_rate: 5.0000e-04\n",
            "Epoch 239/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 1.2344 - learning_rate: 5.0000e-04\n",
            "Epoch 240/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.2220 - learning_rate: 5.0000e-04\n",
            "Epoch 241/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.2044 - learning_rate: 5.0000e-04\n",
            "Epoch 242/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 1.1814 - learning_rate: 5.0000e-04\n",
            "Epoch 243/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 1.1795 - learning_rate: 5.0000e-04\n",
            "Epoch 244/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 1.1721 - learning_rate: 5.0000e-04\n",
            "Epoch 245/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.1643 - learning_rate: 5.0000e-04\n",
            "Epoch 246/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.1809 - learning_rate: 5.0000e-04\n",
            "Epoch 247/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 1.1615 - learning_rate: 5.0000e-04\n",
            "Epoch 248/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.1578 - learning_rate: 5.0000e-04\n",
            "Epoch 249/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.1225 - learning_rate: 5.0000e-04\n",
            "Epoch 250/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - loss: 1.1466 - learning_rate: 5.0000e-04\n",
            "Epoch 251/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 1.1204 - learning_rate: 5.0000e-04\n",
            "Epoch 252/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - loss: 1.1192 - learning_rate: 5.0000e-04\n",
            "Epoch 253/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 1.1066 - learning_rate: 5.0000e-04\n",
            "Epoch 254/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.0891 - learning_rate: 5.0000e-04\n",
            "Epoch 255/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 1.0891 - learning_rate: 5.0000e-04\n",
            "Epoch 256/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.0981 - learning_rate: 5.0000e-04\n",
            "Epoch 257/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 1.0751 - learning_rate: 5.0000e-04\n",
            "Epoch 258/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.0753 - learning_rate: 5.0000e-04\n",
            "Epoch 259/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 1.0578 - learning_rate: 5.0000e-04\n",
            "Epoch 260/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - loss: 1.0572 - learning_rate: 5.0000e-04\n",
            "Epoch 261/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.0564 - learning_rate: 5.0000e-04\n",
            "Epoch 262/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.0350 - learning_rate: 5.0000e-04\n",
            "Epoch 263/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 1.0330 - learning_rate: 5.0000e-04\n",
            "Epoch 264/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 1.0312 - learning_rate: 5.0000e-04\n",
            "Epoch 265/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 1.0218 - learning_rate: 5.0000e-04\n",
            "Epoch 266/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - loss: 1.0215 - learning_rate: 5.0000e-04\n",
            "Epoch 267/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 0.9973 - learning_rate: 5.0000e-04\n",
            "Epoch 268/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - loss: 0.9978 - learning_rate: 5.0000e-04\n",
            "Epoch 269/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.9875 - learning_rate: 5.0000e-04\n",
            "Epoch 270/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 0.9840 - learning_rate: 5.0000e-04\n",
            "Epoch 271/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 0.9790 - learning_rate: 5.0000e-04\n",
            "Epoch 272/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - loss: 0.9781 - learning_rate: 5.0000e-04\n",
            "Epoch 273/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.9788 - learning_rate: 5.0000e-04\n",
            "Epoch 274/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.9606 - learning_rate: 5.0000e-04\n",
            "Epoch 275/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 0.9558 - learning_rate: 5.0000e-04\n",
            "Epoch 276/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 0.9395 - learning_rate: 5.0000e-04\n",
            "Epoch 277/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.9395 - learning_rate: 5.0000e-04\n",
            "Epoch 278/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 0.9403 - learning_rate: 5.0000e-04\n",
            "Epoch 279/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - loss: 0.9289 - learning_rate: 5.0000e-04\n",
            "Epoch 280/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 0.9122 - learning_rate: 5.0000e-04\n",
            "Epoch 281/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.9090 - learning_rate: 5.0000e-04\n",
            "Epoch 282/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 0.8989 - learning_rate: 5.0000e-04\n",
            "Epoch 283/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 0.9102 - learning_rate: 5.0000e-04\n",
            "Epoch 284/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.8847 - learning_rate: 5.0000e-04\n",
            "Epoch 285/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.8981 - learning_rate: 5.0000e-04\n",
            "Epoch 286/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 0.8790 - learning_rate: 5.0000e-04\n",
            "Epoch 287/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.8643 - learning_rate: 5.0000e-04\n",
            "Epoch 288/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 0.8690 - learning_rate: 5.0000e-04\n",
            "Epoch 289/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.8583 - learning_rate: 5.0000e-04\n",
            "Epoch 290/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - loss: 0.8558 - learning_rate: 5.0000e-04\n",
            "Epoch 291/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 0.8659 - learning_rate: 5.0000e-04\n",
            "Epoch 292/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 0.8397 - learning_rate: 5.0000e-04\n",
            "Epoch 293/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.8240 - learning_rate: 5.0000e-04\n",
            "Epoch 294/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 0.8195 - learning_rate: 5.0000e-04\n",
            "Epoch 295/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 0.8091 - learning_rate: 5.0000e-04\n",
            "Epoch 296/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - loss: 0.8154 - learning_rate: 5.0000e-04\n",
            "Epoch 297/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 0.8177 - learning_rate: 5.0000e-04\n",
            "Epoch 298/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.8172 - learning_rate: 5.0000e-04\n",
            "Epoch 299/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.7889 - learning_rate: 5.0000e-04\n",
            "Epoch 300/300\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 0.7896 - learning_rate: 5.0000e-04\n",
            "[INFO] Training finished.\n",
            "[INFO] Inference model ready.\n",
            "\n",
            "\n",
            "--- GENERATED TEXT ---\n",
            "the raven sat was , and is a forbidden thing . in the same author of the lover , and we thus , in the same time , or of the truest , the foundation . and the rock which is edit . shew , by our epoch of the person engaged to edit to lurid which , this and love , or the first virgin limbs to fold in whitest sheets of lilies cold , \" and it is , and my author of the pestilence had be known , in composition , went to out , and the tall primeval\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4hWrz5Z-YqMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Vw-WSVhXhrW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}