{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxM/H0irrD/yc/2T/eWWWH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/Deep-Learning-Projects/blob/main/NER-CoNLL-Dataset/ner_conll_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "aT9j24FR3BMA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPU2lHlP2eOO",
        "outputId": "007ad406-4ca8-4273-abf3-b7a7389ec9b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets==3.6.0 seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libs"
      ],
      "metadata": {
        "id": "eWElKPjb3p_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import (layers, models,\n",
        "                              regularizers, callbacks,\n",
        "                              losses)\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from datasets import load_dataset\n",
        "from seqeval.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "JGZOUcw73DdP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Pipeline"
      ],
      "metadata": {
        "id": "uzzlXkDI30Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prepare_dataset():\n",
        "    \"\"\"\n",
        "    Loads the standard CoNLL-2003 dataset.\n",
        "    Tags: 0:O, 1:B-PER, 2:I-PER, 3:B-ORG, 4:I-ORG, 5:B-LOC, 6:I-LOC, 7:B-MISC, 8:I-MISC\n",
        "    \"\"\"\n",
        "    print(f\"[INFO] - Loading CoNLL dataset from HF...\")\n",
        "    dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "    # Load and split datasets\n",
        "    train_data = (dataset[\"train\"]['tokens'], dataset[\"train\"][\"ner_tags\"])\n",
        "    test_data = (dataset[\"test\"]['tokens'], dataset[\"test\"][\"ner_tags\"])\n",
        "    val_data = (dataset[\"validation\"]['tokens'], dataset[\"validation\"][\"ner_tags\"])\n",
        "\n",
        "    # Get tags\n",
        "    tags = dataset[\"train\"].features['ner_tags'].feature.names\n",
        "\n",
        "    return train_data, test_data, val_data, tags\n",
        "\n",
        "\n",
        "def create_vocab(token_list):\n",
        "    \"\"\"\n",
        "    Creates a mapping from word tokens to unique integers.\n",
        "    Index 0 is reserved for [PAD], Index 1 for [UNK].\n",
        "    \"\"\"\n",
        "    print(f\"[INFO] - Creating vocab and word2idx from train dataset...\")\n",
        "    vocab = set(word for sentence in token_list for word in sentence)\n",
        "    # Build lookup dict\n",
        "    word2idx = {word: id + 2 for id, word in enumerate(vocab)}\n",
        "    word2idx[\"[PAD]\"] = 0\n",
        "    word2idx[\"[UNK]\"] = 1\n",
        "    return word2idx\n",
        "\n",
        "class BiLstmNerModel(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM architecture for Sequence Tagging.\n",
        "    Uses Masking to ignore padded time-steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, num_tags, embedding_dim=128, units=128):\n",
        "        super(BiLstmNerModel, self).__init__()\n",
        "        self.embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
        "        self.bi_lstm = layers.Bidirectional(\n",
        "            layers.LSTM(units, return_sequences=True)\n",
        "        )\n",
        "        self.dropout = layers.Dropout(0.3)\n",
        "        self.classifier = layers.Dense(num_tags)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.bi_lstm(x)\n",
        "\n",
        "        if training:\n",
        "            x = self.dropout(x, training=training)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "def get_stable_weighted_loss(class_weights):\n",
        "    \"\"\"\n",
        "    Weighted Sparse Categorical Crossentropy normalized by batch weight sum.\n",
        "    Prevents gradient explosion and metric collapse.\n",
        "    \"\"\"\n",
        "    def loss_fn(y_true, y_pred):\n",
        "        # Calculate raw cross entropy (per token)\n",
        "        # from_logits=True is mandatory because we didn't add Softmax to the model\n",
        "        cce = losses.SparseCategoricalCrossentropy(from_logits=True, reduction=None)\n",
        "        raw_loss = cce(y_true, y_pred)\n",
        "\n",
        "        # Assign weights to each ground-truth tag\n",
        "        weights = tf.gather(class_weights, tf.cast(y_true, tf.int32))\n",
        "\n",
        "        # Normalize: total_weighted_loss / sum_of_weights\n",
        "        weighted_loss = tf.reduce_sum(raw_loss * weights)\n",
        "        total_weight = tf.reduce_sum(weights)\n",
        "\n",
        "        return weighted_loss / (total_weight + 1e-8)\n",
        "\n",
        "    return loss_fn"
      ],
      "metadata": {
        "id": "Fs6ZbHjU4jQJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Setup data\n",
        "    (train_s, train_t), (val_s, val_t), (test_s, test_t), tag_names = load_and_prepare_dataset()\n",
        "    word2idx = create_vocab(train_s)\n",
        "\n",
        "    # Configuration\n",
        "    MAX_LEN = 64\n",
        "    VOCAB_SIZE = len(word2idx)\n",
        "    NUM_TAGS = len(tag_names)\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    print(f'[INFO] - Vocab Size: {VOCAB_SIZE} | Num Tags: {NUM_TAGS}')\n",
        "\n",
        "    def vectorize_and_pad(sentences, tags_lists):\n",
        "        X = [[word2idx.get(w, 1) for w in s] for s in sentences]\n",
        "        X_p = pad_sequences(X, maxlen=MAX_LEN, padding=\"post\")\n",
        "        y_p = pad_sequences(tags_lists, maxlen=MAX_LEN, padding=\"post\")\n",
        "        return X_p, np.array(y_p)\n",
        "\n",
        "    X_train, y_train = vectorize_and_pad(train_s, train_t)\n",
        "    X_val, y_val = vectorize_and_pad(val_s, val_t)\n",
        "    X_test, y_test = vectorize_and_pad(test_s, test_t)\n",
        "\n",
        "    print(f'[INFO] X_train shape: {X_train.shape} | y_train shape: {y_train.shape}')\n",
        "\n",
        "    # Handling Imabalce (Weigheting\n",
        "    weights = np.ones(NUM_TAGS)\n",
        "    weights[1:] = 5.0\n",
        "    # class_weights = tf.constant(weights, dtype=tf.float32)\n",
        "\n",
        "    # Instantiating the Model\n",
        "    model = BiLstmNerModel(VOCAB_SIZE, NUM_TAGS)\n",
        "    print(\"[INFO] - BiLSTM NER Model Instantiated...\")\n",
        "    weights = tf.constant([1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0], dtype=tf.float32)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss=get_stable_weighted_loss(weights),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n",
        "    )\n",
        "    print(\"[INFO] - Model Summary:\")\n",
        "    model.summary()\n",
        "\n",
        "    # Training\n",
        "    print(\"[INFO] - Training Started...\")\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=12,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks = [\n",
        "            callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
        "            callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"[INFO] - Evaluating Model...\")\n",
        "    y_pred = model.predict(X_test)\n",
        "    pred_indices = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "\n",
        "    # Convert numeric IDs back to strings for seqeval\n",
        "    def idx_to_tag_seq(indices_batch, original_sentences):\n",
        "        converted = []\n",
        "        for i, sentence in enumerate(original_sentences):\n",
        "            length = len(sentence)\n",
        "            # Only keep tags for actual words (ignore padding)\n",
        "            tags = [tag_names[idx] for idx in indices_batch[i][:length]]\n",
        "            converted.append(tags)\n",
        "        return converted\n",
        "\n",
        "    y_true_str = idx_to_tag_seq(y_test, test_s)\n",
        "    y_pred_str = idx_to_tag_seq(pred_indices, test_s)\n",
        "\n",
        "    # Industry standard NER Classification Report\n",
        "    print(\"\\nFinal Classification Report (seqeval):\")\n",
        "    print(classification_report(y_true_str, y_pred_str))\n",
        "\n",
        "# Execution\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "id": "qcN8UUic3SkI",
        "outputId": "8844d7d5-07d0-44c4-d9cb-1c6e03566e24"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] - Loading CoNLL dataset from HF...\n",
            "[INFO] - Creating vocab and word2idx from train dataset...\n",
            "[INFO] - Vocab Size: 23625 | Num Tags: 9\n",
            "[INFO] X_train shape: (14041, 64) | y_train shape: (14041, 64)\n",
            "[INFO] - BiLSTM NER Model Instantiated...\n",
            "[INFO] - Model Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"bi_lstm_ner_model_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"bi_lstm_ner_model_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] - Training Started...\n",
            "Epoch 1/12\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.6830 - loss: 0.3106 - val_accuracy: 0.1916 - val_loss: 0.1070 - learning_rate: 0.0010\n",
            "Epoch 2/12\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.2239 - loss: 0.0419 - val_accuracy: 0.1951 - val_loss: 0.0968 - learning_rate: 0.0010\n",
            "Epoch 3/12\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.2262 - loss: 0.0139 - val_accuracy: 0.1955 - val_loss: 0.0984 - learning_rate: 0.0010\n",
            "Epoch 4/12\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.2255 - loss: 0.0070 - val_accuracy: 0.1949 - val_loss: 0.1033 - learning_rate: 5.0000e-04\n",
            "[INFO] - Evaluating Model...\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "\n",
            "Final Classification Report (seqeval):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.71      0.87      0.78      1831\n",
            "        MISC       0.78      0.75      0.76       922\n",
            "         ORG       0.70      0.71      0.70      1341\n",
            "         PER       0.79      0.83      0.81      1813\n",
            "\n",
            "   micro avg       0.74      0.80      0.77      5907\n",
            "   macro avg       0.74      0.79      0.76      5907\n",
            "weighted avg       0.74      0.80      0.77      5907\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "TIJ2Jxqr4Jct"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functional Implementation"
      ],
      "metadata": {
        "id": "3zhKdBCUCaRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, losses, callbacks, initializers\n",
        "from datasets import load_dataset\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from seqeval.metrics import classification_report\n",
        "import requests\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "nEMzMVV2AFuR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "qMFoQyZXDARk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_glove():\n",
        "    \"\"\"Download GloVe 100D vectors\"\"\"\n",
        "    # Define the directory where GloVe files will be extracted\n",
        "    glove_dir = \"glove_data\"\n",
        "    glove_file_path = os.path.join(glove_dir, \"glove.6B.100d.txt\")\n",
        "\n",
        "    if not os.path.exists(glove_file_path):\n",
        "        print(\">> Downloading GloVe embeddings (please wait)...\")\n",
        "        if not os.path.exists(glove_dir):\n",
        "            os.makedirs(glove_dir) # Create the directory if it doesn't exist\n",
        "\n",
        "        url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "        zip_file_name = os.path.join(glove_dir, \"glove.6B.zip\")\n",
        "        r = requests.get(url)\n",
        "        with open(zip_file_name, \"wb\") as f:\n",
        "            f.write(r.content)\n",
        "        with zipfile.ZipFile(zip_file_name, \"r\") as zip_ref:\n",
        "            # Extract to the created directory\n",
        "            zip_ref.extractall(glove_dir)\n",
        "        print(\">> GloVe embeddings downloaded and extracted.\")\n",
        "    return glove_file_path\n",
        "\n",
        "\n",
        "def load_embedding_matrix(word2idx, embedding_path, embedding_dim=100):\n",
        "    \"\"\"Load GloVe file and map it to out specific vocabulary.\"\"\"\n",
        "    print(\">>> Processing GloVe file...\")\n",
        "    embeddings_index = {}\n",
        "    with open(embedding_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    vocab_size = len(word2idx)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    hits, misses = 0, 0\n",
        "    for word, i in word2idx.items():\n",
        "        embedding_vector = embeddings_index.get(word.lower())\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            hits += 1\n",
        "        else:\n",
        "            embedding_matrix[i] = np.random.normal(scale=0.1, size=(embedding_dim,))\n",
        "            misses += 1\n",
        "\n",
        "    print(f\">>> Loaded {hits} words. {misses} words initialized randomly.\")\n",
        "    return embedding_matrix\n",
        "\n",
        "def prepare_data():\n",
        "    \"\"\"Load CoNLL-2003 and perform tokenization/padding\"\"\"\n",
        "    dataset = load_dataset(\"conll2003\")\n",
        "    tag_names = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "    # Build Vocab\n",
        "    train_tokens = dataset[\"train\"][\"tokens\"]\n",
        "    vocab = set(w for s in train_tokens for w in s)\n",
        "    word2idx = {word: i + 2 for i, word in enumerate(sorted(list(vocab)))}\n",
        "    word2idx[\"[PAD]\"] = 0\n",
        "    word2idx[\"[UNK]\"] = 1\n",
        "\n",
        "    MAX_LEN = 64\n",
        "\n",
        "    def vectorize(split):\n",
        "        X = [[word2idx.get(w, 1) for w in s] for s in dataset[split][\"tokens\"]]\n",
        "        X_p = pad_sequences(X, maxlen=MAX_LEN, padding=\"post\")\n",
        "        y_p = pad_sequences(dataset[split][\"ner_tags\"], maxlen=MAX_LEN, padding=\"post\", value=0)\n",
        "        return X_p, np.asarray(y_p)\n",
        "\n",
        "    X_train, y_train = vectorize('train')\n",
        "    X_val, y_val     = vectorize('validation')\n",
        "    X_test, y_test   = vectorize('test')\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), word2idx, tag_names, dataset['test']['tokens']\n",
        "\n",
        "\n",
        "def get_stable_weighted_loss(class_weights):\n",
        "    \"\"\"\n",
        "    Weighted Sparse Categorical Crossentropy normalized by batch weight sum.\n",
        "    Prevents gradient explosion and metric collapse.\n",
        "    \"\"\"\n",
        "    def loss_fn(y_true, y_pred):\n",
        "        # Calculate raw cross entropy (per token)\n",
        "        # from_logits=True is mandatory because we didn't add Softmax to the model\n",
        "        cce = losses.SparseCategoricalCrossentropy(from_logits=True, reduction=None)\n",
        "        raw_loss = cce(y_true, y_pred)\n",
        "\n",
        "        # Assign weights to each ground-truth tag\n",
        "        weights = tf.gather(class_weights, tf.cast(y_true, tf.int32))\n",
        "\n",
        "        # Normalize: total_weighted_loss / sum_of_weights\n",
        "        weighted_loss = tf.reduce_sum(raw_loss * weights)\n",
        "        total_weight = tf.reduce_sum(weights)\n",
        "\n",
        "        return weighted_loss / (total_weight + 1e-8)\n",
        "\n",
        "    return loss_fn\n",
        "\n",
        "def build_functional_ner(vocab_size, num_tags, maxlen, emb_matrix):\n",
        "    \"\"\"Construct Bi-LSTM model using Keras Functional API.\"\"\"\n",
        "    input_layer = layers.Input(shape=(maxlen,), name=\"input_ids\")\n",
        "\n",
        "    # Pre-trained Embedding Layer\n",
        "    # mask_zero=True tells the model to ignore index 0 during all calculations\n",
        "    x = layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=100,\n",
        "        embeddings_initializer=initializers.Constant(emb_matrix),\n",
        "        trainable=False, # Freeze GloVe weights initially\n",
        "        mask_zero=True,\n",
        "        name=\"glove_embeddings\"\n",
        "    )(input_layer)\n",
        "\n",
        "    # SpatialDropout1D is superior for sequential data (drops whole features)\n",
        "    x = layers.SpatialDropout1D(0.3)(x)\n",
        "\n",
        "    # Bidirectional LSTM for context awareness\n",
        "    x = layers.Bidirectional(\n",
        "        layers.LSTM(128, return_sequences=True),\n",
        "        name=\"bidirectional_lstm\"\n",
        "    )(x)\n",
        "\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "\n",
        "    # Classification Head: Outputting Logits (Linear activation)\n",
        "    # This is more stable when paired with from_logits=True loss\n",
        "    outputs = layers.Dense(num_tags, activation=None, name=\"logits_output\")(x)\n",
        "\n",
        "    model = models.Model(inputs=input_layer, outputs=outputs, name=\"Functional_NER_Model\")\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    # Load Data\n",
        "    (X_train, y_train), (X_val, y_val), (X_test, y_test), word2idx, tag_names, test_tokens = prepare_data()\n",
        "\n",
        "    # Setup Embeddings\n",
        "    glove_path = download_glove()\n",
        "    # glove_path = \"glove.6B.100d.txt\" # This line is redundant and should be removed\n",
        "    if os.path.exists(glove_path):\n",
        "        emb_matrix = load_embedding_matrix(word2idx, glove_path)\n",
        "    else:\n",
        "        print(\"!! GloVe file missing, using random initialization for training demonstration.\")\n",
        "        emb_matrix = np.random.uniform(-0.05, 0.05, (len(word2idx), 100))\n",
        "\n",
        "    # Define Class Weights to boost Recall (Tag 0 is 'O', 1-8 are entities)\n",
        "    # Higher weights (3.0) for entities forces the model to focus on them\n",
        "    weights = tf.constant([1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0], dtype=tf.float32)\n",
        "\n",
        "    # Build and Compile\n",
        "    model = build_functional_ner(len(word2idx), len(tag_names), 64, emb_matrix)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), # Slower LR for stability\n",
        "        loss=get_stable_weighted_loss(weights),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # Training with Callbacks\n",
        "    print(\"\\n>> Starting training...\")\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        batch_size=32,\n",
        "        epochs=12,\n",
        "        callbacks=[\n",
        "            callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n",
        "            callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # ======================================================================================\n",
        "    # 6. EVALUATION\n",
        "    # ======================================================================================\n",
        "    print(\"\\n>> Running final evaluation on test set...\")\n",
        "    raw_preds = model.predict(X_test)\n",
        "    pred_ids = np.argmax(raw_preds, axis=-1)\n",
        "\n",
        "    # Convert numeric IDs back to string labels for the seqeval report\n",
        "    def decode_tags(ids_batch, tokens_batch):\n",
        "        decoded_results = []\n",
        "        for i, sentence in enumerate(tokens_batch):\n",
        "            # Only decode tags for actual tokens (length-aware decoding)\n",
        "            length = len(sentence)\n",
        "            decoded_results.append([tag_names[idx] for idx in ids_batch[i][:length]])\n",
        "        return decoded_results\n",
        "\n",
        "    y_true_str = decode_tags(y_test, test_tokens)\n",
        "    y_pred_str = decode_tags(pred_ids, test_tokens)\n",
        "\n",
        "    print(\"\\nClassification Report (Entity-Level):\")\n",
        "    print(classification_report(y_true_str, y_pred_str))\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DnJ-6OZCC_jM",
        "outputId": "3ec3b7bf-25f7-4e2c-a63a-9856ac6d7f0d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Downloading GloVe embeddings (please wait)...\n",
            ">> GloVe embeddings downloaded and extracted.\n",
            ">>> Processing GloVe file...\n",
            ">>> Loaded 21009 words. 2616 words initialized randomly.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Functional_NER_Model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Functional_NER_Model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ glove_embeddings    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │  \u001b[38;5;34m2,362,500\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ spatial_dropout1d_1 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ glove_embeddings… │\n",
              "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)  │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bidirectional_lstm  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m234,496\u001b[0m │ spatial_dropout1… │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ bidirectional_ls… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ logits_output       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m9\u001b[0m)     │      \u001b[38;5;34m2,313\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ glove_embeddings    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,500</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ spatial_dropout1d_1 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ glove_embeddings… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)  │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bidirectional_lstm  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">234,496</span> │ spatial_dropout1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_ls… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ logits_output       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,313</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,599,309\u001b[0m (9.92 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,599,309</span> (9.92 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m236,809\u001b[0m (925.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">236,809</span> (925.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,362,500\u001b[0m (9.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,500</span> (9.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">> Starting training...\n",
            "Epoch 1/12\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.8313 - loss: 0.2960 - val_accuracy: 0.9789 - val_loss: 0.1522 - learning_rate: 5.0000e-04\n",
            "Epoch 2/12\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9452 - loss: 0.1415 - val_accuracy: 0.9838 - val_loss: 0.1211 - learning_rate: 5.0000e-04\n",
            "Epoch 3/12\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9821 - loss: 0.1172 - val_accuracy: 0.9860 - val_loss: 0.1059 - learning_rate: 5.0000e-04\n",
            "Epoch 4/12\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9854 - loss: 0.0961 - val_accuracy: 0.9873 - val_loss: 0.0977 - learning_rate: 5.0000e-04\n",
            "Epoch 5/12\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9870 - loss: 0.0860 - val_accuracy: 0.9880 - val_loss: 0.0912 - learning_rate: 5.0000e-04\n",
            "Epoch 6/12\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9878 - loss: 0.0786 - val_accuracy: 0.9884 - val_loss: 0.0852 - learning_rate: 5.0000e-04\n",
            "Epoch 7/12\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9887 - loss: 0.0739 - val_accuracy: 0.9889 - val_loss: 0.0831 - learning_rate: 5.0000e-04\n",
            "Epoch 8/12\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 0.9890 - loss: 0.0704 - val_accuracy: 0.9894 - val_loss: 0.0796 - learning_rate: 5.0000e-04\n",
            "Epoch 9/12\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9898 - loss: 0.0641 - val_accuracy: 0.9896 - val_loss: 0.0787 - learning_rate: 5.0000e-04\n",
            "Epoch 10/12\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9900 - loss: 0.0636 - val_accuracy: 0.9902 - val_loss: 0.0764 - learning_rate: 5.0000e-04\n",
            "Epoch 11/12\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9907 - loss: 0.0587 - val_accuracy: 0.9905 - val_loss: 0.0746 - learning_rate: 5.0000e-04\n",
            "Epoch 12/12\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9910 - loss: 0.0554 - val_accuracy: 0.9907 - val_loss: 0.0734 - learning_rate: 5.0000e-04\n",
            "\n",
            ">> Running final evaluation on test set...\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\n",
            "Classification Report (Entity-Level):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.82      0.81      0.81      1657\n",
            "        MISC       0.68      0.66      0.67       702\n",
            "         ORG       0.75      0.65      0.70      1660\n",
            "         PER       0.83      0.77      0.80      1604\n",
            "\n",
            "   micro avg       0.78      0.73      0.76      5623\n",
            "   macro avg       0.77      0.72      0.74      5623\n",
            "weighted avg       0.78      0.73      0.76      5623\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jlz4N36bOjQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "010E-c_3F1RR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}