The primary objective of this implementation is to empirically investigate the impact of the **context window size** on the quality and nature of learned word embeddings using the **Skip-gram** architecture. By training two distinct **Word2Vec** models on the `text8` corpus, the code demonstrates the transition from **syntactic/functional** similarity (captured by small windows) to **topical/thematic** associations (captured by larger windows). This experiment serves as a practical demonstration of how distributional semantics can be tuned to capture either local grammatical roles or broader conceptual domains.
