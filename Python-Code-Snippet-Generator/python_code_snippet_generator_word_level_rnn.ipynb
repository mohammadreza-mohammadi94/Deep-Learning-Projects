{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMXv4+UWdbHxwlrUcmPk5M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/Deep-Learning-Projects/blob/main/Python-Code-Snippet-Generator/python_code_snippet_generator_word_level_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "04cnvhbhqH6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets==3.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ_b7s0QqnkX",
        "outputId": "20eb7dd7-dd8e-4902-ed8b-28a5bd8380d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s4AHGJDOpuCd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "tS-H1_DorxW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset from HF\n",
        "def get_dataset():\n",
        "    \"\"\"Download the MBPP dataset from Hugging Face.\"\"\"\n",
        "    return datasets.load_dataset(\"mbpp\")\n",
        "\n",
        "# Load all data\n",
        "def load_data(dataset):\n",
        "    \"\"\"Collect all code samples and stitch them into one training corpus.\"\"\"\n",
        "    code_snippets = []\n",
        "    for split in dataset.keys():\n",
        "        for item in dataset[split]:\n",
        "            code = item['code']\n",
        "            code_snippets.append(code + \" END_OF_FUNC\")  # Mark where each function ends\n",
        "\n",
        "    python_corpus = \"\\n\\n\".join(code_snippets)  # Separate snippets slightly\n",
        "    return code_snippets, python_corpus\n",
        "\n",
        "# Text Normalizer function\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def code_standardization(input_string):\n",
        "    \"\"\"Keep formatting visible so the model learns structure, not chaos.\"\"\"\n",
        "    text = input_string\n",
        "    text = tf.strings.regex_replace(text, r\"\\n\", r\" <NEWLINE> \")  # Make newlines explicit\n",
        "    text = tf.strings.regex_replace(text, r\"\\t\", r\" <TAB> \")      # Keep tabs meaningful\n",
        "    text = tf.strings.regex_replace(text, \"    \", r\" <INDENT> \")  # Preserve indentation\n",
        "    text = tf.strings.regex_replace(text, r\"([(){}\\[\\]:,=\\\"\\'+*/<>|&!~-])\", r\" \\1 \")  # Isolate symbols\n",
        "    text = tf.strings.regex_replace(text, r'\\s{2,}', ' ')  # Remove messy spacing\n",
        "    return text\n",
        "\n",
        "def create_vectorizer(code, max_tokens=10_000):\n",
        "    \"\"\"Create a tokenizer that maps code tokens to integers.\"\"\"\n",
        "    vectorizer = tf.keras.layers.TextVectorization(\n",
        "        standardize=code_standardization,\n",
        "        max_tokens=max_tokens,\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=None,\n",
        "    )\n",
        "    vectorizer.adapt(code)  # Learn vocabulary from corpus\n",
        "    return vectorizer\n",
        "\n",
        "def prepare_dataset(vectorizer, python_corpus, batch_size, seq_len):\n",
        "    \"\"\"Convert raw token stream into shuffled input-target pairs.\"\"\"\n",
        "    full_text_ids = vectorizer([python_corpus])[0]\n",
        "    word_dataset = tf.data.Dataset.from_tensor_slices(full_text_ids)\n",
        "    sequences = word_dataset.batch(seq_len + 1, drop_remainder=True)  # Extra token for shifting\n",
        "\n",
        "    def split_input_target(seq):\n",
        "        \"\"\"Classic next-token prediction setup.\"\"\"\n",
        "        input_text = seq[:-1]\n",
        "        target_text = seq[1:]\n",
        "        return input_text, target_text\n",
        "\n",
        "    dataset = sequences.map(split_input_target)\n",
        "    dataset = dataset.shuffle(10_000).batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size, stateful=False):\n",
        "    \"\"\"Build a simple stacked GRU language model.\"\"\"\n",
        "    if stateful:\n",
        "        input_layer = tf.keras.Input(batch_shape=(batch_size, None))  # Required for stateful mode\n",
        "    else:\n",
        "        input_layer = tf.keras.Input(shape=(None,))\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        input_layer,\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),  # Token IDs → dense vectors\n",
        "        tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=stateful),\n",
        "        tf.keras.layers.Dropout(0.2),  # Small regularization\n",
        "        tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=stateful),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(units=vocab_size)  # Predict next token\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def generate_code_dynamic(model, vectorizer, start_string, max_generate=200, temp=0.2):\n",
        "    \"\"\"Generate code token-by-token until the model says it's done.\"\"\"\n",
        "    input_ids = vectorizer([start_string])\n",
        "    input_eval = input_ids\n",
        "    vocab = vectorizer.get_vocabulary()\n",
        "    text_generated = []\n",
        "\n",
        "    # Reset internal RNN memory before generating\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'reset_states'):\n",
        "            layer.reset_states()\n",
        "\n",
        "    # Generate up to max_generate tokens (may stop earlier)\n",
        "    for i in range(max_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = predictions[0, -1, :]  # Focus on last timestep\n",
        "        predictions = predictions / temp  # Control randomness\n",
        "        predictions = tf.expand_dims(predictions, 0)\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[0, 0].numpy()\n",
        "\n",
        "        if predicted_id > 1 and predicted_id < len(vocab):\n",
        "            predicted_word = vocab[predicted_id]\n",
        "\n",
        "            # Smart stop: break if model signals function end\n",
        "            if predicted_word == \"END_OF_FUNC\":\n",
        "                print(f\"\\n[INFO] Model finished writing naturally after {i} tokens.\")\n",
        "                break\n",
        "\n",
        "            text_generated.append(predicted_word)\n",
        "\n",
        "        input_eval = tf.constant([[predicted_id]])  # Feed prediction back in\n",
        "\n",
        "    # Reconstruct readable Python code\n",
        "    raw_generated_string = start_string + \" \" + \" \".join(text_generated)\n",
        "    final_code = raw_generated_string.replace(\"< NEWLINE >\", \"\\n\")\n",
        "    final_code = final_code.replace(\"< INDENT >\", \"    \")\n",
        "    final_code = final_code.replace(\"< TAB >\", \"\\t\")\n",
        "    final_code = final_code.replace(\" ( \", \"(\").replace(\" ) \", \")\")\n",
        "    final_code = final_code.replace(\" :\", \":\").replace(\" , \", \", \")\n",
        "\n",
        "    return final_code"
      ],
      "metadata": {
        "id": "YlxolKjHqLyT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = get_dataset()\n",
        "print(f\"[INFO] Dataset Loaded Successfully.\")\n",
        "print(\"===\" * 40)\n",
        "\n",
        "code_snippets, python_corpus = load_data(dataset)\n",
        "print(f\"\\n\\n[INFO] Dataset Loaded Successfully.\")\n",
        "print(f\"Total Python Snippets: {len(code_snippets)}\")\n",
        "print(f\"Total Corpus Length: {len(python_corpus)} characters\")\n",
        "\n",
        "vectorizer = create_vectorizer(code_snippets)\n",
        "print(f\"\\n\\n[INFO] - Vectorizer created successfully.\")\n",
        "print(f\"Voocab Length: {len(vectorizer.get_vocabulary())}\")\n",
        "print(f\"Top 15 tokens: {vectorizer.get_vocabulary()[:15]}\")\n",
        "\n",
        "# Get Dataset\n",
        "BATCH_SIZE = 64\n",
        "SEQ_LEN = 30\n",
        "dataset_train = prepare_dataset(\n",
        "    vectorizer, python_corpus, BATCH_SIZE, SEQ_LEN)\n",
        "print(f\"\\n\\n[INFO] Dataset pipeline created successfully.\")\n",
        "\n",
        "# Get model\n",
        "VOCAB_SIZE = len(vectorizer.get_vocabulary())\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 512\n",
        "model = build_model(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    rnn_units=RNN_UNITS,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "print(f\"\\n\\n[INFO] Model created successfully.\")\n",
        "print(\"Compiling the model...\")\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        ")\n",
        "print(f\"Model Summary:\")\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "print(f\"[INFO] - Model Trained successfully...\")\n",
        "\n",
        "# Define callbacks\n",
        "checkpoint_path = \"training_checkpoints/ckpt_{epoch}.weights.h5\"\n",
        "if not os.path.exists(\"training_checkpoints\"):\n",
        "    os.makedirs(\"training_checkpoints\")\n",
        "\n",
        "callbacks = [\n",
        "        # Stop training if the loss doesn't improve for 7 epochs, and restore the best weights.\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor=\"loss\", patience=5, restore_best_weights=True),\n",
        "        # Save model weights at each epoch, but only keep the best performing one based on loss.\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=checkpoint_path, save_weights_only=True, monitor=\"loss\", save_best_only=True),\n",
        "        # Reduce the learning rate if the loss plateaus for 3 epochs.\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor=\"loss\", factor=0.5, patience=3)\n",
        "    ]\n",
        "\n",
        "# Training\n",
        "EPOCHS = 160\n",
        "history = model.fit(\n",
        "    dataset_train,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "model.save_weights(\"python_coder.weights.h5\") # Save model\n",
        "print(\"Training has been finished successfully.\")\n",
        "print(\"Model's weight saved successfully.\")\n",
        "\n",
        "# Get inference model\n",
        "print(\"\\n\\n[INFO] - Defining Inference Model...\")\n",
        "inference_model = build_model(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    rnn_units=RNN_UNITS,\n",
        "    batch_size=1,\n",
        "    stateful=True\n",
        ")\n",
        "print(\"Load model's weights into inference model...\")\n",
        "inference_model.load_weights(\"python_coder.weights.h5\")\n",
        "inference_model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "# Generate Code\n",
        "print(\"\\n\\n[INFO] - Generating Code...\")\n",
        "start_prompt = \"def calculate_factorial ( n ) :\"\n",
        "generated_python_code = generate_code_dynamic(\n",
        "    model=inference_model,\n",
        "    vectorizer=vectorizer,\n",
        "    start_string=start_prompt,\n",
        "    temp=1\n",
        ")\n",
        "\n",
        "print(generated_python_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Sqz1WE8lrt3s",
        "outputId": "e112bd35-762a-4ea3-90d4-2a47556d9625"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Dataset Loaded Successfully.\n",
            "========================================================================================================================\n",
            "\n",
            "\n",
            "[INFO] Dataset Loaded Successfully.\n",
            "Total Python Snippets: 974\n",
            "Total Corpus Length: 189994 characters\n",
            "\n",
            "\n",
            "[INFO] - Vectorizer created successfully.\n",
            "Voocab Length: 2297\n",
            "Top 15 tokens: ['', '[UNK]', np.str_('<'), np.str_('>'), np.str_('INDENT'), np.str_('NEWLINE'), np.str_(')'), np.str_('('), np.str_('='), np.str_(':'), np.str_('TAB'), np.str_(','), np.str_(']'), np.str_('['), np.str_('1')]\n",
            "\n",
            "\n",
            "[INFO] Dataset pipeline created successfully.\n",
            "\n",
            "\n",
            "[INFO] Model created successfully.\n",
            "Compiling the model...\n",
            "Model Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m588,032\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_12 (\u001b[38;5;33mGRU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,182,720\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_13 (\u001b[38;5;33mGRU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,575,936\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2297\u001b[0m)     │     \u001b[38;5;34m1,178,361\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">588,032</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,182,720</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,575,936</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2297</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,178,361</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,525,049\u001b[0m (17.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,525,049</span> (17.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,525,049\u001b[0m (17.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,525,049</span> (17.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] - Model Trained successfully...\n",
            "Epoch 1/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 5.3124 - learning_rate: 0.0010\n",
            "Epoch 2/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 3.4040 - learning_rate: 0.0010\n",
            "Epoch 3/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 2.8668 - learning_rate: 0.0010\n",
            "Epoch 4/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 2.4965 - learning_rate: 0.0010\n",
            "Epoch 5/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 2.1998 - learning_rate: 0.0010\n",
            "Epoch 6/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 2.0435 - learning_rate: 0.0010\n",
            "Epoch 7/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 1.9192 - learning_rate: 0.0010\n",
            "Epoch 8/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.8087 - learning_rate: 0.0010\n",
            "Epoch 9/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 1.7015 - learning_rate: 0.0010\n",
            "Epoch 10/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 1.6171 - learning_rate: 0.0010\n",
            "Epoch 11/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 1.5367 - learning_rate: 0.0010\n",
            "Epoch 12/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 1.4587 - learning_rate: 0.0010\n",
            "Epoch 13/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 1.4197 - learning_rate: 0.0010\n",
            "Epoch 14/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 1.3661 - learning_rate: 0.0010\n",
            "Epoch 15/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 1.3399 - learning_rate: 0.0010\n",
            "Epoch 16/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.2937 - learning_rate: 0.0010\n",
            "Epoch 17/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 1.2556 - learning_rate: 0.0010\n",
            "Epoch 18/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 1.2261 - learning_rate: 0.0010\n",
            "Epoch 19/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 1.1796 - learning_rate: 0.0010\n",
            "Epoch 20/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 1.1754 - learning_rate: 0.0010\n",
            "Epoch 21/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.1267 - learning_rate: 0.0010\n",
            "Epoch 22/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.0887 - learning_rate: 0.0010\n",
            "Epoch 23/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 1.0679 - learning_rate: 0.0010\n",
            "Epoch 24/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 1.0466 - learning_rate: 0.0010\n",
            "Epoch 25/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 1.0154 - learning_rate: 0.0010\n",
            "Epoch 26/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.9901 - learning_rate: 0.0010\n",
            "Epoch 27/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.9705 - learning_rate: 0.0010\n",
            "Epoch 28/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.9452 - learning_rate: 0.0010\n",
            "Epoch 29/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.9288 - learning_rate: 0.0010\n",
            "Epoch 30/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.8948 - learning_rate: 0.0010\n",
            "Epoch 31/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.8825 - learning_rate: 0.0010\n",
            "Epoch 32/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.8624 - learning_rate: 0.0010\n",
            "Epoch 33/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.8355 - learning_rate: 0.0010\n",
            "Epoch 34/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.8320 - learning_rate: 0.0010\n",
            "Epoch 35/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.8000 - learning_rate: 0.0010\n",
            "Epoch 36/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.7796 - learning_rate: 0.0010\n",
            "Epoch 37/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.7609 - learning_rate: 0.0010\n",
            "Epoch 38/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.7399 - learning_rate: 0.0010\n",
            "Epoch 39/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.7356 - learning_rate: 0.0010\n",
            "Epoch 40/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.7138 - learning_rate: 0.0010\n",
            "Epoch 41/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.6984 - learning_rate: 0.0010\n",
            "Epoch 42/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.6783 - learning_rate: 0.0010\n",
            "Epoch 43/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.6764 - learning_rate: 0.0010\n",
            "Epoch 44/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.6531 - learning_rate: 0.0010\n",
            "Epoch 45/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.6392 - learning_rate: 0.0010\n",
            "Epoch 46/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.6252 - learning_rate: 0.0010\n",
            "Epoch 47/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.6179 - learning_rate: 0.0010\n",
            "Epoch 48/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.5986 - learning_rate: 0.0010\n",
            "Epoch 49/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.5944 - learning_rate: 0.0010\n",
            "Epoch 50/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.5811 - learning_rate: 0.0010\n",
            "Epoch 51/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.5662 - learning_rate: 0.0010\n",
            "Epoch 52/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.5546 - learning_rate: 0.0010\n",
            "Epoch 53/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.5379 - learning_rate: 0.0010\n",
            "Epoch 54/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.5382 - learning_rate: 0.0010\n",
            "Epoch 55/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.5216 - learning_rate: 0.0010\n",
            "Epoch 56/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.5122 - learning_rate: 0.0010\n",
            "Epoch 57/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.5004 - learning_rate: 0.0010\n",
            "Epoch 58/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.5007 - learning_rate: 0.0010\n",
            "Epoch 59/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.4849 - learning_rate: 0.0010\n",
            "Epoch 60/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.4778 - learning_rate: 0.0010\n",
            "Epoch 61/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.4753 - learning_rate: 0.0010\n",
            "Epoch 62/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.4607 - learning_rate: 0.0010\n",
            "Epoch 63/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.4483 - learning_rate: 0.0010\n",
            "Epoch 64/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.4428 - learning_rate: 0.0010\n",
            "Epoch 65/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.4390 - learning_rate: 0.0010\n",
            "Epoch 66/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.4279 - learning_rate: 0.0010\n",
            "Epoch 67/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.4228 - learning_rate: 0.0010\n",
            "Epoch 68/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.4210 - learning_rate: 0.0010\n",
            "Epoch 69/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.4130 - learning_rate: 0.0010\n",
            "Epoch 70/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.4085 - learning_rate: 0.0010\n",
            "Epoch 71/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3951 - learning_rate: 0.0010\n",
            "Epoch 72/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3894 - learning_rate: 0.0010\n",
            "Epoch 73/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3893 - learning_rate: 0.0010\n",
            "Epoch 74/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3818 - learning_rate: 0.0010\n",
            "Epoch 75/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3803 - learning_rate: 0.0010\n",
            "Epoch 76/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.3686 - learning_rate: 0.0010\n",
            "Epoch 77/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.3687 - learning_rate: 0.0010\n",
            "Epoch 78/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3610 - learning_rate: 0.0010\n",
            "Epoch 79/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3615 - learning_rate: 0.0010\n",
            "Epoch 80/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3541 - learning_rate: 0.0010\n",
            "Epoch 81/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3525 - learning_rate: 0.0010\n",
            "Epoch 82/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3462 - learning_rate: 0.0010\n",
            "Epoch 83/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.3368 - learning_rate: 0.0010\n",
            "Epoch 84/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.3351 - learning_rate: 0.0010\n",
            "Epoch 85/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3284 - learning_rate: 0.0010\n",
            "Epoch 86/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.3317 - learning_rate: 0.0010\n",
            "Epoch 87/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3241 - learning_rate: 0.0010\n",
            "Epoch 88/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3213 - learning_rate: 0.0010\n",
            "Epoch 89/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3177 - learning_rate: 0.0010\n",
            "Epoch 90/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.3108 - learning_rate: 0.0010\n",
            "Epoch 91/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.3188 - learning_rate: 0.0010\n",
            "Epoch 92/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3045 - learning_rate: 0.0010\n",
            "Epoch 93/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3090 - learning_rate: 0.0010\n",
            "Epoch 94/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3042 - learning_rate: 0.0010\n",
            "Epoch 95/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.3005 - learning_rate: 0.0010\n",
            "Epoch 96/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2966 - learning_rate: 0.0010\n",
            "Epoch 97/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.2927 - learning_rate: 0.0010\n",
            "Epoch 98/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2951 - learning_rate: 0.0010\n",
            "Epoch 99/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2897 - learning_rate: 0.0010\n",
            "Epoch 100/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2869 - learning_rate: 0.0010\n",
            "Epoch 101/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2862 - learning_rate: 0.0010\n",
            "Epoch 102/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2874 - learning_rate: 0.0010\n",
            "Epoch 103/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2807 - learning_rate: 0.0010\n",
            "Epoch 104/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.2799 - learning_rate: 0.0010\n",
            "Epoch 105/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2822 - learning_rate: 0.0010\n",
            "Epoch 106/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2794 - learning_rate: 0.0010\n",
            "Epoch 107/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2717 - learning_rate: 0.0010\n",
            "Epoch 108/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2713 - learning_rate: 0.0010\n",
            "Epoch 109/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.2694 - learning_rate: 0.0010\n",
            "Epoch 110/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2658 - learning_rate: 0.0010\n",
            "Epoch 111/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2635 - learning_rate: 0.0010\n",
            "Epoch 112/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 0.2686 - learning_rate: 0.0010\n",
            "Epoch 113/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2622 - learning_rate: 0.0010\n",
            "Epoch 114/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2645 - learning_rate: 0.0010\n",
            "Epoch 115/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2597 - learning_rate: 0.0010\n",
            "Epoch 116/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2593 - learning_rate: 0.0010\n",
            "Epoch 117/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2570 - learning_rate: 0.0010\n",
            "Epoch 118/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.2592 - learning_rate: 0.0010\n",
            "Epoch 119/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.2562 - learning_rate: 0.0010\n",
            "Epoch 120/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2520 - learning_rate: 0.0010\n",
            "Epoch 121/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.2530 - learning_rate: 0.0010\n",
            "Epoch 122/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.2536 - learning_rate: 0.0010\n",
            "Epoch 123/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.2475 - learning_rate: 0.0010\n",
            "Epoch 124/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.2523 - learning_rate: 0.0010\n",
            "Epoch 125/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.2518 - learning_rate: 0.0010\n",
            "Epoch 126/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.2487 - learning_rate: 0.0010\n",
            "Epoch 127/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2404 - learning_rate: 5.0000e-04\n",
            "Epoch 128/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2253 - learning_rate: 5.0000e-04\n",
            "Epoch 129/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2242 - learning_rate: 5.0000e-04\n",
            "Epoch 130/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2221 - learning_rate: 5.0000e-04\n",
            "Epoch 131/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2182 - learning_rate: 5.0000e-04\n",
            "Epoch 132/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.2227 - learning_rate: 5.0000e-04\n",
            "Epoch 133/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.2226 - learning_rate: 5.0000e-04\n",
            "Epoch 134/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2202 - learning_rate: 5.0000e-04\n",
            "Epoch 135/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2213 - learning_rate: 5.0000e-04\n",
            "Epoch 136/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2157 - learning_rate: 5.0000e-04\n",
            "Epoch 137/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2153 - learning_rate: 5.0000e-04\n",
            "Epoch 138/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.2198 - learning_rate: 5.0000e-04\n",
            "Epoch 139/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2185 - learning_rate: 5.0000e-04\n",
            "Epoch 140/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2154 - learning_rate: 5.0000e-04\n",
            "Epoch 141/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.2154 - learning_rate: 5.0000e-04\n",
            "Epoch 142/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2158 - learning_rate: 5.0000e-04\n",
            "Epoch 143/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2123 - learning_rate: 5.0000e-04\n",
            "Epoch 144/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.2137 - learning_rate: 5.0000e-04\n",
            "Epoch 145/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2135 - learning_rate: 5.0000e-04\n",
            "Epoch 146/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.2158 - learning_rate: 5.0000e-04\n",
            "Epoch 147/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2129 - learning_rate: 5.0000e-04\n",
            "Epoch 148/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2092 - learning_rate: 5.0000e-04\n",
            "Epoch 149/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2103 - learning_rate: 5.0000e-04\n",
            "Epoch 150/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.2144 - learning_rate: 5.0000e-04\n",
            "Epoch 151/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2095 - learning_rate: 5.0000e-04\n",
            "Epoch 152/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.2137 - learning_rate: 5.0000e-04\n",
            "Epoch 153/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.2098 - learning_rate: 5.0000e-04\n",
            "Epoch 154/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.2088 - learning_rate: 5.0000e-04\n",
            "Epoch 155/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.2075 - learning_rate: 5.0000e-04\n",
            "Epoch 156/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.2113 - learning_rate: 5.0000e-04\n",
            "Epoch 157/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.2115 - learning_rate: 5.0000e-04\n",
            "Epoch 158/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.2078 - learning_rate: 5.0000e-04\n",
            "Epoch 159/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2068 - learning_rate: 2.5000e-04\n",
            "Epoch 160/160\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.2025 - learning_rate: 2.5000e-04\n",
            "Training has been finished successfully.\n",
            "Model's weight saved successfully.\n",
            "\n",
            "\n",
            "[INFO] - Defining Inference Model...\n",
            "Load model's weights into inference model...\n",
            "\n",
            "\n",
            "[INFO] - Generating Code...\n",
            "\n",
            "[INFO] Model finished writing naturally after 181 tokens.\n",
            "def calculate_factorial(n): \n",
            "      m = bisect.bisect_right(a, l)\n",
            "      return area \n",
            " def square_nums(text): \n",
            " x = Counter(dict1)\n",
            " for m, val in result.items(): \n",
            "      result = \" \" \n",
            " \t for char in str1: \n",
            "           if(re.search(regex, Ip)): \n",
            " \t \t \t return(\" Valid \")\n",
            " \t else: \n",
            " \t \t return(\" Valid \")\n",
            " \t else: \n",
            " \t \t return(\" Invalid IP address \")\n",
            " \t else: \n",
            " \t \t return(\" Invalid \" )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-AhxCFrrHnN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hbgc2CiurOI7"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}