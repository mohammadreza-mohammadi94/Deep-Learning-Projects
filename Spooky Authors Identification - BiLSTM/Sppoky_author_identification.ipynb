{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+smwJasisLkhB3/9IwFc5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/Deep-Learning-Projects/blob/main/Spooky%20Authors%20Identification%20-%20BiLSTM/Sppoky_author_identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install & Load Libraries"
      ],
      "metadata": {
        "id": "WExyyP1Y1IlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pandas numpy matplotlib seaborn kaggle tokenizers tensorflow scikit-learn"
      ],
      "metadata": {
        "id": "GdmIn3hg1L80"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import re\n",
        "import zipfile\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Embedding, LSTM, Dense,\n",
        "                                     Dropout, Bidirectional, Input)\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "import sentencepiece as spm\n",
        "import subprocess"
      ],
      "metadata": {
        "id": "cJfMnpqx1NCU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "9tOQZkDTOutk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    COMPETITION_NAME = \"spooky-author-identification\"\n",
        "    TOKENIZER_MODEL_PREFIX = \"author_id_bpe_tokenizer\"\n",
        "    VOCAB_SIZE = 8000\n",
        "    MAX_SEQ_LENGTH = 60\n",
        "    EMBEDDING_DIM = 128\n",
        "    LSTM_UNITS = 96\n",
        "    DROPOUT_RATE = 0.4\n",
        "    VALIDATION_SIZE = 0.2 # 20% of training data will be used for validation\n",
        "    EPOCHS = 15\n",
        "    BATCH_SIZE = 32\n",
        "    PATIENCE = 3"
      ],
      "metadata": {
        "id": "8UawEQSfOujZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pipeline"
      ],
      "metadata": {
        "id": "lJiUjvhT2Km9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_load_data(competition_name: str) -> (pd.DataFrame, pd.DataFrame):\n",
        "    \"\"\"Downloads competition data and loads train and test sets.\"\"\"\n",
        "    print(\"\\t\\t === Downloading and Loading Data ==\")\n",
        "    try:\n",
        "        # Ensure the kaggle.json file is in the correct directory\n",
        "        kaggle_config_dir = '/root/.kaggle/'\n",
        "        kaggle_json_path = os.path.join(kaggle_config_dir, 'kaggle.json')\n",
        "\n",
        "        if not os.path.exists(kaggle_json_path):\n",
        "            print(\"Kaggle API key not found in the expected location.\")\n",
        "            # Try to find kaggle.json in the content directory if it was uploaded there\n",
        "            content_kaggle_json = '/content/kaggle.json'\n",
        "            if os.path.exists(content_kaggle_json):\n",
        "                print(f\"Found kaggle.json in {content_kaggle_json}. Moving to {kaggle_json_path}\")\n",
        "                !mkdir -p {kaggle_config_dir}\n",
        "                !mv {content_kaggle_json} {kaggle_json_path}\n",
        "                !chmod 600 {kaggle_json_path}\n",
        "                print(\"kaggle.json moved successfully.\")\n",
        "            else:\n",
        "                print(\"Please upload your 'kaggle.json' file:\")\n",
        "                print(\"1. Go to your Kaggle account settings.\")\n",
        "                print(\"2. Under the 'API' section, click 'Create New API Token'.\")\n",
        "                print(\"3. A 'kaggle.json' file will be downloaded.\")\n",
        "                print(\"4. In the left sidebar of Colab, click on the 'Files' icon.\")\n",
        "                print(\"5. Upload the 'kaggle.json' file (usually to the default /content/ directory).\")\n",
        "                return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "        !kaggle competitions download -c {competition_name}\n",
        "        !unzip -o {competition_name}.zip\n",
        "        # Unzip the individual data files\n",
        "        !unzip -o train.zip\n",
        "        !unzip -o test.zip\n",
        "        !unzip -o sample_submission.zip\n",
        "\n",
        "        train_df = pd.read_csv('train.csv')\n",
        "        test_df = pd.read_csv('test.csv')\n",
        "        print(f\"Train data loaded successfully. Shape: {train_df.shape}\")\n",
        "        print(f\"Test data loaded successfully. Shape: {test_df.shape}\\n\\n\")\n",
        "        return train_df, test_df\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during data download/loading: {e}\")\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "def preprocess_text(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Cleans the text column of a DataFrame.\"\"\"\n",
        "    df['text_clean'] = df['text'].str.lower()\n",
        "    return df\n",
        "\n",
        "def train_spm_tokenizer(text_series: pd.Series, prefix: str, vocab_size: int):\n",
        "    \"\"\"Trains a SentencePiece tokenizer ONLY on the training text.\"\"\"\n",
        "    print(\"\\t\\t === Training SentencePiece Tokenizer ===\")\n",
        "    text_file = 'train_text_for_spm.txt'\n",
        "    with open(text_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(text_series.tolist()))\n",
        "\n",
        "    spm_command = (f'--input={text_file} --model_prefix={prefix} --vocab_size={vocab_size} --model_type=bpe '\n",
        "                   f'--character_coverage=1.0 --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3')\n",
        "    spm.SentencePieceTrainer.Train(spm_command)\n",
        "    print(f\"Tokenizer training complete. Model saved as '{prefix}.model'.\\n\\n\")\n",
        "\n",
        "def analyze_and_set_maxlen(text_series: pd.Series, sp_processor) -> int:\n",
        "    \"\"\"Analyzes tokenized sequence lengths to determine maxlen.\"\"\"\n",
        "    print(\"\\t\\t=== Analyzing Sequence Lengths to Determine MAX_SEQ_LENGTH ===\")\n",
        "    tokenized_lengths = [len(sp_processor.encode_as_ids(text)) for text in text_series]\n",
        "    percentile = 98\n",
        "    recommended_maxlen = int(np.percentile(tokenized_lengths, percentile))\n",
        "    print(f\"{percentile}th percentile of sequence lengths is: {recommended_maxlen} tokens.\")\n",
        "    return recommended_maxlen\n",
        "\n",
        "def build_bilstm_model(config: Config, num_classes: int) -> Model:\n",
        "    \"\"\"Generates a simple Bidirectional LSTM model for text classification.\"\"\"\n",
        "    print(\"\\t\\t=== Building Bidirectional LSTM Model ===\")\n",
        "\n",
        "    input_layer = Input(shape=(config.MAX_SEQ_LENGTH,), name='input_layer')\n",
        "    embedding_layer = Embedding(\n",
        "        input_dim=config.VOCAB_SIZE, output_dim=config.EMBEDDING_DIM,\n",
        "        mask_zero=True, name='embedding_layer'\n",
        "    )(input_layer)\n",
        "    bilstm_layer = Bidirectional(\n",
        "        LSTM(config.LSTM_UNITS, return_sequences=False, recurrent_dropout=0.2),\n",
        "        name='bilstm_layer'\n",
        "    )(embedding_layer)\n",
        "    dropout_layer = Dropout(config.DROPOUT_RATE, name='dropout_layer')(bilstm_layer)\n",
        "    dense_layer = Dense(64, activation='relu', name='dense_layer')(dropout_layer)\n",
        "    output_layer = Dense(num_classes, activation='softmax', name='output_layer')(dense_layer)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "PvZnC2nDXLcM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    config = Config()\n",
        "\n",
        "    # --- Load Data ---\n",
        "    train_df, test_df = download_and_load_data(config.COMPETITION_NAME)\n",
        "    if train_df.empty or test_df.empty:\n",
        "        return\n",
        "\n",
        "    # --- Preprocess Data ---\n",
        "    train_df = preprocess_text(train_df)\n",
        "    test_df = preprocess_text(test_df)\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_labels = label_encoder.fit_transform(train_df['author'])\n",
        "\n",
        "    # --- Tokenizer ---\n",
        "    # IMPORTANT: Train tokenizer ONLY on training data\n",
        "    train_spm_tokenizer(train_df['text_clean'], config.TOKENIZER_MODEL_PREFIX, config.VOCAB_SIZE)\n",
        "    sp = spm.SentencePieceProcessor(model_file=f'{config.TOKENIZER_MODEL_PREFIX}.model')\n",
        "\n",
        "    # Analyze max length based ONLY on training data\n",
        "    config.MAX_SEQ_LENGTH = analyze_and_set_maxlen(train_df['text_clean'], sp)\n",
        "\n",
        "    # Tokenize and Pad both train and test data using the SAME tokenizer and maxlen\n",
        "    X = pad_sequences(\n",
        "        [sp.encode_as_ids(text) for text in train_df['text_clean']],\n",
        "        maxlen=config.MAX_SEQ_LENGTH, padding='post', truncating='post'\n",
        "    )\n",
        "    X_submission = pad_sequences(\n",
        "        [sp.encode_as_ids(text) for text in test_df['text_clean']],\n",
        "        maxlen=config.MAX_SEQ_LENGTH, padding='post', truncating='post'\n",
        "    )\n",
        "\n",
        "    # --- Split training data into train and validation sets ---\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y_labels,\n",
        "        test_size=config.VALIDATION_SIZE,\n",
        "        random_state=42,\n",
        "        stratify=y_labels\n",
        "    )\n",
        "    print(f\"\\nData splits: Train={len(X_train)}, Validation={len(X_val)}, Test={len(X_submission)}\\n\\n\")\n",
        "\n",
        "    # --- Build and Train Model ---\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "    model = build_bilstm_model(config, num_classes)\n",
        "\n",
        "    callbacks = [\n",
        "        ModelCheckpoint('best_author_model.keras', save_best_only=True, monitor='val_accuracy', mode='max'),\n",
        "        EarlyStopping(monitor='val_loss', patience=config.PATIENCE, restore_best_weights=True)\n",
        "    ]\n",
        "\n",
        "    print(\"\\t\\t=== Training the Model ===\")\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=config.EPOCHS,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # --- Evaluate Model on the Validation Set ---\n",
        "    print(\"\\t\\t=== Evaluating Model on Validation Set ===\")\n",
        "    best_model = tf.keras.models.load_model('best_author_model.keras')\n",
        "    val_loss, val_accuracy = best_model.evaluate(X_val, y_val)\n",
        "    print(f\"\\nValidation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "    y_pred_val_probs = best_model.predict(X_val)\n",
        "    y_pred_val = np.argmax(y_pred_val_probs, axis=1)\n",
        "\n",
        "    print(\"\\nClassification Report (on Validation Data):\")\n",
        "    print(classification_report(y_val, y_pred_val, target_names=label_encoder.classes_))\n",
        "\n",
        "    # --- Generate Submission File ---\n",
        "    print(\"\\t\\t=== Generating Submission File for Kaggle ===\")\n",
        "    submission_predictions = best_model.predict(X_submission)\n",
        "\n",
        "    submission_df = pd.DataFrame(submission_predictions, columns=label_encoder.classes_)\n",
        "    submission_df['id'] = test_df['id']\n",
        "    submission_df = submission_df[['id'] + list(label_encoder.classes_)] # Reorder columns\n",
        "\n",
        "    submission_df.to_csv('submission.csv', index=False)\n",
        "    print(\"Submission file 'submission.csv' created successfully.\")\n",
        "    print(\"Head of submission file:\")\n",
        "    print(submission_df.head())\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "Zfl2uCeU7EnY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "618a3dd3-c006-4f5e-c3be-302bd690e255"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\t === Downloading and Loading Data ==\n",
            "Kaggle API key not found in the expected location.\n",
            "Found kaggle.json in /content/kaggle.json. Moving to /root/.kaggle/kaggle.json\n",
            "kaggle.json moved successfully.\n",
            "Downloading spooky-author-identification.zip to /content\n",
            "  0% 0.00/1.81M [00:00<?, ?B/s]\n",
            "100% 1.81M/1.81M [00:00<00:00, 884MB/s]\n",
            "Archive:  spooky-author-identification.zip\n",
            "  inflating: sample_submission.zip   \n",
            "  inflating: test.zip                \n",
            "  inflating: train.zip               \n",
            "Archive:  train.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  test.zip\n",
            "  inflating: test.csv                \n",
            "Archive:  sample_submission.zip\n",
            "  inflating: sample_submission.csv   \n",
            "Train data loaded successfully. Shape: (19579, 3)\n",
            "Test data loaded successfully. Shape: (8392, 2)\n",
            "\n",
            "\n",
            "\t\t === Training SentencePiece Tokenizer ===\n",
            "Tokenizer training complete. Model saved as 'author_id_bpe_tokenizer.model'.\n",
            "\n",
            "\n",
            "\t\t=== Analyzing Sequence Lengths to Determine MAX_SEQ_LENGTH ===\n",
            "98th percentile of sequence lengths is: 96 tokens.\n",
            "\n",
            "\n",
            "\n",
            "Data splits: Train=15663, Validation=3916, Test=8392\n",
            "\t\t=== Building Bidirectional LSTM Model ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_layer     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m1,024,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │    \u001b[38;5;34m172,800\u001b[0m │ embedding_layer[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_layer       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bilstm_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_layer (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m12,352\u001b[0m │ dropout_layer[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │        \u001b[38;5;34m195\u001b[0m │ dense_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_layer     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">172,800</span> │ embedding_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_layer       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bilstm_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │ dropout_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │ dense_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,209,347\u001b[0m (4.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,209,347</span> (4.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,209,347\u001b[0m (4.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,209,347</span> (4.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training the Model ---\n",
            "Epoch 1/15\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 645ms/step - accuracy: 0.6089 - loss: 0.8166 - val_accuracy: 0.8289 - val_loss: 0.4495\n",
            "Epoch 2/15\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 615ms/step - accuracy: 0.8895 - loss: 0.2984 - val_accuracy: 0.8266 - val_loss: 0.4775\n",
            "Epoch 3/15\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 616ms/step - accuracy: 0.9418 - loss: 0.1796 - val_accuracy: 0.8292 - val_loss: 0.4872\n",
            "Epoch 4/15\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 600ms/step - accuracy: 0.9573 - loss: 0.1295 - val_accuracy: 0.8154 - val_loss: 0.5767\n",
            "\t\t=== Evaluating Model on Validation Set ===\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.8213 - loss: 0.4977\n",
            "\n",
            "Validation Accuracy: 82.92%\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 113ms/step\n",
            "\n",
            "Classification Report (on Validation Data):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         EAP       0.83      0.84      0.83      1580\n",
            "         HPL       0.85      0.81      0.83      1127\n",
            "         MWS       0.81      0.84      0.82      1209\n",
            "\n",
            "    accuracy                           0.83      3916\n",
            "   macro avg       0.83      0.83      0.83      3916\n",
            "weighted avg       0.83      0.83      0.83      3916\n",
            "\n",
            "\t\t=== Generating Submission File for Kaggle ===\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step\n",
            "Submission file 'submission.csv' created successfully.\n",
            "Head of submission file:\n",
            "        id       EAP       HPL       MWS\n",
            "0  id02310  0.005809  0.001517  0.992674\n",
            "1  id24541  0.425214  0.197628  0.377157\n",
            "2  id00134  0.065237  0.929305  0.005458\n",
            "3  id27757  0.773636  0.184558  0.041806\n",
            "4  id04081  0.980842  0.011890  0.007267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bad73fa6"
      },
      "source": [],
      "execution_count": 5,
      "outputs": []
    }
  ]
}