# -*- coding: utf-8 -*-
"""YouTube_Views_Predictions_LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lOjVGAmrVGLO9FTU5O34507TKh22KIdi

# Setup Enviorment

*Download Dataset From Kaggle*
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("datasnaek/youtube")
print("Path to dataset files:", path)

"""*Import Libraries*"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

"""*Setup Enviroment*"""

# Setup warnings
import warnings
warnings.filterwarnings('ignore')

# Setup logging
import logging
logging.basicConfig(
    format='%(asctime)s %(levelname)-8s %(message)s',
    level=logging.INFO,
    handlers=[
        logging.FileHandler("app_log.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)



"""*Load Dataset*"""

# Define PATH
DATA = "/root/.cache/kagglehub/datasets/datasnaek/youtube/versions/24/USvideos.csv"

# Load dataset
df = pd.read_csv(
    DATA,
    on_bad_lines='skip',  # 'skip' to silently drop bad lines
    quoting=1,
    encoding='utf-8'
)

df.head() # Check first rows



"""# Define Required Functions

*Load & Prepare Dataset*
"""

def load_and_prepare_data(file_path):
    logger.info("Loading and preparing dataset...")
    try:
        df = pd.read_csv(
            DATA,
            on_bad_lines='skip',
            quoting=1,
            encoding='utf-8')
        # Select features
        df = df[['title', 'tags', 'category_id', 'likes', 'dislikes', 'comment_total', 'views']]
        # Drop dupe and nans
        df = df.dropna().drop_duplicates()

        logger.info(f"Dataset loaded and cleaned. Shape: {df.shape}")
        return df
    except Exception as e:
        logger.error(f"Error loading and preparing dataset: {e}")
        raise



"""*Preprocessing Data*"""

def preprocess_data(df, max_length=50, max_words=5000):
    logger.info("Preprocessing data...")
    try:
        # merge title and tags
        texts = df['title'].fillna("") + " " + df['tags'].fillna("")

        # Text tokenization
        tokenizer = Tokenizer(num_words = max_words, oov_token="<OOV>")
        tokenizer.fit_on_texts(texts)
        sequences = tokenizer.texts_to_sequences(texts)
        padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')

        # Normalizing numeric values
        scaler = MinMaxScaler()
        views = scaler.fit_transform(df['views'].values.reshape(-1, 1))
        likes = scaler.fit_transform(df['likes'].values.reshape(-1, 1))
        dislikes = scaler.fit_transform(df['dislikes'].values.reshape(-1, 1))
        comment_total = scaler.fit_transform(df['comment_total'].values.reshape(-1, 1))
        category_id = scaler.fit_transform(df['category_id'].values.reshape(-1, 1))

        # Combine features
        X = np.concatenate([padded_sequences, category_id, likes,
                            dislikes, comment_total], axis=1)
        y = views

        # Spliting to X, y
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                            random_state=42)
        logger.info(f"Preprocessed data. X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
        return X_train, X_test, y_train, y_test, tokenizer, scaler, max_length
    except Exception as e:
        logger.error(f"Error in preprocessing: {e}")
        raise



"""*Build Model*"""

def build_lstm_model(vocab_size, sequence_length):
    logger.info("Building LSTM model...")
    try:
        model = Sequential([
            Embedding(vocab_size, 64, input_length=sequence_length + 4),
            LSTM(128, return_sequences=False),
            Dropout(0.3),
            Dense(64, activation='relu'),
            Dropout(0.3),
            Dense(1, activation='sigmoid')
        ])
        model.compile(loss='mse',
                      optimizer='adam',
                      metrics=['mae'])
        logger.info("LSTM model built successfully.")
        return model
    except Exception as e:
        logger.error(f"Error in building LSTM model: {e}")
        raise



"""*Train Model*"""

def train_model(model, X_train, y_train, X_test, y_test, epochs=20):
    logger.info("Training model....")
    try:
        # Defin early_stopping
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=3,
            restore_best_weights=True
        )

        # Trian
        history = model.fit(X_train, y_train,
                            batch_size=32,
                            validation_split=0.2,
                            epochs=epochs)
        loss, mae = model.evaluate(X_test, y_test, verbose=0)
        logger.info(f"Test loss: {loss}, Test MAE: {mae}")
        logger.info("Model trained successfully.")
        return history
    except Exception as e:
        logger.error(f"Error training model: {e}")
        raise



"""*Predict Views*"""

def predict_views(model, tokenizer, scaler, text, category_id,
                  likes, dislikes, comment_total, max_length):
    logger.info("Predicting views...")
    try:
        # Input preprocessing
        text_seq = tokenizer.texts_to_sequences([text])
        padded_seq = pad_sequences(text_seq, maxlen=max_length, padding='post')
        category_id_scaled = scaler.fit_transform(np.array([[category_id]]))
        likes_scaled = scaler.fit_transform(np.array([[likes]]))
        dislikes_scaled = scaler.fit_transform(np.array([[dislikes]]))
        comment_total_scaled = scaler.fit_transform(np.array([[comment_total]]))
        input_data = np.concatenate([padded_seq, category_id_scaled, likes_scaled, dislikes_scaled, comment_total_scaled], axis=1)

        # Prediction
        prediction = model.predict(input_data, verbose=0)
        if prediction.ndim > 2:
            prediction = prediction.reshape(prediction.shape[0], -1)
        predicted_views = scaler.inverse_transform(prediction)[0][0]
        logger.info(f"Predicted views: {predicted_views}")
        return predicted_views
    except Exception as e:
        logger.error(f"Error predicting views: {e}")
        raise



"""*Plot Loss*"""

def plot_training_history(history):
    logger.info("Plotting training history...")
    try:
        # Plot loss
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 2, 1)
        plt.plot(history.history['loss'], label='Training Loss')
        plt.plot(history.history['val_loss'], label='Validation Loss')
        plt.title('Model Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()

        # Plot MAE
        plt.subplot(1, 2, 2)
        plt.plot(history.history['mae'], label='Training MAE')
        plt.plot(history.history['val_mae'], label='Validation MAE')
        plt.title('Model MAE')
        plt.xlabel('Epoch')
        plt.ylabel('MAE')
        plt.legend()

        plt.tight_layout()
        plt.savefig('training_history.png')
        plt.show()
        plt.close()
        logger.info("Training history plotted and saved as training_history.png")
    except Exception as e:
        logger.error(f"Error plotting training history: {e}")
        raise



"""# Run Functions"""

def main():
    logger.info("Starting YT views predictions project...")

    # Load data
    df = load_and_prepare_data(DATA)
    # Preprocessing
    X_train, X_test, y_train, y_test, tokenizer, scaler, max_length = preprocess_data(df)
    # Creat model
    vocab_size = min(5000, len(tokenizer.word_index) + 1)
    model = build_lstm_model(vocab_size, max_length)
    print("\n ---- SUMMARY ---- \n")
    model.summary()
    # Train model
    print("\n ---- TRAINING ---- \n")
    history = train_model(model, X_train, y_train, X_test, y_test)
    # Plots
    print("\n ---- PLOTTING ---- \n")
    plot_training_history(history)

    # Prediction sample
    sample_text = "Amazing AI tutorial video | ai, technology, tutorial"
    sample_category_id = 28
    sample_likes = 150
    sample_dislikes = 10
    sample_comment_total = 30
    predicted_views = predict_views(model, tokenizer,
                                    scaler, sample_text,
                                    sample_category_id,
                                    sample_likes, sample_dislikes,
                                    sample_comment_total, max_length)

    # Save model
    model.save("yt_views_model.h5")
    logger.info("Model save as yt_views_model.h5")

# Run fun
main()

