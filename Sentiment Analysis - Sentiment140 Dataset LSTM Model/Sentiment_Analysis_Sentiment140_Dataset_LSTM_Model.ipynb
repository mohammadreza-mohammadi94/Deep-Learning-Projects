{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNSk2cWqGbEYGEy4q8ONGh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/Deep-Learning-Projects/blob/main/Sentiment%20Analysis%20-%20Sentiment140%20Dataset%20LSTM%20Model/Sentiment_Analysis_Sentiment140_Dataset_LSTM_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libs & Setup Enviorment"
      ],
      "metadata": {
        "id": "5JxkdkWLt3xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM ,Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "# Setup warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setup logger\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "    format='%(asctime)s : %(levelname)s : %(message)s',\n",
        "    level=logging.ERROR,\n",
        "    handlers=[\n",
        "        logging.FileHandler('app.log'),\n",
        "        logging.StreamHandler()\n",
        "    ])\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info(\"Logger started...\")\n",
        "\n",
        "# Download dataset\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "logger.info(f\"Dataset download to {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhDxf-vet7al",
        "outputId": "aa144bc3-f4a2-45c4-9dc3-7c1174e58d0b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/sentiment140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Parameters"
      ],
      "metadata": {
        "id": "D0eHYwEFu0H2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ecOHAbRUtFmT"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = '/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv'\n",
        "VOCAB_SIZE = 15000          # Number of most frequent words (used as input_dim for Embedding)\n",
        "EMBEDDING_DIM = 128         # Size of word vector (output_dim for Embedding)\n",
        "MAX_LENGTH = 30             # Max tweet length (input_length for Embedding, timesteps for LSTM)\n",
        "BATCH_SIZE = 256            # Number of samples per batch\n",
        "EPOCHS = 15                 # Number of traning epochs\n",
        "TRAIN_SAMPLES = 100000      # Limit training to 100,000 tweets for faster training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load & Preprocess Dataset"
      ],
      "metadata": {
        "id": "6w7wqws4voKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset contains: polarity(0 = negative, 4=positive), text (tweet text)\n",
        "# Load only a subset for faster training\n",
        "try:\n",
        "    logger.info(\"Loading Sentiment140 Dataset...\")\n",
        "    data = pd.read_csv(DATASET_PATH,\n",
        "                       encoding='latin-1',\n",
        "                       names=['polarity', 'id', 'date', 'query', 'user', 'text']\n",
        "                       ).sample(TRAIN_SAMPLES)\n",
        "    logger.info(f\"{TRAIN_SAMPLES} Loaded from dataset.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading dataset: {e}\")"
      ],
      "metadata": {
        "id": "Jidq1BBRtxsO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert & correct labels of target\n",
        "try:\n",
        "    logger.info(\"Encoding target variables.\")\n",
        "    data['polarity'] = data['polarity'].map({\n",
        "                                                0: 1,\n",
        "                                                4: 1\n",
        "                                                })\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error encoding target variables: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "epx59vEVwTgG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)     # remove mentions\n",
        "    text = re.sub(r'#\\w+', '', text)     # remove hashtags\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
        "    return text.lower()\n",
        "data['text_processed'] = data['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "HpaY882_wkju"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spliting Train & Test Sets"
      ],
      "metadata": {
        "id": "cWKZ8zfKxEDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    logger.info(\"Splitting Train/Test.\")\n",
        "    train_size = int(0.8 * len(data))\n",
        "    train_data = data[:train_size]\n",
        "    test_data = data[train_size:]\n",
        "\n",
        "    # train-test\n",
        "    X_train = train_data['text_processed']\n",
        "    y_train = train_data['polarity']\n",
        "    X_test = test_data['text_processed']\n",
        "    y_test = test_data['polarity']\n",
        "    logger.info(f\"Data Splitted: X_train={len(X_train)}\\t y_train={len(y_train)}\\t X_test={len(X_test)}\\t y_test={len(y_test)}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error splitting data: {e}\")\n",
        "\n",
        "\n",
        "print(f\"Training tweets: {len(X_train)}\")\n",
        "print(f\"Testing tweets: {len(y_train)}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_train shape: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfHd2EPUxBhj",
        "outputId": "8426915d-f5aa-4b4f-c501-c108c94efc55"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training tweets: 80000\n",
            "Testing tweets: 80000\n",
            "y_train shape: (80000,)\n",
            "y_train shape: (20000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization & Text-to-Seq conversion"
      ],
      "metadata": {
        "id": "gbtZ7LfZyFR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    logger.info(\"Converting words to IDs\")\n",
        "    # Convert words to token IDs using tensorflow Tokenizer\n",
        "    tokenizer = Tokenizer(num_words=VOCAB_SIZE,\n",
        "                        oov_token=\"<oov>\")\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "    # Convert texts to integer sequences\n",
        "    logger.info(\"Converting texts to integer sequences\")\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "    logger.info(\"Conversion successfully done.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error converting words to IDs: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "hVbyNzxexBfL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show example\n",
        "print(f\"Sample tweet: {X_train.iloc[0]}\")\n",
        "print(f\"Tokenized sequence: {X_train_seq[0]}\")\n",
        "# Each word is mapped to an ID between 1 and VOCAB_SIZE-1\n",
        "# <OOV> (out-of-vocabulary) words are mapped to ID=1\n",
        "\n",
        "# Explanation:\n",
        "# - 80000/20000: number of tweets\n",
        "# - 30: MAX_LENGTH\n",
        "# Each element in x_train_padded is a token ID between 0 and VOCAB_SIZE-1 (14999)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3RvZ-gSxBce",
        "outputId": "a82e5da4-91c2-4fef-ac5f-a1a32b1c0863"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample tweet:  you are probably very interesting in real life just a guess but ill give you the benefit of the doubt \n",
            "Tokenized sequence: [8, 34, 398, 110, 685, 11, 405, 176, 21, 5, 255, 20, 101, 316, 8, 4, 5474, 13, 4, 1196]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Padding sequences to uniform length\n",
        "# All tweets should be of MAX_LENGTH\n",
        "# If shorter, pad with 0s (padding='pre'); if longer, truncate from start (truncating='pre')\n",
        "try:\n",
        "    logger.info(\"Padding sequences to uniform length\")\n",
        "    X_train_padded = pad_sequences(X_train_seq,\n",
        "                                maxlen=MAX_LENGTH,\n",
        "                                padding='pre', truncating='pre')\n",
        "    X_test_padded = pad_sequences(X_test_seq,\n",
        "                                maxlen=MAX_LENGTH,\n",
        "                                padding='pre', truncating='pre')\n",
        "    logger.info(\"Padding successfully done.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error padding sequences: {e}\")\n",
        "    raise\n",
        "\n",
        "# Check padded shapes\n",
        "print(f\"x_train_padded shape: {X_train_padded.shape}\")  # e.g., (80000, 30)\n",
        "print(f\"x_test_padded shape: {X_test_padded.shape}\")    # e.g., (20000, 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJAmJO1q1H5k",
        "outputId": "d2b78ed7-447d-4b9f-9e37-efca0f7886e7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train_padded shape: (80000, 30)\n",
            "x_test_padded shape: (20000, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Model"
      ],
      "metadata": {
        "id": "vnWGGBsXzMi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    logger.info(\"Starting to build a LSTM Model.\")\n",
        "    model = Sequential([\n",
        "        # Embedding:\n",
        "        # Input: [batch_size, input_length] ==> e.g [256, 30]\n",
        "        # Output: [batch_size, input_length, embedding_dim] ==> e.g. [256, 30, 128]\n",
        "        Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
        "\n",
        "        # LSTM:\n",
        "        # Input: [batch_size, timesteps, featuers] ==> e.g. [256, 30, 128]\n",
        "        # Output: [batch_size, timesteps, units] = [256, 30, 100] (returen_sequences=True)\n",
        "        LSTM(units=100, return_sequences=True, recurrent_dropout=0.2),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        # LSTM 2:\n",
        "        LSTM(units=100, recurrent_dropout=0.2),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        # Dense:\n",
        "        # for binary classification\n",
        "        # Output: [batch_size, 1], activation='sigmoid'\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    logger.info(\"Model created successfully.\")\n",
        "\n",
        "    # Compile Model\n",
        "    optimizer = tf.keras.optimizers.Adam(clipnorm=0.2)\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        "    logger.info(f\"Model's Optimizer: {optimizer}\\t Loss: {loss}\")\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "    logger.info(\"Model compiled successfully.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error building model: {e}\")\n",
        "    raise\n",
        "\n",
        "# Check summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "CbMipIZ4xBZ2",
        "outputId": "ae266165-afb7-4d05-e5c9-694654fdeb31"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure callbacks\n",
        "# ReduceLROnPlateau: reduce learning rate if val_loss doesn't improve\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              factor=0.2,\n",
        "                              patience=3,\n",
        "                              min_lr=0.0001)\n",
        "# ModelCheckpoint: save the best model based on val_loss\n",
        "checkpoint = ModelCheckpoint('sentiment140_model.h5',\n",
        "                             monitor='val_loss',\n",
        "                             save_best_only=True)"
      ],
      "metadata": {
        "id": "wQnvwIQ50TUw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "# Input: x_train_padded with shape [80000, 30]\n",
        "# Output: y_train with shape [80000,] (binary labels)\n",
        "# validation_split=0.2 means 20% of training data is used for validation\n",
        "try:\n",
        "    logger.info(f\"Starting to train the model for {EPOCHS} epochs\")\n",
        "    history = model.fit(\n",
        "        X_train_padded, y_train,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[reduce_lr, checkpoint],\n",
        "        verbose=1\n",
        "    )\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error occured during training: {str(e)}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1Aw-I9a0Tk2",
        "outputId": "9fe012ea-1d37-45d5-9e53-aad25cddd70b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 508ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 0.0010\n",
            "Epoch 2/15\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 553ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 0.0010\n",
            "Epoch 3/15\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 585ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 0.0010\n",
            "Epoch 4/15\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 566ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 2.0000e-04\n",
            "Epoch 5/15\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 537ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 2.0000e-04\n",
            "Epoch 6/15\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 521ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 2.0000e-04\n",
            "Epoch 7/15\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 515ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 1.0000e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 528ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 1.0000e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 506ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 1.0000e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 516ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 1.0000e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 517ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 1.0000e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 520ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 1.0000e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 512ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 1.0000e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 523ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 1.0000e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 512ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "3BAvQqZx3aDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss & Accuracy plot to evaluate learning process\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "#  Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "#  Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig('sentiment140_metrics.png')"
      ],
      "metadata": {
        "id": "qWIWUxyU2fD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model\n",
        "# Test trained model on test set\n",
        "# Input: X_test_padded ==> [20000, 30]\n",
        "# Output: y_test ==> [, 20000]\n",
        "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test, verbose=0)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIFoGxs93JYy",
        "outputId": "4a7598d5-eb71-47b1-8e57-592a5b891855"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: nan, Test Accuracy: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting with model\n",
        "# Sample tweet\n",
        "sample_tweet = X_test[:1]  # متن خام\n",
        "sample_seq = tokenizer.texts_to_sequences(sample_tweet)\n",
        "sample_padded = pad_sequences(sample_seq, maxlen=MAX_LENGTH, padding='pre', truncating='pre')\n",
        "\n",
        "# sample_padded ==> [1, 30]\n",
        "prediction = model.predict(sample_padded)\n",
        "print(f\"Tweet: {sample_tweet[0]}\")\n",
        "print(f\"Prediction: {'Positive' if prediction[0] > 0.5 else 'Negative'}\")\n",
        "print(f\"True Label {'Positive' if y_test[0] == 1 else 'Negative'}\")"
      ],
      "metadata": {
        "id": "arwk1-PN3JWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J2FFm0903JLo"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}