{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNw0E+Fb6bC3MGnF0fw4e6d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/Deep-Learning-Projects/blob/main/NER-WikiANN-Dataset/NER_WikiANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset"
      ],
      "metadata": {
        "id": "07XhrInxg6xk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2BdQERSgzzX",
        "outputId": "ad07c3e4-be3c-4cf8-98e2-7c053fd4c5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-18 22:04:24--  https://raw.githubusercontent.com/davidsbatista/NER-datasets/refs/heads/master/WikiANN/en/dev\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1002261 (979K) [text/plain]\n",
            "Saving to: ‘dev.1’\n",
            "\n",
            "\rdev.1                 0%[                    ]       0  --.-KB/s               \rdev.1               100%[===================>] 978.77K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2026-02-18 22:04:24 (31.1 MB/s) - ‘dev.1’ saved [1002261/1002261]\n",
            "\n",
            "--2026-02-18 22:04:24--  https://raw.githubusercontent.com/davidsbatista/NER-datasets/refs/heads/master/WikiANN/en/test\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1000256 (977K) [text/plain]\n",
            "Saving to: ‘test.1’\n",
            "\n",
            "test.1              100%[===================>] 976.81K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2026-02-18 22:04:24 (26.8 MB/s) - ‘test.1’ saved [1000256/1000256]\n",
            "\n",
            "--2026-02-18 22:04:24--  https://raw.githubusercontent.com/davidsbatista/NER-datasets/refs/heads/master/WikiANN/en/train\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1994834 (1.9M) [text/plain]\n",
            "Saving to: ‘train.1’\n",
            "\n",
            "train.1             100%[===================>]   1.90M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2026-02-18 22:04:24 (48.2 MB/s) - ‘train.1’ saved [1994834/1994834]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/davidsbatista/NER-datasets/refs/heads/master/WikiANN/en/dev\n",
        "!wget https://raw.githubusercontent.com/davidsbatista/NER-datasets/refs/heads/master/WikiANN/en/test\n",
        "!wget https://raw.githubusercontent.com/davidsbatista/NER-datasets/refs/heads/master/WikiANN/en/train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "n019YArSviey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8DXBp79fg6a_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "uB-M89UkxAOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(file_path):\n",
        "    \"\"\"Loads a dataset from a given file path into a list of lines.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the dataset file.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of strings, where each string is a line from the file.\n",
        "    \"\"\"\n",
        "    with open(file_path) as f:\n",
        "        file_set = f.readlines()\n",
        "    return file_set\n",
        "\n",
        "def prepare_split(file_set):\n",
        "    \"\"\"Processes a raw dataset (list of lines) into tokenized sentences and their corresponding labels.\n",
        "    Each sentence is separated by a blank line in the input file.\n",
        "\n",
        "    Args:\n",
        "        file_set (list): A list of strings, each representing a line from the raw dataset file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two lists:\n",
        "            - sentences_tokens (list[list[str]]): A list of sentences, where each sentence is a list of tokens.\n",
        "            - sentences_labels (list[list[str]]): A list of sentences, where each sentence is a list of labels.\n",
        "    \"\"\"\n",
        "    sentences_tokens = []\n",
        "    sentences_labels = []\n",
        "    current_sentence_tokens = []\n",
        "    current_sentence_labels = []\n",
        "\n",
        "    for line in file_set:\n",
        "        line = line.strip()\n",
        "        if not line:  # An empty line signals the end of a sentence\n",
        "            if current_sentence_tokens:  # Add the collected sentence if it's not empty\n",
        "                sentences_tokens.append(current_sentence_tokens)\n",
        "                sentences_labels.append(current_sentence_labels)\n",
        "            # Reset for the next sentence\n",
        "            current_sentence_tokens = []\n",
        "            current_sentence_labels = []\n",
        "        else:\n",
        "            # Ensure the line has the expected 'lang:token label' format before splitting\n",
        "            parts = line.replace(\"\\t\", \" \").split(\" \", 1)\n",
        "            if len(parts) == 2 and ':' in parts[0]:\n",
        "                token_part = parts[0]\n",
        "                label_part = parts[1]\n",
        "\n",
        "                # Extract the token, removing the language prefix and any surrounding quotes\n",
        "                token = token_part.split(\":\", 1)[1].strip(\"'\\\".\")\n",
        "                label = label_part\n",
        "\n",
        "                current_sentence_tokens.append(token)\n",
        "                current_sentence_labels.append(label)\n",
        "\n",
        "    # Add the very last sentence if the file doesn't end with an empty line\n",
        "    if current_sentence_tokens:\n",
        "        sentences_tokens.append(current_sentence_tokens)\n",
        "        sentences_labels.append(current_sentence_labels)\n",
        "\n",
        "    return sentences_tokens, sentences_labels\n",
        "\n",
        "def create_vocabulary(split):\n",
        "    \"\"\"Generates a sorted list of unique tokens (vocabulary) from a list of tokenized sentences.\n",
        "\n",
        "    Args:\n",
        "        split (list[list[str]]): A list of sentences, where each sentence is a list of tokens.\n",
        "\n",
        "    Returns:\n",
        "        list: A sorted list of unique tokens, representing the vocabulary.\n",
        "    \"\"\"\n",
        "    # Flatten the list of lists into a single list of tokens for vocabulary creation\n",
        "    all_tokens = [token for sentence in split for token in sentence]\n",
        "    return sorted(list(set(all_tokens)))\n",
        "\n",
        "def get_tag_list(train_tags):\n",
        "    \"\"\"Extracts and sorts a list of all unique named entity tags from a list of tag sequences.\n",
        "\n",
        "    Args:\n",
        "        train_tags (list[list[str]]): A list of tag sequences, where each sequence is a list of tags for a sentence.\n",
        "\n",
        "    Returns:\n",
        "        list: A sorted list of unique named entity tags.\n",
        "    \"\"\"\n",
        "    # Flatten the list of lists into a single list of tags to find all unique tags\n",
        "    all_tags = [tag for sentence_tags in train_tags for tag in sentence_tags]\n",
        "    return sorted(list(set(all_tags)))\n",
        "\n",
        "def create_word_indices(vocab):\n",
        "    \"\"\"Creates word-to-index and index-to-word mappings for the vocabulary.\n",
        "    Special tokens [PAD] and [UNK] are added at indices 0 and 1, respectively.\n",
        "\n",
        "    Args:\n",
        "        vocab (list): A sorted list of unique words (the vocabulary).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two dictionaries:\n",
        "            - word2idx (dict): Maps words to their integer indices.\n",
        "            - idx2word (dict): Maps integer indices back to words.\n",
        "    \"\"\"\n",
        "    # Assign indices to words, starting from 2 to reserve 0 and 1 for special tokens\n",
        "    word2idx = {word: idx + 2 for idx, word in enumerate(vocab)}\n",
        "    word2idx[\"[PAD]\"] = 0  # Padding token for shorter sequences\n",
        "    word2idx[\"[UNK]\"] = 1  # Unknown token for words not in the vocabulary\n",
        "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "    return word2idx, idx2word\n",
        "\n",
        "def create_tag_indices(tags):\n",
        "    \"\"\"Creates tag-to-index and index-to-tag mappings for the named entity tags.\n",
        "\n",
        "    Args:\n",
        "        tags (list): A sorted list of unique named entity tags.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two dictionaries:\n",
        "            - tag2idx (dict): Maps tags to their integer indices.\n",
        "            - idx2tag (dict): Maps integer indices back to tags.\n",
        "    \"\"\"\n",
        "    tag2idx = {tag: idx for idx, tag in enumerate(tags)}\n",
        "    idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
        "    return tag2idx, idx2tag\n",
        "\n",
        "def preprocess_and_padding(tokens_list_of_sentences, tags_list_of_sentences, word2idx, tag2idx, maxlen):\n",
        "    \"\"\"Converts token and tag sequences to numerical representations and applies padding/truncation.\n",
        "\n",
        "    Args:\n",
        "        tokens_list_of_sentences (list[list[str]]): A list of tokenized sentences.\n",
        "        tags_list_of_sentences (list[list[str]]): A list of tag sequences for the sentences.\n",
        "        word2idx (dict): Dictionary mapping words to integer indices.\n",
        "        tag2idx (dict): Dictionary mapping tags to integer indices.\n",
        "        maxlen (int): The maximum length for sequences. Shorter sequences are padded; longer ones are truncated.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two numpy arrays:\n",
        "            - X_padded (np.array): Padded and numerical token sequences.\n",
        "            - y_padded (np.array): Padded and numerical tag sequences.\n",
        "    \"\"\"\n",
        "    X_padded = []\n",
        "    for sentence_tokens in tokens_list_of_sentences:\n",
        "        # Convert tokens to their integer IDs, using [UNK] for unknown words\n",
        "        encoded_tokens = [word2idx.get(token, word2idx[\"[UNK]\"]) for token in sentence_tokens]\n",
        "\n",
        "        if len(encoded_tokens) > maxlen:\n",
        "            # Truncate if the sequence is longer than maxlen\n",
        "            X_padded.append(encoded_tokens[:maxlen])\n",
        "        else:\n",
        "            # Pad with [PAD] token index (0) if the sequence is shorter\n",
        "            padded_tokens = encoded_tokens + [word2idx[\"[PAD]\"]] * (maxlen - len(encoded_tokens))\n",
        "            X_padded.append(padded_tokens)\n",
        "\n",
        "    y_padded = []\n",
        "    # The 'O' (Outside) tag is commonly used for padding labels. Find its index.\n",
        "    pad_tag_idx = tag2idx['O']  # Assuming 'O' is always present and appropriate for padding\n",
        "\n",
        "    for sentence_tags in tags_list_of_sentences:\n",
        "        # Convert tags to their integer IDs\n",
        "        encoded_tags = [tag2idx[tag] for tag in sentence_tags]\n",
        "\n",
        "        if len(encoded_tags) > maxlen:\n",
        "            # Truncate if the sequence is longer than maxlen\n",
        "            y_padded.append(encoded_tags[:maxlen])\n",
        "        else:\n",
        "            # Pad with 'O' tag index if the sequence is shorter\n",
        "            padded_tags = encoded_tags + [pad_tag_idx] * (maxlen - len(encoded_tags))\n",
        "            y_padded.append(padded_tags)\n",
        "\n",
        "    return np.array(X_padded), np.array(y_padded)\n",
        "\n",
        "def build_model(vocab_size, tag_output_dim, embedding_dim, lstm_units, maxlen):\n",
        "    \"\"\"Constructs a Bidirectional LSTM model for Named Entity Recognition.\n",
        "\n",
        "    The model consists of:\n",
        "    - An Embedding layer to convert integer-encoded words into dense vectors.\n",
        "    - A Bidirectional LSTM layer to capture context from both directions of a sequence.\n",
        "    - A Dense layer with softmax activation for multi-class classification (tag prediction).\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): The total number of unique words in the vocabulary (including [PAD] and [UNK]).\n",
        "        tag_output_dim (int): The total number of unique named entity tags.\n",
        "        embedding_dim (int): The dimensionality of the word embedding vectors.\n",
        "        lstm_units (int): The number of units in the LSTM layer.\n",
        "        maxlen (int): The maximum length of input sequences.\n",
        "\n",
        "    Returns:\n",
        "        tensorflow.keras.Model: The compiled Keras model ready for training.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
        "        Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
        "        Dense(tag_output_dim, activation='softmax')\n",
        "    ])\n",
        "    # Explicitly build the model to determine output shapes and parameter counts\n",
        "    model.build(input_shape=(None, maxlen))  # (batch_size, sequence_length)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_model(model, X_train, y_train, X_dev, y_dev, callbacks, epochs=5, batch_size=32):\n",
        "    \"\"\"Trains the provided Keras model using the training and development datasets.\n",
        "\n",
        "    Args:\n",
        "        model (tensorflow.keras.Model): The compiled Keras model to be trained.\n",
        "        X_train (np.array): Training data (numerical token sequences).\n",
        "        y_train (np.array): Training labels (numerical tag sequences).\n",
        "        X_dev (np.array): Development/validation data (numerical token sequences).\n",
        "        y_dev (np.array): Development/validation labels (numerical tag sequences).\n",
        "        callbacks (list): A list of Keras callbacks to apply during training (e.g., ModelCheckpoint, EarlyStopping).\n",
        "        epochs (int, optional): The number of training epochs. Defaults to 5.\n",
        "        batch_size (int, optional): The number of samples per gradient update. Defaults to 32.\n",
        "\n",
        "    Returns:\n",
        "        tensorflow.keras.callbacks.History: A History object containing training metrics.\n",
        "    \"\"\"\n",
        "    history = model.fit(X_train, y_train,\n",
        "              validation_data=(X_dev, y_dev),\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              callbacks=callbacks,\n",
        "              verbose=1)\n",
        "    return model"
      ],
      "metadata": {
        "id": "K2y-7b5ahHjr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution"
      ],
      "metadata": {
        "id": "-PsFeJSxxofL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Datasets\n",
        "train = load_dataset(\"/content/train\")\n",
        "test = load_dataset(\"/content/test\")\n",
        "dev = load_dataset(\"/content/dev\")\n",
        "\n",
        "# Split datasets into sentences\n",
        "train_sentences, train_tags = prepare_split(train)\n",
        "test_sentences, test_tags = prepare_split(test)\n",
        "dev_sentences, dev_tags = prepare_split(dev)\n",
        "\n",
        "print(f\"Train Sentences: {len(train_sentences)} \\tTrain Tags: {len(train_tags)}\")\n",
        "print(f\"Test Sentences: {len(test_sentences)} \\tTest Tags: {len(test_tags)}\")\n",
        "print(f\"Dev Sentences: {len(dev_sentences)} \\tDev Tags: {len(dev_tags)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWB743jLkEXm",
        "outputId": "17f3d966-056d-479d-a4d2-73a2b3502984"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Sentences: 20000 \tTrain Tags: 20000\n",
            "Test Sentences: 10000 \tTest Tags: 10000\n",
            "Dev Sentences: 10000 \tDev Tags: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get tags list\n",
        "tags = get_tag_list(train_tags)\n",
        "print(f\"Number of Tags: {len(tags)}\")\n",
        "# Create vocabulary\n",
        "vocab = create_vocabulary(train_sentences)\n",
        "print(f\"Length of Vocabulary: {len(vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO_sHEpUkKEY",
        "outputId": "ace660ef-6424-42ea-99f8-b71246cf7851"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Tags: 7\n",
            "Length of Vocabulary: 33393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate word2idx and idx2word\n",
        "word2idx, idx2word = create_word_indices(vocab)\n",
        "\n",
        "# Generate tag2idx and idx2tag\n",
        "tag2idx, idx2tag = create_tag_indices(tags)\n",
        "\n",
        "print(f\"[PAD] index: {word2idx.get('[PAD]', 'Not Found')}\")\n",
        "print(f\"[UNK] index: {word2idx.get('[UNK]', 'Not Found')}\")\n",
        "\n",
        "# Creating X/ y sets for all splits\n",
        "X_train, y_train = preprocess_and_padding(train_sentences, train_tags, word2idx, tag2idx, maxlen=100)\n",
        "X_test, y_test = preprocess_and_padding(test_sentences, test_tags, word2idx, tag2idx, maxlen=100)\n",
        "X_dev, y_dev = preprocess_and_padding(dev_sentences, dev_tags, word2idx, tag2idx, maxlen=100)\n",
        "\n",
        "# Check shapes\n",
        "print(f\"X_Train shape: {X_train.shape}\\ty_Train shape: {y_train.shape}\")\n",
        "print(f\"X_test Shape: {X_test.shape}\\ty_test shape: {y_test.shape}\")\n",
        "print(f\"X_dev Shape: {X_dev.shape}\\ty_dev Shape: {y_dev.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9PKJUvpk4Qm",
        "outputId": "d4432ad3-bced-4fad-a85a-f1b1b05fd98d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PAD] index: 0\n",
            "[UNK] index: 1\n",
            "X_Train shape: (20000, 100)\ty_Train shape: (20000, 100)\n",
            "X_test Shape: (10000, 100)\ty_test shape: (10000, 100)\n",
            "X_dev Shape: (10000, 100)\ty_dev Shape: (10000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "vocab_size = len(vocab) + 2 # +2 for [PAD] and [UNK] tokens\n",
        "tag_output_dim = len(tags)\n",
        "maxlen = 100\n",
        "embedding_dim = 100\n",
        "lstm_units = 128\n",
        "\n",
        "# Build the model using the function\n",
        "model = build_model(vocab_size, tag_output_dim, embedding_dim, lstm_units, maxlen)\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "8NmIzoV7oqGs",
        "outputId": "f6913471-7192-4e17-ff83-5bfe4d73f3fe"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m3,339,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m234,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m7\u001b[0m)         │         \u001b[38;5;34m1,799\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,339,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">234,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,799</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,575,795\u001b[0m (13.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,575,795</span> (13.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,575,795\u001b[0m (13.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,575,795</span> (13.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up callbacks to control training behavior and save the best model\n",
        "model_callbacks = [\n",
        "    # Save the best model based on validation loss\n",
        "    ModelCheckpoint(\n",
        "        filepath='model.weights.keras',\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss',\n",
        "    ),\n",
        "    # Stop training early if validation loss doesn't improve for 3 epochs\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=3,\n",
        "        restore_best_weights=True,\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train the model using our prepared data and callbacks\n",
        "history = train_model(\n",
        "    model, X_train, y_train,\n",
        "    X_dev, y_dev, model_callbacks, epochs=12, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5J8Zb4xwZm2",
        "outputId": "474a7242-a3ae-4729-ef68-fe851c6cf572"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.9541 - loss: 0.1541 - val_accuracy: 0.9825 - val_loss: 0.0532\n",
            "Epoch 2/12\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.9880 - loss: 0.0387 - val_accuracy: 0.9866 - val_loss: 0.0402\n",
            "Epoch 3/12\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.9953 - loss: 0.0165 - val_accuracy: 0.9878 - val_loss: 0.0382\n",
            "Epoch 4/12\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - accuracy: 0.9976 - loss: 0.0084 - val_accuracy: 0.9875 - val_loss: 0.0438\n",
            "Epoch 5/12\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.9987 - loss: 0.0050 - val_accuracy: 0.9870 - val_loss: 0.0486\n",
            "Epoch 6/12\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 0.9870 - val_loss: 0.0523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check Classification Report"
      ],
      "metadata": {
        "id": "KQOaskIh0u8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model predictions on the test set\n",
        "y_pred_probs = model.predict(X_test)\n",
        "\n",
        "# Convert predicted probabilities to actual tag labels\n",
        "y_pred = np.argmax(y_pred_probs, axis=-1)\n",
        "\n",
        "# Get the index of the 'O' (Outside) tag, used for padding\n",
        "O_tag_idx = tag2idx['O']\n",
        "\n",
        "# Prepare true and predicted labels, ignoring padding tokens for evaluation\n",
        "y_true_flat = []\n",
        "y_pred_flat = []\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    for j in range(len(y_test[i])):\n",
        "        # Only evaluate non-padding tokens\n",
        "        if y_test[i][j] != O_tag_idx:\n",
        "            y_true_flat.append(y_test[i][j])\n",
        "            y_pred_flat.append(y_pred[i][j])\n",
        "\n",
        "# Get tag names and their numerical indices for the classification report (excluding 'O')\n",
        "true_tags = [idx2tag[i] for i in sorted(tag2idx.values()) if idx2tag[i] != 'O']\n",
        "report_labels = [i for i in sorted(tag2idx.values()) if i != O_tag_idx]\n",
        "\n",
        "# Print the detailed classification report\n",
        "print(\"\\nClassification Report on Test Set (excluding padding tokens):\\n\")\n",
        "print(classification_report(y_true_flat, y_pred_flat, target_names=true_tags, labels=report_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_j1DIoZw4d-",
        "outputId": "53f7709c-b603-438a-f6cf-b3072b607015"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
            "\n",
            "Classification Report on Test Set (excluding padding tokens):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.78      0.74      0.76      4657\n",
            "       B-ORG       0.73      0.65      0.69      4745\n",
            "       B-PER       0.79      0.85      0.82      4521\n",
            "       I-LOC       0.80      0.71      0.76      6447\n",
            "       I-ORG       0.83      0.73      0.78     11607\n",
            "       I-PER       0.84      0.79      0.82      7437\n",
            "\n",
            "   micro avg       0.81      0.74      0.77     39414\n",
            "   macro avg       0.80      0.75      0.77     39414\n",
            "weighted avg       0.81      0.74      0.77     39414\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IJPM-FTQyiIH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}