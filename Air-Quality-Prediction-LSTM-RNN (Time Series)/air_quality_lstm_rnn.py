# -*- coding: utf-8 -*-
"""Air-Quality - LSTM-RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CAki2E6nQ-sob8rGFWc6wuYg8IFQgXAn

# Download Dataset & Setup Project
"""

#------------------#
# Download Dataset #
#------------------#
import kagglehub

path = kagglehub.dataset_download("rohanrao/air-quality-data-in-india")
print("Path to dataset files:", path)

#------------------#
# Import Libraries #
#------------------#
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

from sklearn.preprocessing import MinMaxScaler, RobustScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import tensorflow as tf
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping

from datetime import datetime
import logging
import warnings

#-----------------#
# Loading Dataset #
#-----------------#
df = pd.read_csv("city_hour.csv")
df.head(5)

#-----------#
# Setup Env #
#-----------#
warnings.filterwarnings('ignore')
logging.basicConfig(
    format='%(asctime)s : %(levelname)s : %(message)s',
    level=logging.DEBUG,
    handlers=[
        logging.FileHandler('air_quality.log'),
        logging.StreamHandler()
    ])
logging.getLogger('matplotlib').setLevel(logging.INFO)
logger = logging.getLogger(__name__)

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

df.head(5)

numeric_cols = df.select_dtypes(include='number').columns
n_cols = len(numeric_cols)
n_cols_per_row = 3
n_rows = (n_cols + n_cols_per_row - 1) // n_cols_per_row

plt.figure(figsize=(15, 3 * n_rows))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(n_rows, n_cols_per_row, i)
    data = df[col].dropna()
    if data.min() <= 0:
        plt.hist(data, bins=200, color='skyblue', edgecolor='black')
    else:
        plt.hist(data, bins=200, color='skyblue', edgecolor='black', log=True)
        plt.yscale('log')
    plt.title(f'{col} Distribution')
    plt.xlabel(f'{col}')
    plt.ylabel('Frequency')

plt.tight_layout()
plt.savefig('distribution.png')
plt.show()
plt.close()
logger.info("Histogram plot saved as 'distribution.png'")

"""# Preparing Data"""

#---------------#
# Perprocessing #
#---------------#

# def preprocess(city="Delhi"):
#     logger.info(f"Starting preprocessing for {city}")
#     try:
#         df_city = df[df["City"] == city]                                        # Filter city
#         if df_city.empty:
#             raise ValueError(f"No data found for {city}")
#         logger.info(f"Filetered data for {city}. Shape: {df_city.shape}")


#         df_city['Datetime'] = pd.to_datetime(df['Datetime'])                    # Corret datetime
#         logger.debug("Converedted datatime column to datetime format")

#         # Define features and target
#         features = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2',
#                     'O3', 'Benzene', 'Toluene', 'Xylene']
#         target = 'AQI'
#         logger.debug(f"Features: {features}, Target: {target}")
#         # Check if all features and target exits
#         missing_cols = [col for col in features + [target] if col not in df_city.columns]
#         if missing_cols:
#             raise ValueError(f"Missing Cols: {missing_cols}")

#         # Log transformation for features & targe
#         for col in features + [target]:
#             if (df_city[col] < 0).any():
#                 logger.warning(f"Column {col} contains negative values. Skipping Log Transformation")
#                 continue
#             df_city[col] = np.log1p(df_city[col])
#             logger.info(f"Applied log transformation to {col}")

#         # Filling missing values with mean
#         for col in features + [target]:
#             if df_city[col].isnull().any():
#                 df_city[col] = df_city[col].fillna(df[col].mean())
#                 logger.info(f"Filling missing value: {col} with mean: {df[col].mean()}")
#             else:
#                 logger.debug(f"No Missing values in {col}")

#         # Normalization
#         scaler = RobustScaler()
#         df_scaled = scaler.fit_transform(df_city[features + [target]])
#         logger.info("Normalization done.")

#         # Saving Scaler for reverse normalization
#         # Features
#         feature_scaler = MinMaxScaler()
#         feature_scaler.fit(df_city[features])
#         logger.debug("Feature scaler crated successfully")

#         # Target
#         target_scaler = MinMaxScaler()
#         target_scaler.fit(df_city[[target]])
#         logger.debug("Target scaler crated successfully")

#         return df_scaled, features, target, feature_scaler, target_scaler
#     except Exception as e:
#         logger.error(f"Preprocessing failed: {str(e)}")
#         raise

def preprocess(city="Delhi"):
    logger.info(f"Starting preprocessing for {city}")
    try:
        df_city = df[df["City"] == city]
        if df_city.empty:
            raise ValueError(f"No data found for {city}")
        logger.info(f"Filtered data for {city}. Shape: {df_city.shape}")

        df_city['Datetime'] = pd.to_datetime(df['Datetime'])
        logger.debug("Converted datetime column to datetime format")

        features = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2',
                    'O3', 'Benzene', 'Toluene', 'Xylene']
        target = 'AQI'
        logger.debug(f"Features: {features}, Target: {target}")

        missing_cols = [col for col in features + [target] if col not in df_city.columns]
        if missing_cols:
            raise ValueError(f"Missing Cols: {missing_cols}")

        # Log transformation only for AQI
        if (df_city[target] < 0).any():
            logger.warning(f"Target {target} contains negative values. Skipping Log Transformation")
        else:
            df_city[target] = np.log1p(df_city[target])
            logger.info(f"Applied log transformation to {target}")

        for col in features + [target]:
            if df_city[col].isnull().any():
                df_city[col] = df_city[col].fillna(df_city[col].mean())
                logger.info(f"Filling missing value: {col} with mean: {df_city[col].mean()}")
            else:
                logger.debug(f"No Missing values in {col}")
        # Normalization
        scaler = RobustScaler()
        df_scaled = scaler.fit_transform(df_city[features + [target]])
        logger.info("Normalization done with RobustScaler.")

        # Saving Scaler for reverse normalization
        # Features
        feature_scaler = RobustScaler()
        feature_scaler.fit(df_city[features])
        # Target
        target_scaler = RobustScaler()
        target_scaler.fit(df_city[[target]])

        logger.info(f"Scaled AQI range: min={df_scaled[:, -1].min():.4f}, max={df_scaled[:, -1].max():.4f}")
        return df_scaled, features, target, feature_scaler, target_scaler
    except Exception as e:
        logger.error(f"Preprocessing failed: {str(e)}")
        raise
#--------------------#
# Creating Sequences #
#--------------------#

def create_sequences(data, seq_length):
    logger.info(f"Creating sequences with length: {seq_length}")
    try:
        if data.shape[1] < 2:
            raise ValueError("Data must have at least one feature and one target")
        if seq_length <= 0:
            raise ValueError("Sequence length must be positive")

        X, y = [], []
        for i in range(len(data) - seq_length):
            X.append(data[i: i + seq_length, :-1])
            y.append(data[i + seq_length, -1])
        X, y = np.array(X), np.array(y)

        logger.info(f"Sequence created. X Shape: {X.shape}, y Shape: {y.shape}")
        return X, y
    except Exception as e:
        logger.error(f"Sequence creation failed: {str(e)}")
        raise

#------------------#
# Define RNN Model #
#------------------#

def rnn_model(seq_length, n_features):
    logger.info("Buidling LSTM Model")
    try:
        model = tf.keras.Sequential([
            tf.keras.layers.SimpleRNN(128, input_shape=(seq_length, n_features),
                                    return_sequences=True),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.SimpleRNN(64),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dense(1)
        ])
        # Compiling Model
        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                     loss=tf.keras.losses.Huber(),
                     metrics=['mae'])
        logger.info("RNN model compiled successfully")
        logger.debug(f"RNN model summary:\n{model.summary()}")
        return model
    except Exception as e:
        logger.error(f"RNN model creation failed: {str(e)}")
        raise

#-------------------#
# Define LSTM Model #
#-------------------#

def lstm_model(seq_length, n_features):
    logger.info("Buidling LSTM Model")
    try:
        model = tf.keras.Sequential([
            tf.keras.layers.LSTM(128, input_shape=(seq_length, n_features),
                                return_sequences=True),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.LSTM(64),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dense(1)
        ])
        # Compiling the Model
        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                     loss=tf.keras.losses.Huber(),
                     metrics=['mae'])
        logger.info("LSTM model compiled successfully")
        logger.debug(f"LSTM model summary:\n{model.summary()}")
        return model
    except Exception as e:
        logger.error(f"LSTM model creation failed: {str(e)}")
        raise

#--------------------#
# Evaluate the Modes #
#--------------------#
def evaluate_and_plot(y_true, y_pred_rnn,
                      y_pred_lstm, history_rnn,
                      history_lstm, target_scaler):
    logger.info("Evaluating models and generating plots")
    try:
        # Inversing Normalization for predictions
        y_true = target_scaler.inverse_transform(y_true.reshape(-1, 1))
        y_pred_rnn = target_scaler.inverse_transform(y_pred_rnn)
        y_pred_lstm = target_scaler.inverse_transform(y_pred_lstm)
        logger.debug("Inverse normalization completed")

        y_true = np.expm1(y_true)
        y_pred_rnn = np.expm1(y_pred_rnn)
        y_pred_lstm = np.expm1(y_pred_lstm)
        logger.info("Applied inverse log transformation to predictions")

        logger.info(f"True AQI range after inverse transform: min={y_true.min():.2f}, max={y_true.max():.2f}")
        logger.info(f"RNN Predictions range: min={y_pred_rnn.min():.2f}, max={y_pred_rnn.max():.2f}")
        logger.info(f"LSTM Predictions range: min={y_pred_lstm.min():.2f}, max={y_pred_lstm.max():.2f}")

        # Calculate metrics
        def print_metrics(y_true, y_pred, model_name):
            mse = mean_squared_error(y_true, y_pred)
            mae = mean_absolute_error(y_true, y_pred)
            r2 = r2_score(y_true, y_pred)
            logger.info(f"{model_name} Metrics: MSE={mse:.4f}, MAE={mae:.4f}, R²={r2:.4f}")
            print(f"{model_name} Metrics:")
            print(f"MSE: {mse:.4f}")
            print(f"MAE: {mae:.4f}")
            print(f"R²: {r2:.4f}\n")

        print_metrics(y_true, y_pred_rnn, "RNN")
        print_metrics(y_true, y_pred_lstm, "LSTM")

        # Plot predictions
        plt.figure(figsize=(15, 6))
        plt.plot(y_true, label='Actual AQI', color='blue')
        plt.plot(y_pred_rnn, label='RNN Predictions', color='red', linestyle='--')
        plt.plot(y_pred_lstm, label='LSTM Predictions', color='green', linestyle='--')
        plt.title('AQI Prediction Comparison: RNN vs LSTM')
        plt.xlabel('Time Step')
        plt.ylabel('AQI')
        plt.legend()
        plt.grid(True)
        plt.savefig('aqi_predictions.png')
        plt.close()
        logger.info("Prediction plot saved as 'aqi_predictions.png'")

        plt.figure(figsize=(15, 6))
        plt.plot(history_rnn.history['loss'], label='RNN Training Loss', color='blue')
        plt.plot(history_rnn.history['val_loss'], label='RNN Validation Loss', color='blue', linestyle='--')
        plt.plot(history_lstm.history['loss'], label='LSTM Training Loss', color='green')
        plt.plot(history_lstm.history['val_loss'], label='LSTM Validation Loss', color='green', linestyle='--')
        plt.title('Training and Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        plt.savefig('aqi_loss_history.png')
        plt.close()
        logger.info("Loss history plot saved as 'aqi_loss_history.png'")
    except Exception as e:
        logger.error(f"Error during evaluating and plotting: {str(e)}")
        raise

"""# Train the Models"""

#------------------#
# Main Function #
#------------------#
def main():
    logger.info("Starting main execution")
    SEQ_LENGTH = 48
    N_EPOCHS = 50
    BATCH_SIZE = 32
    logger.debug(f"Parameters: SEQ_LENGTH={SEQ_LENGTH}, N_EPOCHS={N_EPOCHS}, BATCH_SIZE={BATCH_SIZE}")

    try:
        # Preprocessing
        data, features, target, feature_scaler, target_scaler = preprocess()

        # Create sequences
        X, y = create_sequences(data, SEQ_LENGTH)

        # Split Data
        train_size = int(0.8 * len(X))
        logger.info(f"Training size: {train_size}, Test size: {len(X) - train_size}")
        X_train, X_test = X[:train_size], X[train_size:]
        y_train, y_test = y[:train_size], y[train_size:]
        logger.debug(f"Train shapes: X={X_train.shape}, y={y_train.shape}")
        logger.debug(f"Test shapes: X={X_test.shape}, y={y_test.shape}")

        # Define LearningScheduler
        lr_scheduler = ReduceLROnPlateau(
              monitor='val_loss',
              factor=0.5,
              patience=3,
              min_lr=1e-6,
              verbose=1
          )
        # Define EarlyStopping callback
        early_stopping = EarlyStopping(
              monitor='val_loss',
              patience=20,
              restore_best_weights=True,
              verbose=1
          )

        # Create RNN model
        logger.info("Training RNN model")
        model_rnn = rnn_model(SEQ_LENGTH, len(features))
        history_rnn = model_rnn.fit(X_train, y_train,
                                    epochs=N_EPOCHS,
                                    batch_size=BATCH_SIZE,
                                    validation_data=(X_test, y_test),
                                    callbacks=[lr_scheduler, early_stopping],
                                    verbose=1)
        logger.info("RNN model training completed")

        # Create LSTM model
        logger.info("Training LSTM model")
        model_lstm = lstm_model(SEQ_LENGTH, len(features))
        history_lstm = model_lstm.fit(X_train, y_train,
                                        epochs=N_EPOCHS,
                                        batch_size=BATCH_SIZE,
                                        validation_data=(X_test, y_test),
                                        callbacks=[lr_scheduler, early_stopping],
                                        verbose=1)
        logger.info("LSTM model training completed")

        # Prediction
        logger.info("Generating Predictions")
        rnn_predictions = model_rnn.predict(X_test)
        lstm_predictions = model_lstm.predict(X_test)
        logger.debug(f"Prediction shapes: RNN={rnn_predictions.shape}, LSTM={lstm_predictions.shape}")

        # Evaluate and show the results
        evaluate_and_plot(y_test, rnn_predictions, lstm_predictions,
                        history_rnn, history_lstm, target_scaler)
        logger.info("Main execution completed successfully")
    except Exception as e:
        logger.error(f"Main execution failed: {str(e)}")
        raise
main()









