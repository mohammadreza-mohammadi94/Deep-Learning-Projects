# -*- coding: utf-8 -*-
"""Next Word Prediction - RNNs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16QP9Je97qBX6VMY9vRi5Iso_p9f0QWNB

# Import Frameworks and Setup Enviorment
"""

# Install libs
!pip install -q datasets

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding
from tensorflow.keras.optimizers import Adam
import datasets
from datasets import load_dataset
import re
import numpy as np
import matplotlib.pyplot as plt

# Setup warnings
import warnings
warnings.filterwarnings('ignore')

# Setup logging
import logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[
                        logging.FileHandler('training.log'),
                        logging.StreamHandler()
                    ])

"""# Load Dataset"""

def load_wiki_dataset():
    dataset = load_dataset("wikipedia", "20220301.simple")
    texts = dataset['train']['text']
    cleaned_texts = []
    for text in texts[:1000]:
        text = re.sub(r'\[.*?\]|\n|\t', ' ', text)
        text = re.sub(r'[^a-zA-Z\s]', ' ', text)
        sentences = text.lower().split('.')
        cleaned_texts.extend([s.strip() for s in sentences if len(s.strip().split()) > 2])
    return cleaned_texts

"""# Preparing Data"""

def prepare_data(texts):
    # tokenization
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(texts)
    sequences = tokenizer.texts_to_sequences(texts)

    # X, y
    X, y = [], []
    max_len = 5     # Maximum len of sequence
    vocab_size = len(tokenizer.word_index) + 1

    for seq in sequences:
        for i in range(1, len(seq)):
            n_grams = seq[max(0, i - max_len): i]
            X.append(pad_sequences([n_grams], maxlen=max_len, padding='pre')[0])
            y.append(seq[i])

    X = np.array(X)
    y = np.array(y)
    y = np.expand_dims(y, -1)

    print(f"Count of Sequences: {len(X)}")
    print(f"Vocab Size: {vocab_size}")
    print(f"Max Length: {max_len}")

    return X, y, vocab_size, tokenizer, max_len

"""# Build Model"""

def build_model(vocab_size, max_len, embedding_dim, lstm_units=128):
    inputs = Input(shape=(max_len,))
    x = Embedding(vocab_size, embedding_dim)(inputs)
    x = LSTM(lstm_units, return_sequences=False)(x)
    x = Dense(vocab_size, activation='softmax')(x)

    model = Model(inputs, x)
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# Prediction function
def predict_next_word(model, tokenizer, text, max_len):
    sequence = tokenizer.texts_to_sequences([text.lower().split()[-max_len:]])
    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='pre')
    prediction = model.predict(padded_sequence, verbose=0)
    predicted_word_index = np.argmax(prediction[0])
    predicted_word = [word for word, index in tokenizer.word_index.items() if index == predicted_word_index][0]
    return predicted_word

"""# Run Model"""

# Load and preparing data
texts = load_wiki_dataset()
X, y, vocab_size, tokenizer, max_len = prepare_data(texts)

# Creating and fitting the model
model = build_model(vocab_size, max_len, embedding_dim=50)
model.summary()

# Fit
history = model.fit(X, y, batch_size=32, epochs=50, validation_split=0.2, verbose=1)
model.save('next_word_predictor.h5')

# Prediction
test_sentences = ["i go to", "the cat is", "she likes to"]
for sentence in test_sentences:
    next_word = predict_next_word(model, tokenizer, sentence, max_len)
    print(f"Input: {sentence} -> Prediction: {next_word}")