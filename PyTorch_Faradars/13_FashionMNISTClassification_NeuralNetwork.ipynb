{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnJHflnmJ7h1LrnIGhHrab",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/Deep_Learning_Projects/blob/main/PyTorch_Faradars/13_FashionMNISTClassification_NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VyGUVDKjXjMV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Retrieval"
      ],
      "metadata": {
        "id": "TEjcvgzgZ6sO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.FashionMNIST(\n",
        "    root='./train',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root='./test',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "treVaIGAZ50C",
        "outputId": "61b6c4d1-3be1-4b61-86da-ef1476bf7306"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./train/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 16758748.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./train/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./train/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./train/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 283041.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./train/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./train/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./train/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 4980260.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./train/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./train/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./train/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 13411352.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./train/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./train/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./test/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 17903815.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./test/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./test/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./test/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 269420.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./test/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./test/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./test/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5057906.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./test/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./test/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./test/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 17497793.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./test/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./test/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Training Device"
      ],
      "metadata": {
        "id": "A0NhYXHVaeWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using {device} device')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzto6OV-aZvp",
        "outputId": "2e259146-b37b-4db8-d0fa-98dac2d1bd2d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Model"
      ],
      "metadata": {
        "id": "nxCtt3HtcIDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model's architecture:**\n",
        "\n",
        "Three layers:\n",
        "* `Fully-connected Layer_1 --> (784, 64)`\n",
        "* `Fully-connected Layer_2 --> (64, 64)`\n",
        "* `Fully-connected Layer_3 --> (64, 10)`"
      ],
      "metadata": {
        "id": "g32rpJ6KcK87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features = 784,\n",
        "                             out_features = 64)\n",
        "        self.fc2 = nn.Linear(in_features = 64,\n",
        "                             out_features = 64)\n",
        "        self.fc3 = nn.Linear(in_features = 64,\n",
        "                             out_features = 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.softmax(self.fc3(x), dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mkOk9SSfanLt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleNN()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-yo9I_zCH3d",
        "outputId": "a8faf4a1-d43c-4a2c-a0ff-37caa38d298d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleNN(\n",
              "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a simple feedforward neural network (often called a **fully connected network** or **Multi-Layer Perceptron (MLP)**) using PyTorch. Below is a step-by-step explanation of each component, with particular focus on the `__init__` and `forward` methods, which are critical for defining and applying the architecture.\n",
        "\n",
        "### 1. **Class Definition:**\n",
        "```python\n",
        "class SimpleNN(nn.Module):\n",
        "```\n",
        "- **Explanation**: The `SimpleNN` class inherits from `torch.nn.Module`, the base class for all neural network modules in PyTorch. This base class helps organize the layers and forward-pass operations of the neural network.\n",
        "- **Purpose**: By subclassing `nn.Module`, you can define the layers and logic of your custom neural network, which can then be easily integrated with PyTorch’s training loop and other functionalities.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **The `__init__` Method (Initialization)**\n",
        "```python\n",
        "def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features=784, out_features=64)\n",
        "    self.fc2 = nn.Linear(in_features=64, out_features=64)\n",
        "    self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
        "```\n",
        "\n",
        "#### **Step-by-step Explanation**:\n",
        "- **`__init__`**: This is the constructor of the class. It is called when you create an instance of the `SimpleNN` class.\n",
        "  \n",
        "- **`super().__init__()`**: This calls the parent class `nn.Module`'s constructor. This is necessary because `nn.Module` contains some essential setup steps (such as tracking parameters, layers, etc.) that need to be initialized.\n",
        "\n",
        "- **Defining Layers**:\n",
        "    - `self.fc1 = nn.Linear(in_features=784, out_features=64)`:\n",
        "        - **`nn.Linear`** defines a fully connected (dense) layer. This layer has 784 input features and 64 output features. Each feature is connected to every other feature, and the weights are learned during training.\n",
        "        - **Why 784?**: If you are working with images of size 28x28 pixels (such as in the MNIST dataset), then flattening the image results in a 784-dimensional vector (28 x 28 = 784). This is why the input size is 784.\n",
        "        - **Why 64?**: This is a design choice. You can experiment with different numbers of neurons in this hidden layer, but 64 is a typical small hidden layer size.\n",
        "\n",
        "    - `self.fc2 = nn.Linear(in_features=64, out_features=64)`:\n",
        "        - The second layer also has 64 input features and 64 output features. It is another fully connected layer, meaning each neuron in this layer is connected to each neuron in the previous layer.\n",
        "\n",
        "    - `self.fc3 = nn.Linear(in_features=64, out_features=10)`:\n",
        "        - The final layer has 64 input features and 10 output features. This is because the network is likely designed for a classification task with 10 possible classes (e.g., digits 0-9 in the MNIST dataset).\n",
        "  \n",
        "#### **Purpose of `__init__`**:\n",
        "- The `__init__` method is used to **define the architecture** of the neural network. It initializes the layers (here, 3 fully connected layers) and specifies the number of input and output features for each layer. No computations are performed in this method; it simply sets up the building blocks of the network.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **The `forward` Method (Forward Pass)**\n",
        "```python\n",
        "def forward(self, x):\n",
        "    x = x.view(-1, 784)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.softmax(self.fc3(x), dim=1)\n",
        "    return x\n",
        "```\n",
        "\n",
        "#### **Step-by-step Explanation**:\n",
        "- **`forward`**: This method defines how the input `x` will pass through the network layers and what computations will be applied at each layer. It is automatically called when you pass data through the model (i.e., `model(input)`).\n",
        "\n",
        "- **Input Reshaping**:\n",
        "  - `x = x.view(-1, 784)`:\n",
        "    - This line reshapes the input tensor `x` to have 784 features.\n",
        "    - **Why `view(-1, 784)`?**: The `-1` means that PyTorch will automatically infer the appropriate batch size from the data. For instance, if your input is a batch of images of shape `[batch_size, 1, 28, 28]`, the `.view(-1, 784)` flattens each image from 28x28 into a vector of size 784 while keeping the batch size the same.\n",
        "  \n",
        "- **First Layer (fc1)**:\n",
        "  - `x = F.relu(self.fc1(x))`:\n",
        "    - The input `x` is passed through the first fully connected layer `fc1`. This layer takes the 784-dimensional input and reduces it to 64 dimensions.\n",
        "    - The output is then passed through the **ReLU activation function** (`F.relu`). ReLU (Rectified Linear Unit) introduces non-linearity into the model, allowing it to learn more complex representations.\n",
        "  \n",
        "- **Second Layer (fc2)**:\n",
        "  - `x = F.relu(self.fc2(x))`:\n",
        "    - Similarly, the output from the first layer (which now has 64 features) is passed through the second fully connected layer `fc2`, which maintains the 64-dimensional size.\n",
        "    - Again, ReLU is applied to introduce non-linearity.\n",
        "  \n",
        "- **Third Layer (fc3)**:\n",
        "  - `x = F.softmax(self.fc3(x), dim=1)`:\n",
        "    - The output from the second layer is passed to the final fully connected layer `fc3`, which outputs a 10-dimensional vector (since there are 10 output classes).\n",
        "    - The **Softmax activation function** is applied to this output. Softmax converts the 10 raw output values (logits) into probabilities, where the sum of all probabilities equals 1. The model can then use this to predict the class with the highest probability.\n",
        "\n",
        "- **Return**:\n",
        "  - The output `x` is returned. This is typically a tensor containing the class probabilities for each input sample in the batch.\n",
        "\n",
        "#### **Purpose of `forward`**:\n",
        "- The `forward` method is where the **actual computations** happen. It defines how the input data flows through the network and what transformations (linear layers, activations) are applied. This method is essential for training and inference, as it specifies how inputs map to outputs.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of Purpose:\n",
        "- **`__init__`**: Initializes the network's layers, setting up the architecture (but no computation is done here). The layers are defined as objects, so they can be reused during the forward pass.\n",
        "- **`forward`**: Defines the forward pass (i.e., how data moves through the network). It performs all the computations, including applying activation functions like ReLU and Softmax.\n",
        "\n",
        "### Would you like more details on how to train or evaluate this model, or maybe about backpropagation through the layers?"
      ],
      "metadata": {
        "id": "qUjkSVCyD1k3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eachWeJ8_Zjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4BmNqQte_ZcG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}