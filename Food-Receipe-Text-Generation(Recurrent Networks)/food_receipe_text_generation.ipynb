{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOZHO43ewaV8Jr3TDUHCsez",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/Deep-Learning-Projects/blob/main/Food-Receipe-Text-Generation(Recurrent%20Networks)/food_receipe_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "W5e7sumdhvjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets==3.6.0"
      ],
      "metadata": {
        "id": "-amHlvqjh0Ax"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X3xv_87Z3zlB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Dataset"
      ],
      "metadata": {
        "id": "fUU1LHrKiDO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "MAX_TOKENS = 10_000       # Vocabulary size (ingredients are diverse)\n",
        "SEQ_LENGTH = 60           # Context window (recipes are long)\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 512\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50"
      ],
      "metadata": {
        "id": "tocbasq-s7mZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Pipeline"
      ],
      "metadata": {
        "id": "9P-3lU_VzR24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset():\n",
        "    return datasets.load_dataset(\"m3hrdadfi/recipe_nlg_lite\",\n",
        "                                 trust_remote_code=True)\n",
        "\n",
        "def format_recipe(input):\n",
        "    name = input['name'] if input['name'] else 'unkown'\n",
        "    ingredients = input['ingredients'] if input['ingredients'] else ''\n",
        "    steps = input['steps'] if input['steps'] else \"\"\n",
        "\n",
        "    text = f\"<START> title: {name} <ING> {ingredients} <DIR> {steps} <END>\"\n",
        "    return text\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def receipe_standardization(input):\n",
        "    text = tf.strings.lower(input)\n",
        "    text = tf.strings.regex_replace(text, f\"(<start>|<ing>|<dir>|<end>)\", r\" \\1 \")\n",
        "    text = tf.strings.regex_replace(text, r\"([.,!?()])\", r\" \\1 \")\n",
        "    text = tf.strings.regex_replace(text, r\"[^a-zA-Z0-9.,!?()<>: ]\", \"\")\n",
        "    text = tf.strings.regex_replace(text, r\"\\s{2,}\", \" \")\n",
        "    return text\n",
        "\n",
        "def create_dataset_pipeline(text_corpus):\n",
        "    vectorizer = tf.keras.layers.TextVectorization(\n",
        "        standardize=receipe_standardization,\n",
        "        max_tokens=MAX_TOKENS,\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=None\n",
        "    )\n",
        "    vectorizer.adapt(text_corpus)\n",
        "    vocab = vectorizer.get_vocabulary()\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # Convert into stream of integers\n",
        "    full_text = \" \".join(text_corpus)\n",
        "    full_text_ids = vectorizer([full_text])[0]\n",
        "\n",
        "    ids_dataset = tf.data.Dataset.from_tensor_slices(full_text_ids)\n",
        "    sequences = ids_dataset.batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
        "\n",
        "    def split_input_target(seq):\n",
        "        return seq[:-1], seq[1:]\n",
        "\n",
        "    dataset = sequences.map(split_input_target)\n",
        "    dataset = dataset.shuffle(10_000).batch(\n",
        "        BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset, vectorizer, len(vocab)\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size, stateful=False):\n",
        "    if stateful:\n",
        "        input_layer = tf.keras.Input(batch_shape=(batch_size, None))\n",
        "    else:\n",
        "        input_layer = tf.keras.Input(shape=(None,))\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        input_layer,\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "        tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=stateful),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=stateful),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def generate_receipe(model, vectorizer, food_name, temp=0.6):\n",
        "    start_string = f\"<start> title: {food_name}\"\n",
        "    input_ids = vectorizer([start_string])\n",
        "    input_eval = input_ids\n",
        "\n",
        "    vocab = vectorizer.get_vocabulary()\n",
        "    idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
        "    generated_tokens = []\n",
        "\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'reset_states'):\n",
        "            layer.reset_states()\n",
        "    print(f\"Generating recipe for {food_name}\")\n",
        "\n",
        "    for i in range(300):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        # Temperature\n",
        "        predictions = predictions / temp\n",
        "        # Sample\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Stop token\n",
        "        predicted_word = idx2word.get(predicted_id, \"\")\n",
        "        if predicted_word == \"<end>\":\n",
        "            break\n",
        "\n",
        "        generated_tokens.append(predicted_word)\n",
        "\n",
        "        # Next input\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    raw_text = \" \".join(generated_tokens)\n",
        "    formatted_text = raw_text.replace(\" <ing> \", \"\\n\\n[INGREDIENTS]:\\n\")\n",
        "    formatted_text = formatted_text.replace(\" <dir> \", \"\\n\\n[DIRECTIONS]:\\n\")\n",
        "    # Clean up spacing around punctuation\n",
        "    formatted_text = formatted_text.replace(\" , \", \", \").replace(\" . \", \". \")\n",
        "\n",
        "    return f\"Title: {food_name}\\n\" + formatted_text\n",
        "\n",
        "def main():\n",
        "    # get dataset\n",
        "    print(f\"[INFO] - Downloading dataset from HF...\")\n",
        "    dataset = get_dataset()\n",
        "\n",
        "    # format receipe\n",
        "    print(f\"\\n[INFO] - Formatting dataset...\")\n",
        "    SUBSET = dataset['train'].num_rows\n",
        "\n",
        "    print(f\"\\n[INFO] Processing first {SUBSET} recipes...\")\n",
        "    train_dataset = dataset['train'].select(range(SUBSET))\n",
        "    text_corpus = [format_recipe(text) for text in train_dataset]\n",
        "    print(f\"\\n[INFO] Text corpus created successfully.\")\n",
        "    print(f\"Total samples: {len(text_corpus)}\")\n",
        "\n",
        "    # Show example of text_corpus\n",
        "    print(\"\\nSample Formatted Recipe\")\n",
        "    print(text_corpus[0])\n",
        "\n",
        "    # Vectorization\n",
        "    print(f\"\\n[INFO] Creating dataset...\")\n",
        "    dataset, vectorizer, vocab_size = create_dataset_pipeline(text_corpus)\n",
        "    print(\"Dataset created successfully.\")\n",
        "    print(f\"Vocab Size: {vocab_size}\")\n",
        "\n",
        "    # Build Model\n",
        "    print(f\"\\n[INFO] - Creating training model...\")\n",
        "    model = build_model(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        rnn_units=RNN_UNITS,\n",
        "        batch_size=BATCH_SIZE\n",
        "        )\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    )\n",
        "    print(\"Model created successfully.\")\n",
        "    print(\"Check model's summary:\")\n",
        "    model.summary()\n",
        "\n",
        "    # Define callbacks\n",
        "    # Callbacks\n",
        "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "        \"recipe_model.weights.h5\", save_best_only=True, save_weights_only=True, monitor='loss'\n",
        "    )\n",
        "    early_stop_cb = tf.keras.callbacks.EarlyStopping(patience=3, monitor='loss')\n",
        "\n",
        "    print(\"[INFO] Starting Training...\")\n",
        "    model.fit(\n",
        "        dataset, epochs=EPOCHS, callbacks=[checkpoint_cb, early_stop_cb])\n",
        "    print(\"Training finished...\")\n",
        "\n",
        "    # Inference Model\n",
        "    print(f\"[INFO] - Creating inference model...\")\n",
        "    inference_model = build_model(\n",
        "        vocab_size, EMBEDDING_DIM, RNN_UNITS, batch_size=1, stateful=True)\n",
        "    inference_model.load_weights(\"recipe_model.weights.h5\")\n",
        "    inference_model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "    # Test Generation\n",
        "    print(f\"Generating....\")\n",
        "    result = generate_receipe(\n",
        "        inference_model, vectorizer, \"chicken pizza\", temp=0.6\n",
        "    )\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(result)\n",
        "    print(\"=\"*40 + \"\\n\")\n",
        "\n",
        "# Execution\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qq7skTh4hwix",
        "outputId": "18ecdc08-1f48-4539-d2b8-47d5bad8d992"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] - Downloading dataset from HF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] - Formatting dataset...\n",
            "\n",
            "[INFO] Processing first 6118 recipes...\n",
            "\n",
            "[INFO] Text corpus created successfully.\n",
            "Total samples: 6118\n",
            "\n",
            "Sample Formatted Recipe\n",
            "<START> title: pork chop noodle soup <ING> 3.0 bone in pork chops, salt, pepper, 2.0 tablespoon vegetable oil, 2.0 cup chicken broth, 4.0 cup vegetable broth, 1.0 red onion, 4.0 carrots, 2.0 clove garlic, 1.0 teaspoon dried thyme, 0.5 teaspoon dried basil, 1.0 cup rotini pasta, 2.0 stalk celery <DIR> season pork chops with salt and pepper . heat oil in a dutch oven over medium high heat . add chops and cook for about 4 minutes, until golden brown . flip and cook 4 minutes more, until golden brown . transfer chops to a plate and set aside . pour half of chicken broth into pot, scraping all browned bits from bottom . add remaining chicken broth, vegetable broth, onion, carrots, celery and garlic . mix well and bring to a simmer . add 1 quart water, thyme, basil, 2 teaspoons salt and 1 teaspoon pepper . mix well and bring to a simmer . add chops back to pot and return to simmer . reduce heat and simmer for 90 minutes, stirring occasionally, being careful not to break up chops . transfer chops to plate, trying not to break them up . set aside to cool . raise the heat and bring the soup to a boil . add pasta and cook for about 12 minutes, until tender . when the chops are cool, pull them apart, discarding all the bones and fat . add the meat back to soup and stir well . taste for salt and pepper, and add if needed, before serving. <END>\n",
            "\n",
            "[INFO] Creating dataset...\n",
            "Dataset created successfully.\n",
            "Vocab Size: 10000\n",
            "\n",
            "[INFO] - Creating training model...\n",
            "Model created successfully.\n",
            "Check model's summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │     \u001b[38;5;34m2,560,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_2 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,182,720\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_3 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,575,936\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)    │     \u001b[38;5;34m5,130,000\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,182,720</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,575,936</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130,000</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,448,656\u001b[0m (39.86 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,448,656</span> (39.86 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,448,656\u001b[0m (39.86 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,448,656</span> (39.86 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Starting Training...\n",
            "Epoch 1/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 93ms/step - loss: 6.3752\n",
            "Epoch 2/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 104ms/step - loss: 5.9847\n",
            "Epoch 3/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 114ms/step - loss: 5.4265\n",
            "Epoch 4/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 107ms/step - loss: 4.8880\n",
            "Epoch 5/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 111ms/step - loss: 4.4183\n",
            "Epoch 6/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 110ms/step - loss: 4.0305\n",
            "Epoch 7/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 3.7393\n",
            "Epoch 8/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 111ms/step - loss: 3.5386\n",
            "Epoch 9/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 3.3657\n",
            "Epoch 10/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 111ms/step - loss: 3.2367\n",
            "Epoch 11/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 111ms/step - loss: 3.1305\n",
            "Epoch 12/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 3.0421\n",
            "Epoch 13/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - loss: 2.9683\n",
            "Epoch 14/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.9068\n",
            "Epoch 15/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 111ms/step - loss: 2.8424\n",
            "Epoch 16/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.7965\n",
            "Epoch 17/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - loss: 2.7514\n",
            "Epoch 18/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.7040\n",
            "Epoch 19/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.6601\n",
            "Epoch 20/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.6185\n",
            "Epoch 21/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 111ms/step - loss: 2.5900\n",
            "Epoch 22/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 116ms/step - loss: 2.5621\n",
            "Epoch 23/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 110ms/step - loss: 2.5189\n",
            "Epoch 24/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.4896\n",
            "Epoch 25/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.4582\n",
            "Epoch 26/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.4345\n",
            "Epoch 27/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 113ms/step - loss: 2.4059\n",
            "Epoch 28/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.3876\n",
            "Epoch 29/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.3658\n",
            "Epoch 30/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 113ms/step - loss: 2.3440\n",
            "Epoch 31/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 111ms/step - loss: 2.3225\n",
            "Epoch 32/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.3044\n",
            "Epoch 33/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.2779\n",
            "Epoch 34/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.2670\n",
            "Epoch 35/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 113ms/step - loss: 2.2450\n",
            "Epoch 36/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.2283\n",
            "Epoch 37/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.2118\n",
            "Epoch 38/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.1943\n",
            "Epoch 39/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.1805\n",
            "Epoch 40/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.1701\n",
            "Epoch 41/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.1552\n",
            "Epoch 42/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.1379\n",
            "Epoch 43/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 113ms/step - loss: 2.1293\n",
            "Epoch 44/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.1092\n",
            "Epoch 45/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.1024\n",
            "Epoch 46/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 113ms/step - loss: 2.0902\n",
            "Epoch 47/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 112ms/step - loss: 2.0839\n",
            "Epoch 48/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.0665\n",
            "Epoch 49/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.0541\n",
            "Epoch 50/50\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - loss: 2.0477\n",
            "Training finished...\n",
            "[INFO] - Creating inference model...\n",
            "Generating....\n",
            "Generating recipe for chicken pizza\n",
            "\n",
            "========================================\n",
            "Title: chicken pizza\n",
            "<ing> 1 12 cups cooked chicken, 3 tbsp extra virgin olive oil, 1 12 tbsp olive oil, 12 tsp salt, 1 tsp turmeric, 14 tsp black pepper, 12 tsp dried thyme, 14 tsp freshly ground black pepper, 14 cup low sodium chicken broth, 1 cup fresh parsley, 1 tbsp freshly grated parmigiano reggiano\n",
            "\n",
            "[DIRECTIONS]:\n",
            "preheat oven to 425 degrees f. spray a baking sheet with cooking spray. cut the baguette into 14 inch slices, then cut the rounds into rounds and transfer them to a large mixing bowl. drizzle with a little olive oil, then sprinkle with salt and pepper. discard the skin. wrap the asparagus on a paper towel and allow to cool for about 15 minutes. you can use a knife to cut the strands off, so they are cut at the top end of the bread. remove the stem and cut the baguette in half lengthwise to make a nice cross. cut the chicken breasts into 14 inch pieces. in a medium bowl, combine the shredded chicken, cheese and lemon juice. sprinkle with salt and pepper. bake for 20 minutes, or until the cheese is melted and crisp .\n",
            "========================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "9fRzgBjGibT8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "Pl3s9bELizXx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# قدم ۱: طراحی فرمت متن (Prompt Engineering)\n",
        "# قدم ۲: تابع تبدیل داده (Data Preparation)\n",
        "# قدم ۳: استانداردسازی و توکن‌سازی (Vectorization)\n",
        "# قدم ۴: ساخت دیتاست (Input/Target Pipeline)\n",
        "# قدم ۵: مدل و آموزش\n",
        "# قدم ۶: تولید دستور پخت (Inference)"
      ],
      "metadata": {
        "id": "pF4aV6orlB3U"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}